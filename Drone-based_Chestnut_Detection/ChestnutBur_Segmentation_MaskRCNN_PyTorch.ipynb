{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Chestnut Bur Detection and Segmentation using MaskRCNN in PyTorch</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, Subset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms as _transforms, tv_tensors\n",
    "import torchvision.transforms.v2 as T\n",
    "from torchvision.models.detection import MaskRCNN\n",
    "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import gc\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <center> Load the image and annotation data </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load annotations from json file\n",
    "annos = json.load(open(\"S:/Zack/Deep Learning/Chestnut_Bur_Instance_Segmentation/route9_orchard3/data/_annotations.coco.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the annos dict to a df\n",
    "annos_df = pd.DataFrame(annos[\"annotations\"])\n",
    "df = pd.DataFrame()\n",
    "df[\"tree_id\"] = annos_df[\"image_id\"].apply(lambda x: annos[\"images\"][x][\"file_name\"].split(\"_\")[0])\n",
    "df[\"file_name\"] = annos_df[\"image_id\"].apply(lambda x: annos[\"images\"][x][\"file_name\"])\n",
    "df[\"file_name\"] = df[\"file_name\"].apply(lambda x: x.split(\"_\")[0] + \".png\")\n",
    "categories = [cat[\"name\"] for cat in annos[\"categories\"]]\n",
    "df[\"category_name\"] = annos_df[\"category_id\"].apply(lambda x: categories[x])\n",
    "df[\"bbox\"] = annos_df[\"bbox\"].apply(lambda x: torch.tensor(x))\n",
    "df[\"area\"] = annos_df[\"area\"].apply(lambda x: torch.tensor(x))\n",
    "df[\"segmentation\"] = annos_df[\"segmentation\"].apply(lambda x: torch.tensor(x))\n",
    "df[\"iscrowd\"] = annos_df[\"iscrowd\"]\n",
    "\n",
    "# Filter df to remove treeIds not reviewed. \n",
    "reviewed_trees = [\n",
    "    14, 44, 51, 60, 79, 91, 92, 117, 118, 146, 152, 171, \n",
    "    172, 210, 272, 276, 280, 286, 304, 309, 320, 329, \n",
    "    366, 369, 371, 392, 394, 405\n",
    "]\n",
    "\n",
    "df = df[df[\"tree_id\"].isin([str(tree_id) for tree_id in reviewed_trees])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = \"S:/Zack/Deep Learning/Chestnut_Bur_Instance_Segmentation/route9_orchard3/data/images\"\n",
    "image_names = df[\"file_name\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <center> Pre-process and transform image and annotation data </center>\n",
    "\n",
    "##### Adapted from: https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html#an-instance-segmentation-model-for-pennfudan-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_fill(mask, polys, color):\n",
    "    for poly in polys:\n",
    "        cv2.fillPoly(mask, [poly], color)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChestnutBurSegmentation(Dataset):\n",
    "    \"\"\"Custom Dataset for Chestnut Bur Segmentation in UAV Images\"\"\"\n",
    "\n",
    "    def __init__(self, image_dir, df, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        self.tree_ids = df[\"tree_id\"].unique()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        tree_id = self.tree_ids[idx]\n",
    "\n",
    "        row = self.df[self.df[\"tree_id\"] == tree_id]\n",
    "        image_path = Path(self.image_dir) / row[\"file_name\"].values[0]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        # filter the df for tree_id\n",
    "        tree_df = self.df[self.df[\"tree_id\"] == tree_id]\n",
    "\n",
    "        canopy_poly = tree_df[tree_df[\"category_name\"] == \"Canopy\"][\"segmentation\"].values\n",
    "        canopy_poly = [np.array(poly[0]).reshape(-1, 2).astype(np.int32) for poly in canopy_poly]\n",
    "        canopy_mask = np.zeros((image.size[1], image.size[0]), dtype=np.uint8)\n",
    "        canopy_mask = mask_fill(canopy_mask, canopy_poly, 1)\n",
    "\n",
    "        bur_poly = tree_df[tree_df[\"category_name\"] == \"Chestnut-burr\"][\"segmentation\"].values\n",
    "        bur_poly = [np.array(poly[0]).reshape(-1, 2).astype(np.int32) for poly in bur_poly]\n",
    "        bur_masks = []\n",
    "        for poly in bur_poly:\n",
    "            mask = np.zeros((image.size[1], image.size[0]), dtype=np.uint8)\n",
    "            mask = mask_fill(mask, [poly], 1)\n",
    "            bur_masks.append(mask)\n",
    "        mask_image = np.stack(bur_masks, axis=0)\n",
    "\n",
    "        # Ensure the image and masks have compatible shapes\n",
    "        image_np = np.array(image)  # Convert image to [H, W, 3]\n",
    "        mask_image = mask_image.transpose(1, 2, 0)  # Convert masks to [H, W, N]\n",
    "\n",
    "        # Stack the image and masks along the channel dimension\n",
    "        combined_image = np.dstack((image_np, mask_image))\n",
    "\n",
    "        # Convert combined image to tensor for cropping\n",
    "        combined_image = torch.tensor(combined_image, dtype=torch.uint8)\n",
    "\n",
    "        # crop combined image to padded canopy bbox\n",
    "        canopy_bbox = tree_df[tree_df[\"category_name\"] == \"Canopy\"][\"bbox\"].values[0]\n",
    "        canopy_bbox = [int(bbox) for bbox in canopy_bbox]\n",
    "        padding = 100\n",
    "        padded_bbox = [\n",
    "            max(0, canopy_bbox[0] - padding),\n",
    "            max(0, canopy_bbox[1] - padding),\n",
    "            min(image_np.shape[1], canopy_bbox[0] + canopy_bbox[2] + padding),\n",
    "            min(image_np.shape[0], canopy_bbox[1] + canopy_bbox[3] + padding)\n",
    "        ]\n",
    "        cropped_image = combined_image[padded_bbox[1]:padded_bbox[3], padded_bbox[0]:padded_bbox[2]]\n",
    "\n",
    "        # Calculate the offset of the crop\n",
    "        offset_x, offset_y = padded_bbox[0], padded_bbox[1]\n",
    "\n",
    "        # Separate the image and masks after cropping\n",
    "        image = cropped_image[:, :, :3].numpy()\n",
    "        mask_image = cropped_image[:, :, 3:].numpy()\n",
    "\n",
    "        # Fill the background with white where the canopy mask is not present\n",
    "        fill_color = [255, 255, 255]  # RGB for white\n",
    "        mask = canopy_mask[padded_bbox[1]:padded_bbox[3], padded_bbox[0]:padded_bbox[2]].astype(bool)\n",
    "        for c in range(3):\n",
    "            image[:, :, c][~mask] = fill_color[c]\n",
    "\n",
    "        image = image.transpose(2, 0, 1)\n",
    "        mask_image = mask_image.transpose(2, 0, 1)\n",
    "\n",
    "        # Convert image and masks to tv_tensor\n",
    "        image = tv_tensors.Image(image)\n",
    "        mask_image = tv_tensors.Mask(mask_image, dtype=torch.bool)\n",
    "\n",
    "        # get target values for features in mask image\n",
    "        labels = tree_df[tree_df[\"category_name\"] == \"Chestnut-burr\"][\"category_name\"].values\n",
    "        labels = [categories.index(label) for label in labels]\n",
    "        bboxes = tree_df[tree_df[\"category_name\"] == \"Chestnut-burr\"][\"bbox\"].values\n",
    "        bboxes = [torch.tensor([bbox[0] - offset_x, bbox[1] - offset_y, bbox[0] + bbox[2] - offset_x, bbox[1] + bbox[3] - offset_y], dtype=torch.float32) for bbox in bboxes]\n",
    "        bboxes = torch.stack([bbox for bbox in bboxes], dim=0)\n",
    "        area = tree_df[tree_df[\"category_name\"] == \"Chestnut-burr\"][\"area\"].values.astype(np.float32)\n",
    "        iscrowd = tree_df[tree_df[\"category_name\"] == \"Chestnut-burr\"][\"iscrowd\"].values.astype(np.uint8)\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": tv_tensors.BoundingBoxes(bboxes, format=tv_tensors.BoundingBoxFormat.XYXY, canvas_size=(image.shape[1], image.shape[2])),\n",
    "            \"labels\": torch.tensor(labels),\n",
    "            \"image_id\": torch.tensor([idx]),\n",
    "            \"area\": torch.tensor(area, dtype=torch.float32),\n",
    "            \"iscrowd\": torch.tensor(iscrowd, dtype=torch.uint8),\n",
    "            \"masks\": mask_image\n",
    "        }\n",
    "\n",
    "        if self.transform:\n",
    "            image, target = self.transform(image, target)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tree_ids)\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define transforms for the dataset\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    transforms.append(T.ToImage())\n",
    "    if train:\n",
    "        transforms.append(T.RandomIoUCrop(min_scale=0.3, max_scale=0.9))\n",
    "        transforms.append(T.RandomApply([T.ColorJitter(brightness=0.05, contrast=0.1, saturation=0.1, hue=0)], p=0.5))\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    transforms.append(T.Resize(size=(512,), max_size=None))\n",
    "    transforms.append(T.SanitizeBoundingBoxes()) ## for segmentations too\n",
    "    transforms.append(T.ClampBoundingBoxes()) # for segmentations too\n",
    "    transforms.append(T.ToDtype(dtype = torch.float32, scale = True))\n",
    "    transforms.append(T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]))\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ds = ChestnutBurSegmentation(image_dir, df, get_transform(train = True))\n",
    "\n",
    "sample_dl = DataLoader(sample_ds, \n",
    "                       batch_size = 8, \n",
    "                       shuffle = True, \n",
    "                       collate_fn = ChestnutBurSegmentation.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, targets = next(iter(sample_dl))\n",
    "images = [img for img in images]\n",
    "targets = [{k: v for k, v in target.items()} for target in targets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <center> Plot sample transformed images, targets, and masks </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sample_images_and_targets(images, targets):\n",
    "    batch_size = len(images)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        # Get the image and target from the batch\n",
    "        image = images[i]\n",
    "        target = targets[i]\n",
    "\n",
    "        # Convert image tensor to numpy array for plotting\n",
    "        image_np = image.permute(1, 2, 0).cpu().numpy()  # Convert from [C, H, W] to [H, W, C]\n",
    "\n",
    "        # Plot the image\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(10, 10), dpi=300)  # Increase DPI for higher resolution\n",
    "        ax.imshow(image_np)\n",
    "        ax.set_title(f\"Image {i}\")\n",
    "\n",
    "        # Plot bounding boxes\n",
    "        bboxes = target['boxes'].cpu().numpy()\n",
    "        for bbox in bboxes:\n",
    "            rect = plt.Rectangle((bbox[0], bbox[1]), bbox[2] - bbox[0], bbox[3] - bbox[1], fill=False, color='red', linewidth=0.5)\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "plot_sample_images_and_targets(images, targets)\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot each transformed image with its chestnut burr masks\n",
    "\n",
    "for i, (image, target) in enumerate(zip(images, targets)):\n",
    "\n",
    "    tree_id = target[\"image_id\"]\n",
    "    image = image.permute(1, 2, 0)\n",
    "    mask = target[\"masks\"].permute(1, 2, 0)\n",
    "\n",
    "    #plot mask on image and save to .png file\n",
    "    plt.imshow(image)\n",
    "    plt.imshow(mask.sum(axis=2), cmap='gray', alpha=0.75)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <center> Construct MaskRCNN Model </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_instance_segmentation_model(depth=34, trainable_backbone_layers=3, min_size=512, max_size=None, image_mean=0, \n",
    "                                    image_std=1, num_classes=2, box_score_thresh=0.1, box_nms_thresh=0.5,\n",
    "                                    box_detections_per_img=1500):\n",
    "    # Create the backbone with FPN\n",
    "    if depth == 18:\n",
    "        backbone = resnet_fpn_backbone(backbone_name='resnet18', \n",
    "                                       weights=torchvision.models.ResNet18_Weights.DEFAULT, \n",
    "                                       trainable_layers=trainable_backbone_layers\n",
    "                                       )\n",
    "    elif depth == 34:\n",
    "        backbone = resnet_fpn_backbone(backbone_name='resnet34', \n",
    "                                       weights=torchvision.models.ResNet34_Weights.DEFAULT,\n",
    "                                       trainable_layers=trainable_backbone_layers\n",
    "                                       )\n",
    "    elif depth == 50:\n",
    "        backbone = resnet_fpn_backbone(backbone_name='resnet50', \n",
    "                                       weights=torchvision.models.ResNet50_Weights.DEFAULT,\n",
    "                                       trainable_layers=trainable_backbone_layers\n",
    "                                       )\n",
    "    elif depth == 101:\n",
    "        backbone = resnet_fpn_backbone(backbone_name='resnet101', \n",
    "                                       weights=torchvision.models.ResNet101_Weights.DEFAULT, \n",
    "                                       trainable_layers=trainable_backbone_layers\n",
    "                                       )\n",
    "    elif depth == 152:\n",
    "        backbone = resnet_fpn_backbone(backbone_name='resnet152', \n",
    "                                       weights=torchvision.models.ResNet152_Weights.DEFAULT, \n",
    "                                       trainable_layers=trainable_backbone_layers\n",
    "                                       )\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model depth\")\n",
    "\n",
    "    # Create the Mask R-CNN model with the custom backbone\n",
    "    model = MaskRCNN(backbone, \n",
    "                     num_classes=num_classes,\n",
    "                     min_size=min_size,\n",
    "                     max_size=max_size,\n",
    "                     image_mean=image_mean,\n",
    "                     image_std=image_std,\n",
    "                     box_score_thresh=box_score_thresh,\n",
    "                     box_nms_thresh=box_nms_thresh,\n",
    "                     box_detections_per_img=box_detections_per_img\n",
    "                     )\n",
    "\n",
    "    # Get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "    # Replace the pre-trained head with a new one to reflect the number of classes\n",
    "    model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # Get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "\n",
    "    # Replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = torchvision.models.detection.mask_rcnn.MaskRCNNPredictor(\n",
    "        in_features_mask,\n",
    "        hidden_layer,\n",
    "        num_classes\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_instance_segmentation_model())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Tune model hyperparameters using Ray Tune</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian Optimization HyperBand (BOHB) with HyperBand scheduler\n",
    "# # https://proceedings.mlr.press/v80/falkner18a.html\n",
    "\n",
    "from torch_lr_finder import LRFinder, TrainDataLoaderIter\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import gc\n",
    "import time\n",
    "import ray\n",
    "from ray import train, tune\n",
    "from ray.tune import JupyterNotebookReporter\n",
    "from ray.tune.search import ConcurrencyLimiter\n",
    "from ray.tune.schedulers.hb_bohb import HyperBandForBOHB\n",
    "from ray.tune.search.bohb import TuneBOHB\n",
    "import ray.cloudpickle as pickle\n",
    "from segmentation_pytorch import engine\n",
    "from segmentation_pytorch.coco_utils import get_coco_api_from_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ray Tune Trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducible training\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def calculate_f1_score(precision, recall):\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "def train_ChestnutBurSegmentation(config):\n",
    "    import ray\n",
    "    import torch\n",
    "    from segmentation_pytorch import engine\n",
    "    import tempfile\n",
    "    from pathlib import Path\n",
    "    import ray.cloudpickle as pickle\n",
    "\n",
    "     # Set random seed for reproducible training\n",
    "    set_seed(666)\n",
    "\n",
    "    # Get dataset references directly from config\n",
    "    dataset_train = ray.get(config[\"dataset_train_ref\"])\n",
    "    data_loader_val = ray.get(config[\"data_loader_val_ref\"])\n",
    "    train_coco_ds = ray.get(config[\"train_coco_ds_ref\"])\n",
    "    val_coco_ds = ray.get(config[\"val_coco_ds_ref\"])\n",
    "\n",
    "    # Use gradient accumulation due to memory constraints (batch_size=16 maxes out GPU)\n",
    "    training_steps = [\n",
    "        {\"step\": 0, \"batch_size\": config[\"batch_size\"], \"epochs\": 10, \"print_freq\": 25, \"accumulation_steps\": 2, \"score_thresh\": 0.1, \"nms_thresh\": 0.5},\n",
    "        {\"step\": 1, \"batch_size\": config[\"batch_size\"], \"epochs\": 10, \"print_freq\": 25, \"accumulation_steps\": 4, \"score_thresh\": 0.2, \"nms_thresh\": 0.4},\n",
    "        {\"step\": 2, \"batch_size\": config[\"batch_size\"], \"epochs\": 5, \"print_freq\": 25, \"accumulation_steps\": 8, \"score_thresh\": 0.3, \"nms_thresh\": 0.3}\n",
    "    ]\n",
    "\n",
    "    # Load checkpoint if available\n",
    "    checkpoint = train.get_checkpoint()\n",
    "    if checkpoint:\n",
    "        with checkpoint.as_directory() as checkpoint_dir:\n",
    "            data_path = Path(checkpoint_dir) / \"data.pkl\"\n",
    "            with open(data_path, \"rb\") as fp:\n",
    "                checkpoint_state = pickle.load(fp)\n",
    "            start_epoch = checkpoint_state[\"epoch\"] + 1\n",
    "            current_step = checkpoint_state[\"current_step\"]\n",
    "            batch_size = checkpoint_state[\"batch_size\"]\n",
    "            accumulation_steps = checkpoint_state[\"accumulation_steps\"]\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        current_step = 0\n",
    "        batch_size = config[\"batch_size\"]\n",
    "        accumulation_steps = training_steps[0][\"accumulation_steps\"]\n",
    "\n",
    "    # Initialize step index\n",
    "    step_index = current_step\n",
    "\n",
    "    # Loop through training_steps during training to increase batch size\n",
    "    while step_index < len(training_steps):\n",
    "        step = training_steps[step_index]\n",
    "\n",
    "        batch_size = step['batch_size']\n",
    "        total_epochs = step['epochs']\n",
    "        print_freq = step['print_freq']\n",
    "        accumulation_steps = step['accumulation_steps']\n",
    "        box_score_thresh = step['score_thresh']\n",
    "        box_nms_thresh = step['nms_thresh']\n",
    "\n",
    "        model = get_instance_segmentation_model(depth=config[\"resnet\"],\n",
    "                                                trainable_backbone_layers=config[\"backbone_lyrs\"],\n",
    "                                                min_size=512,\n",
    "                                                max_size=None,\n",
    "                                                image_mean=0,\n",
    "                                                image_std=1,\n",
    "                                                num_classes=2, # background, chestnut bur\n",
    "                                                box_score_thresh=box_score_thresh,\n",
    "                                                box_nms_thresh=box_nms_thresh,\n",
    "                                                box_detections_per_img=1500) \n",
    "        \n",
    "        device = \"cpu\"\n",
    "        if torch.cuda.is_available():\n",
    "            device = \"cuda:0\"\n",
    "            if torch.cuda.device_count() > 1:\n",
    "                model = nn.DataParallel(model) # train on multiple gpus if available\n",
    "        model.to(device)\n",
    "\n",
    "        # construct an optimizer\n",
    "        params = [p for p in model.parameters() if p.requires_grad]\n",
    "        optimizer = torch.optim.SGD(params, \n",
    "                                    lr = config[\"lr\"], \n",
    "                                    momentum = config[\"momentum\"], \n",
    "                                    weight_decay = config[\"weight_decay\"])\n",
    "\n",
    "        # construct a learning rate scheduler\n",
    "        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, \n",
    "                                                    step_size = config[\"step_size\"], \n",
    "                                                    gamma = config[\"gamma_lr\"])\n",
    "\n",
    "        # Restore model and optimizer state if checkpoint is available\n",
    "        if checkpoint:\n",
    "            model.load_state_dict(checkpoint_state[\"model_state_dict\"])\n",
    "            optimizer.load_state_dict(checkpoint_state[\"optimizer_state_dict\"])\n",
    "\n",
    "        # Calculate the remaining epochs for the current step\n",
    "        remaining_epochs = total_epochs - (start_epoch % total_epochs)\n",
    "\n",
    "        data_loader = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size,\n",
    "                                                  shuffle=True, \n",
    "                                                  collate_fn=ChestnutBurSegmentation.collate_fn,\n",
    "                                                  num_workers=0, pin_memory=True)\n",
    "\n",
    "        print(f'Training step {step[\"step\"]}... batch size: {batch_size * accumulation_steps}')\n",
    "        print()\n",
    "\n",
    "        for epoch in range(start_epoch, start_epoch + remaining_epochs):\n",
    "            train_metric_logger, val_metric_logger = engine.train_one_epoch(model, optimizer, data_loader, device,\n",
    "                                                                     epoch, print_freq, accumulation_steps,\n",
    "                                                                     data_loader_val)\n",
    "\n",
    "            # Evaluate on the val dataset\n",
    "            train_coco_evaluator, val_coco_evaluator = engine.evaluate(model, data_loader_val, val_coco_ds, device, data_loader, train_coco_ds)\n",
    "\n",
    "            # Update the learning rate\n",
    "            lr_scheduler.step()\n",
    "\n",
    "            checkpoint_data = {\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"current_step\": step[\"step\"],\n",
    "                \"batch_size\": batch_size,\n",
    "                \"accumulation_steps\": accumulation_steps,\n",
    "            }\n",
    "\n",
    "            with tempfile.TemporaryDirectory() as checkpoint_dir:\n",
    "                data_path = Path(checkpoint_dir) / \"data.pkl\"\n",
    "                with open(data_path, \"wb\") as fp:\n",
    "                    pickle.dump(checkpoint_data, fp)\n",
    "                train.report(\n",
    "                    {\"epoch\": epoch,\n",
    "                     \"train_loss\": train_metric_logger.loss.avg,  # loss averaged across all batches\n",
    "                     \"val_loss\": val_metric_logger.loss.avg,\n",
    "                     \"train_mAP\": train_coco_evaluator.coco_eval['segm'].stats[0],  # mAP (IoU=0.50:0.95)\n",
    "                     \"val_mAP\": val_coco_evaluator.coco_eval['segm'].stats[0],\n",
    "                     \"train_mAR\": train_coco_evaluator.coco_eval['segm'].stats[8],  # mAR (IoU=0.50:0.95)\n",
    "                     \"val_mAR\": val_coco_evaluator.coco_eval['segm'].stats[8],\n",
    "                     \"train_f1\": calculate_f1_score(train_coco_evaluator.coco_eval['segm'].stats[0],  # AP (IoU=0.50:0.95)\n",
    "                                                    train_coco_evaluator.coco_eval['segm'].stats[8]  # AR (IoU=0.50:0.95)\n",
    "                                                    ),\n",
    "                     \"val_f1\": calculate_f1_score(val_coco_evaluator.coco_eval['segm'].stats[0],\n",
    "                                                  val_coco_evaluator.coco_eval['segm'].stats[8]\n",
    "                                                  )},\n",
    "                    checkpoint=train.Checkpoint.from_directory(checkpoint_dir),\n",
    "                )\n",
    "\n",
    "        # Set start_epoch to the next epoch for the next training step\n",
    "        start_epoch += remaining_epochs\n",
    "        step_index += 1\n",
    "\n",
    "    print('Tuning Trial Complete!')\n",
    "\n",
    "\n",
    "def test_best_model(best_trial, best_checkpoint):\n",
    "    best_model = get_instance_segmentation_model(depth=best_trial.config[\"resnet\"],\n",
    "                                                trainable_backbone_layers=best_trial.config[\"backbone_lyrs\"],\n",
    "                                                min_size=512,\n",
    "                                                max_size=None,\n",
    "                                                image_mean=0,\n",
    "                                                image_std=1,\n",
    "                                                num_classes=2, # background, chestnut bur\n",
    "                                                box_score_thresh=0.3, # from last training step\n",
    "                                                box_nms_thresh=0.3,\n",
    "                                                box_detections_per_img=1500) # background, chestnut bur\n",
    "\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        \n",
    "    best_model.to(device)\n",
    "\n",
    "    with best_checkpoint.as_directory() as checkpoint_dir:\n",
    "        data_path = Path(checkpoint_dir) / \"data.pkl\"\n",
    "        with open(data_path, \"rb\") as fp:\n",
    "            best_checkpoint_data = pickle.load(fp)\n",
    "\n",
    "        best_model.load_state_dict(best_checkpoint_data[\"model_state_dict\"])\n",
    "\n",
    "    data_loader_test = ray.get(best_trial.config[\"data_loader_test_ref\"])\n",
    "    test_coco_ds = ray.get(best_trial.config[\"test_coco_ds_ref\"]) \n",
    "    \n",
    "    test_results = engine.evaluate(best_model, data_loader_test, test_coco_ds, device, train_data_loader=None, train_coco_ds=None)\n",
    "\n",
    "    print(f'Best trial test set mAP: {test_results.coco_eval[\"segm\"].stats[0]}') # IoU=0.50:0.95\n",
    "    print(f'Best trial test set mAR: {test_results.coco_eval[\"segm\"].stats[8]}') # IoU=0.50:0.95\n",
    "    print(f'Best trial test set f1-score: {calculate_f1_score(test_results.coco_eval[\"segm\"].stats[0], test_results.coco_eval[\"segm\"].stats[8])}') # IoU=0.50:0.95\n",
    "\n",
    "\n",
    "def trial_dirname_creator(trial):\n",
    "    return f\"{trial.trial_id}\"\n",
    "\n",
    "\n",
    "def create_coco_datasets(train_dataset, val_dataset, test_dataset):\n",
    "    \"\"\"\n",
    "    Create COCO dataset objects from torch.utils.data.Dataset using get_coco_api_from_dataset.\n",
    "    This function creates the COCO dataset objects in parallel.\n",
    "    \n",
    "    :param train_dataset: torch.utils.data.Dataset\n",
    "    :param val_dataset: torch.utils.data.Dataset\n",
    "    :param test_dataset: torch.utils.data.Dataset\n",
    "    :return: train_coco_ds, val_coco_ds, test_coco_ds\n",
    "    \"\"\"\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        train_future = executor.submit(get_coco_api_from_dataset, train_dataset)\n",
    "        val_future = executor.submit(get_coco_api_from_dataset, val_dataset)\n",
    "        test_future = executor.submit(get_coco_api_from_dataset, test_dataset)\n",
    "\n",
    "        train_coco_ds = train_future.result()\n",
    "        val_coco_ds = val_future.result()\n",
    "        test_coco_ds = test_future.result()\n",
    "\n",
    "    return train_coco_ds, val_coco_ds, test_coco_ds\n",
    "\n",
    "\n",
    "def train_lr_finder(config):\n",
    "\n",
    "    class CustomTrainDataLoaderIter(TrainDataLoaderIter):\n",
    "        def inputs_labels_from_batch(self, batch_data):\n",
    "            inputs = [image.to('cuda:0') for image in batch_data[0]]\n",
    "            labels = [{k: v.to('cuda:0') for k, v in t.items()} for t in batch_data[1]]\n",
    "            return inputs, labels\n",
    "\n",
    "    dataset_train = ray.get(config[\"dataset_train_ref\"])\n",
    "    accumulation_steps = 2  ## FIXME: hardcoded for now\n",
    "\n",
    "    data_loader_train = torch.utils.data.DataLoader(dataset_train, batch_size=config[\"batch_size\"],\n",
    "                                                    shuffle=True,\n",
    "                                                    collate_fn=ChestnutBurSegmentation.collate_fn,\n",
    "                                                    num_workers=0, pin_memory=True)\n",
    "\n",
    "    model = get_instance_segmentation_model(depth=config[\"resnet\"],\n",
    "                                            trainable_backbone_layers=config[\"backbone_lyrs\"],\n",
    "                                            min_size=512,\n",
    "                                            max_size=None,\n",
    "                                            image_mean=0,\n",
    "                                            image_std=1,\n",
    "                                            num_classes=2, # background, chestnut bur\n",
    "                                            box_score_thresh=0.1, # from first training step\n",
    "                                            box_nms_thresh=0.5,\n",
    "                                            box_detections_per_img=1500).to('cuda:0')\n",
    "\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(\n",
    "        params, lr=1e-7, momentum=config[\"momentum\"], weight_decay=config[\"weight_decay\"]\n",
    "    )\n",
    "\n",
    "    train_iter = CustomTrainDataLoaderIter(data_loader_train)\n",
    "    grad_scaler = torch.GradScaler()\n",
    "\n",
    "    class CustomLRFinder(LRFinder):\n",
    "        def __init__(self, model, optimizer, criterion, device=None, amp_backend=\"native\", amp_config=None, grad_scaler=None):\n",
    "            super().__init__(model, optimizer, criterion, device)\n",
    "            self.amp_backend = amp_backend\n",
    "            self.amp_config = amp_config\n",
    "            self.grad_scaler = grad_scaler or torch.GradScaler()\n",
    "\n",
    "        def _train_batch(self, train_iter, accumulation_steps, non_blocking_transfer=True):\n",
    "            self.model.train()\n",
    "            total_loss = 0\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            for _ in range(accumulation_steps):\n",
    "                inputs, labels = next(train_iter)\n",
    "                inputs, labels = self._move_to_device(inputs, labels, non_blocking=non_blocking_transfer)\n",
    "\n",
    "                with torch.autocast(device_type=\"cuda:0\"):\n",
    "                    outputs = self.model(inputs, labels)\n",
    "                    loss = sum(loss for loss in outputs.values())\n",
    "\n",
    "                loss /= accumulation_steps\n",
    "                self.grad_scaler.scale(loss).backward()\n",
    "                total_loss += loss\n",
    "\n",
    "            self.grad_scaler.step(self.optimizer)\n",
    "            self.grad_scaler.update()\n",
    "\n",
    "            return total_loss.item()\n",
    "\n",
    "    lr_finder = CustomLRFinder(model, optimizer, None, device='cuda:0', amp_backend='torch', amp_config=None, grad_scaler=grad_scaler)\n",
    "    lr_finder.range_test(train_iter, end_lr=0.01, num_iter=100, step_mode='exp', accumulation_steps=accumulation_steps)\n",
    "    suggested_lr = lr_finder.plot(suggest_lr=True)\n",
    "\n",
    "    lr_finder.reset()\n",
    "\n",
    "    # Ensure consistent return value\n",
    "    try:\n",
    "        if isinstance(suggested_lr, tuple):\n",
    "            axes, suggested_lr_value = suggested_lr\n",
    "            return suggested_lr_value\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected return type from plot method: {type(suggested_lr)}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Error during learning rate finding: {e}\")\n",
    "        # Return a default learning rate if an error occurs\n",
    "        return 5e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Main tuning program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store indices in random order list for subsetting\n",
    "indices = torch.randperm(len(sample_ds)).tolist()\n",
    "\n",
    "def main(num_samples, max_num_epochs, restore_path=\"\"):\n",
    "    ray.shutdown()\n",
    "    ray.init()\n",
    "\n",
    "    dataset_train = ChestnutBurSegmentation(image_dir, df, get_transform(train = True))\n",
    "    dataset_val = ChestnutBurSegmentation(image_dir, df, get_transform(train = False))\n",
    "    dataset_test = ChestnutBurSegmentation(image_dir, df, get_transform(train = False))\n",
    "\n",
    "    dataset_train = Subset(dataset_train, \n",
    "                        indices[:-int(len(indices)*0.2)]) # first 80% of dataset for training                   \n",
    "\n",
    "    dataset_val = Subset(dataset_val, \n",
    "                                indices[-int(len(indices)*0.2):-int(len(indices)*0.05)]) # next 15% of dataset for validation                                \n",
    "\n",
    "    dataset_test = Subset(dataset_test, \n",
    "                                indices[-int(len(indices)*0.05):]) # last 5% of dataset for testing                            \n",
    "    \n",
    "    data_loader_val = torch.utils.data.DataLoader(\n",
    "        dataset_val, batch_size=1, shuffle=False,\n",
    "        collate_fn=ChestnutBurSegmentation.collate_fn, num_workers=0, pin_memory=True\n",
    "    )\n",
    "\n",
    "    data_loader_test = torch.utils.data.DataLoader(\n",
    "        dataset_test, batch_size=1, shuffle=False,\n",
    "        collate_fn=ChestnutBurSegmentation.collate_fn, num_workers=0, pin_memory=True\n",
    "    )\n",
    "\n",
    "    # Create COCO dataset objects for train, val, and test datasets\n",
    "    train_coco_ds, val_coco_ds, test_coco_ds = create_coco_datasets(dataset_train, dataset_val, dataset_test)\n",
    "\n",
    "    # Create ObjectRefs\n",
    "    dataset_train_ref = ray.put(dataset_train)\n",
    "    data_loader_val_ref = ray.put(data_loader_val)\n",
    "    data_loader_test_ref = ray.put(data_loader_test)\n",
    "    train_coco_ds_ref = ray.put(train_coco_ds)\n",
    "    val_coco_ds_ref = ray.put(val_coco_ds)\n",
    "    test_coco_ds_ref = ray.put(test_coco_ds)\n",
    "\n",
    "    config = {\n",
    "        \"lr\": tune.sample_from(lambda config: train_lr_finder(config)),\n",
    "        \"resnet\": tune.choice([18, 34, 50]),\n",
    "        \"momentum\": tune.uniform(0.4, 0.9),\n",
    "        \"weight_decay\": tune.loguniform(0.00001, 0.01),\n",
    "        \"step_size\": tune.choice([5, 10, 15]),\n",
    "        \"gamma_lr\": tune.uniform(0.1, 0.5),\n",
    "        \"backbone_lyrs\": tune.choice([2, 3, 4]),\n",
    "        \"batch_size\": tune.choice([2, 4, 8]),\n",
    "        \"dataset_train_ref\": dataset_train_ref,\n",
    "        \"data_loader_val_ref\": data_loader_val_ref,\n",
    "        \"data_loader_test_ref\": data_loader_test_ref,\n",
    "        \"train_coco_ds_ref\": train_coco_ds_ref,\n",
    "        \"val_coco_ds_ref\": val_coco_ds_ref,\n",
    "        \"test_coco_ds_ref\": test_coco_ds_ref\n",
    "    }\n",
    "\n",
    "    if tune.Tuner.can_restore(os.path.abspath(restore_path)):\n",
    "        tuner = tune.Tuner.restore(\n",
    "            os.path.abspath(restore_path),\n",
    "            trainable=ChestnutBurSegmentation,\n",
    "            param_space=config,  # pass same config with new ObjectRefs\n",
    "            resume_unfinished=True,\n",
    "            resume_errored=False\n",
    "        )\n",
    "        print(f\"Tuner Restored from {restore_path}\")\n",
    "    else:\n",
    "        algo = TuneBOHB(seed=666)  # set for identical initial configurations\n",
    "        \n",
    "        algo = ConcurrencyLimiter(algo, max_concurrent=1)\n",
    "\n",
    "        scheduler = HyperBandForBOHB(\n",
    "            time_attr=\"training_iteration\",\n",
    "            max_t=int(max_num_epochs),\n",
    "            reduction_factor=4,\n",
    "            stop_last_trials=False,\n",
    "        )\n",
    "\n",
    "        reporter = JupyterNotebookReporter(overwrite=True,\n",
    "            metric_columns=[\"epoch\", \"train_loss\", \"val_loss\", \"train_mAP\", \"val_mAP\", \"train_mAR\", \"val_mAR\", \"train_f1\", \"val_f1\"],\n",
    "            parameter_columns=[\"lr\", \"resnet\", \"momentum\", \"weight_decay\", \"step_size\", \"gamma_lr\", \"batch_size\", \"alpha\", \"gamma_loss\", \"backbone_lyrs\"],\n",
    "            print_intermediate_tables=True,\n",
    "            sort_by_metric=True\n",
    "        )\n",
    "\n",
    "        # Dictionary to store train_f1 scores for each trial\n",
    "        val_f1_history = defaultdict(list)\n",
    "\n",
    "        def custom_stop(trial_id, result):\n",
    "            # Ensure the required keys are in the result dictionary\n",
    "            required_keys = [\"training_iteration\", \"val_f1\"]\n",
    "            if all(key in result for key in required_keys):\n",
    "                # Append the current val_f1 score to the trial's history\n",
    "                val_f1_history[trial_id].append(result[\"val_f1\"])\n",
    "                \n",
    "                # Check if there are at least 5 epochs recorded\n",
    "                if len(val_f1_history[trial_id]) >= 5:\n",
    "                    # Calculate the improvement over the last 5 epochs\n",
    "                    initial_f1 = val_f1_history[trial_id][-5]\n",
    "                    current_f1 = val_f1_history[trial_id][-1]\n",
    "                    \n",
    "                    # Check if initial_f1 is zero\n",
    "                    if initial_f1 == 0:\n",
    "                        if current_f1 == 0:\n",
    "                            return True  # no improvement if both initial and current f1 are zero (stop)\n",
    "                        improvement = float('inf')  # avoid zero division\n",
    "                    else:\n",
    "                        improvement = (current_f1 - initial_f1) / initial_f1\n",
    "                    \n",
    "                    # Check if the improvement is less than 0.05%\n",
    "                    if improvement < 0.0005:\n",
    "                        return True # (stop)\n",
    "            return False\n",
    "\n",
    "        tuner = tune.Tuner(\n",
    "            tune.with_resources(\n",
    "                train_ChestnutBurSegmentation,\n",
    "                resources={\"cpu\": 24.0, \"gpu\": 1.0} \n",
    "            ),\n",
    "            run_config=train.RunConfig(\n",
    "                name=f\"BOHB_MaskRCNN_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
    "                failure_config=train.FailureConfig(max_failures=1),\n",
    "                stop=custom_stop,\n",
    "                progress_reporter=reporter,\n",
    "            ),\n",
    "            tune_config=tune.TuneConfig(\n",
    "                mode=\"max\",\n",
    "                metric=\"val_f1\",\n",
    "                search_alg=algo,\n",
    "                scheduler=scheduler,\n",
    "                num_samples=int(num_samples),\n",
    "                trial_dirname_creator=trial_dirname_creator\n",
    "            ),\n",
    "            param_space=config\n",
    "        )\n",
    "    results = tuner.fit()\n",
    "\n",
    "    best_trial = results.get_best_result(\"val_f1\", \"max\")\n",
    "\n",
    "    print(\"Best trial config: {}\".format(best_trial.config))\n",
    "    print()\n",
    "    print(\"Best trial final training loss: {}\".format(best_trial.metrics[\"train_loss\"]))\n",
    "    print(\"Best trial final validation loss: {}\".format(best_trial.metrics[\"val_loss\"]))\n",
    "    print(\"Best trial final training mAP: {}\".format(best_trial.metrics[\"train_mAP\"]))\n",
    "    print(\"Best trial final validation mAP: {}\".format(best_trial.metrics[\"val_mAP\"]))\n",
    "    print(\"Best trial final training mAR: {}\".format(best_trial.metrics[\"train_mAR\"]))\n",
    "    print(\"Best trial final validation mAR: {}\".format(best_trial.metrics[\"val_mAR\"]))\n",
    "    print(\"Best trial final training f1-score: {}\".format(best_trial.metrics[\"train_f1\"]))\n",
    "    print(\"Best trial final validation f1-score: {}\".format(best_trial.metrics[\"val_f1\"]))\n",
    "    print()\n",
    "\n",
    "    best_checkpoint = best_trial.get_best_checkpoint(metric=\"val_f1\", mode=\"max\")\n",
    "\n",
    "    test_best_model(best_trial, best_checkpoint)\n",
    "\n",
    "    return train_coco_ds, val_coco_ds, test_coco_ds, results, best_trial\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    train_coco_ds, val_coco_ds, test_coco_ds, results, best_trial = main(num_samples=30,\n",
    "                                                                         max_num_epochs=25,\n",
    "                                                                         restore_path=\"C:/Users/zack/ray_results/FALSE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train Model Using Tuned Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.profiler\n",
    "\n",
    "def main(train_coco_ds, val_coco_ds, best_trial):\n",
    "    # Set seed\n",
    "    set_seed(666)\n",
    "\n",
    "    training_steps = [\n",
    "        {\"step\": 0, \"batch_size\": best_trial.config[\"batch_size\"], \"epochs\": 10, \"print_freq\": 25, \"accumulation_steps\": 2, \"score_thresh\": 0.1, \"nms_thresh\": 0.5},\n",
    "        {\"step\": 1, \"batch_size\": best_trial.config[\"batch_size\"], \"epochs\": 10, \"print_freq\": 25, \"accumulation_steps\": 4, \"score_thresh\": 0.2, \"nms_thresh\": 0.4},\n",
    "        {\"step\": 2, \"batch_size\": best_trial.config[\"batch_size\"], \"epochs\": 5, \"print_freq\": 25, \"accumulation_steps\": 8, \"score_thresh\": 0.3, \"nms_thresh\": 0.3}\n",
    "    ]\n",
    "\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "\n",
    "    # Initialize the tensorboard writer\n",
    "    current_datetime = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    writer = SummaryWriter(log_dir=f'C:/Users/exx/Documents/GitHub/SSD_VGG_PyTorch/runs/RetinaNet/{current_datetime}')\n",
    "\n",
    "    # Store one checkpoint dictionary for each epoch in a list of dictionaries. \n",
    "    checkpoints = []\n",
    "\n",
    "    dataset = ChestnutBurSegmentation(image_dir, df, get_transform(train = True))\n",
    "    dataset_val = ChestnutBurSegmentation(image_dir, df, get_transform(train = False))\n",
    "\n",
    "    dataset = Subset(dataset, indices[:-int(len(indices)*0.2)]) # first 80% of dataset for training                   \n",
    "\n",
    "    dataset_val = Subset(dataset_val, indices[-int(len(indices)*0.2):-int(len(indices)*0.05)]) # next 15% of dataset for validation                                                      \n",
    "    \n",
    "    data_loader_val = torch.utils.data.DataLoader(\n",
    "        dataset_val, batch_size=1, shuffle=False,\n",
    "        collate_fn=ChestnutBurSegmentation.collate_fn, num_workers=0, pin_memory=True\n",
    "    )\n",
    "    \n",
    "    start_epoch = 0\n",
    "\n",
    "    # Loop through training_steps during training to increase batch size and decrease learning rate\n",
    "    for step in training_steps:\n",
    "        batch_size = step['batch_size']\n",
    "        num_epochs = step['epochs']\n",
    "        print_freq = step['print_freq']\n",
    "        accumulation_steps = step['accumulation_steps']\n",
    "        box_score_thresh = step['score_thresh']\n",
    "        box_nms_thresh = step['nms_thresh']\n",
    "\n",
    "        # Reinitialize the model with the current hyperparameters\n",
    "        model = get_instance_segmentation_model(depth=best_trial.config[\"resnet\"],\n",
    "                                                trainable_backbone_layers=best_trial.config[\"backbone_lyrs\"],\n",
    "                                                min_size=512,\n",
    "                                                max_size=None,\n",
    "                                                image_mean=0,\n",
    "                                                image_std=1,\n",
    "                                                num_classes=2, # background, chestnut bur\n",
    "                                                box_score_thresh=box_score_thresh,\n",
    "                                                box_nms_thresh=box_nms_thresh,\n",
    "                                                box_detections_per_img=1500) \n",
    "\n",
    "        model.to(device)\n",
    "\n",
    "        # Construct an optimizer with the suggested learning rate\n",
    "        params = [p for p in model.parameters() if p.requires_grad]\n",
    "        optimizer = torch.optim.SGD(params, lr=best_trial.config[\"lr\"],\n",
    "                                    momentum=best_trial.config[\"momentum\"], \n",
    "                                    weight_decay=best_trial.config[\"weight_decay\"])\n",
    "        \n",
    "        # And a learning rate scheduler\n",
    "        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                       step_size=best_trial.config[\"step_size\"],\n",
    "                                                       gamma=best_trial.config[\"gamma_lr\"])\n",
    "\n",
    "        # Define training and validation data loaders\n",
    "        data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, \n",
    "                                                  shuffle=True, \n",
    "                                                  collate_fn=ChestnutBurSegmentation.collate_fn, \n",
    "                                                  num_workers=0,\n",
    "                                                  pin_memory=True)\n",
    "        \n",
    "        print(f'Beginning training step {step[\"step\"]}... batch size: {batch_size * accumulation_steps}')\n",
    "\n",
    "        #########################################################\n",
    "        ##               The main training loop                ##\n",
    "        #########################################################\n",
    "        for epoch in range(start_epoch, num_epochs + start_epoch):\n",
    "            # Monitor memory usage at the start of the epoch\n",
    "            print(f\"Epoch {epoch} - Memory allocated: {torch.cuda.memory_allocated(device)} bytes\")\n",
    "\n",
    "            train_metric_logger, val_metric_logger = engine.train_one_epoch(model, optimizer, data_loader, device, \n",
    "                                                                            epoch, print_freq, accumulation_steps,\n",
    "                                                                            data_loader_val)\n",
    "\n",
    "            # Evaluate on the validation dataset\n",
    "            train_coco_evaluator, val_coco_evaluator = engine.evaluate(model, data_loader_val, val_coco_ds, device,\n",
    "                                                                        data_loader, train_coco_ds)\n",
    "            \n",
    "            # Update the learning rate\n",
    "            lr_scheduler.step()\n",
    "\n",
    "            # Store training and validation metrics in checkpoint dictionary. \n",
    "            checkpoint = {\n",
    "                \"epoch\": epoch,\n",
    "                \"train_loss\": train_metric_logger.loss.avg, # average across entire training epoch\n",
    "                \"val_loss\": val_metric_logger.loss.avg,\n",
    "                \"train_mAP\": train_coco_evaluator.coco_eval['segm'].stats[0],# IoU=0.50:0.95\n",
    "                \"train_mAR\": train_coco_evaluator.coco_eval['segm'].stats[8],# IoU=0.50:0.95\n",
    "                \"val_mAP\": val_coco_evaluator.coco_eval['segm'].stats[0],\n",
    "                \"val_mAR\": val_coco_evaluator.coco_eval['segm'].stats[8],\n",
    "                \"train_f1\": calculate_f1_score(train_coco_evaluator.coco_eval['segm'].stats[0], \n",
    "                                                train_coco_evaluator.coco_eval['segm'].stats[8]\n",
    "                                                ),\n",
    "                \"val_f1\": calculate_f1_score(val_coco_evaluator.coco_eval['segm'].stats[0],\n",
    "                                                val_coco_evaluator.coco_eval['segm'].stats[8]\n",
    "                                                ),\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict()\n",
    "            }\n",
    "\n",
    "            # Append checkpoint to checkpoints list\n",
    "            checkpoints.append(checkpoint)\n",
    "\n",
    "            # Report training and validation scalars to tensorboard\n",
    "            writer.add_scalar('Loss/Train', np.array(float(checkpoint[\"train_loss\"])), epoch) # use tags to group scalars\n",
    "            writer.add_scalar('Loss/Val', np.array(float(checkpoint[\"val_loss\"])), epoch)\n",
    "            writer.add_scalar('mAP/Train', np.array(float(checkpoint[\"train_mAP\"])), epoch)\n",
    "            writer.add_scalar('mAP/Val', np.array(float(checkpoint[\"val_mAP\"])), epoch)\n",
    "            writer.add_scalar('mAR/Train', np.array(float(checkpoint[\"train_mAR\"])), epoch)\n",
    "            writer.add_scalar('mAR/Val', np.array(float(checkpoint[\"val_mAR\"])), epoch)\n",
    "            writer.add_scalar('F1/Train', np.array(float(checkpoint[\"train_f1\"])), epoch)\n",
    "            writer.add_scalar('F1/Val', np.array(float(checkpoint[\"val_f1\"])), epoch)\n",
    "\n",
    "            # Clear CUDA cache and collect garbage \n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "            # Monitor memory usage at the end of the epoch\n",
    "            print(f\"Epoch {epoch} - Max memory allocated: {torch.cuda.max_memory_allocated(device)} bytes\")\n",
    "\n",
    "        # Set start_epoch to current epoch for next training step\n",
    "        start_epoch += num_epochs\n",
    "\n",
    "    print('All Training Steps Complete!')\n",
    "\n",
    "    # Close tensorboard writer\n",
    "    writer.close()\n",
    "\n",
    "    return checkpoints\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    checkpoints = main(train_coco_ds, val_coco_ds, best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best train epoch is dictionary in checkpoints with highest val_mAP_50 value\n",
    "best_train_epoch = max(checkpoints, key = lambda x: x['val_mAP_50'])\n",
    "\n",
    "model = get_instance_segmentation_model(depth=best_trial.config[\"resnet\"],\n",
    "                                                trainable_backbone_layers=best_trial.config[\"backbone_lyrs\"],\n",
    "                                                min_size=512,\n",
    "                                                max_size=None,\n",
    "                                                image_mean=0,\n",
    "                                                image_std=1,\n",
    "                                                num_classes=2, # background, chestnut bur\n",
    "                                                box_score_thresh=0.3, # from last training step\n",
    "                                                box_nms_thresh=0.3,\n",
    "                                                box_detections_per_img=1500)\n",
    "\n",
    "# load model weights from best_train_epoch\n",
    "model.load_state_dict(best_train_epoch[\"model_state_dict\"])\n",
    "\n",
    "# save model weights to .pth file\n",
    "torch.save(model.state_dict(), f\"C:/Users/zack/Documents/GitHub/Savanna-Institute/Drone-based_Chestnut_Detection/BurrSegmentation_MaskRCNN_{time.strftime('%Y%m%d')}_{time.strftime('%H%M%S')}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy checkpoints and remove model and optimizer state dicts\n",
    "checkpoints_copy = checkpoints.copy()\n",
    "for c in checkpoints_copy:\n",
    "    del c[\"model_state_dict\"]\n",
    "    del c[\"optimizer_state_dict\"]\n",
    "\n",
    "# save checkpoints list to text file\n",
    "with open(f\"C:/Users/zack/Documents/GitHub/Savanna-Institute/Drone-based_Chestnut_Detection/MaskRCNN_checkpoints_{time.strftime('%Y%m%d')}_{time.strftime('%H%M%S')}.txt\", 'w') as f:\n",
    "    for item in checkpoints_copy:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = ChestnutBurSegmentation(image_dir, df, get_transform(train = False))\n",
    "dataset_test = Subset(dataset_test, indices[-int(len(indices)*0.05):]) # last 5% of dataset for testing\n",
    "data_loader_test = torch.utils.data.DataLoader(dataset_test, batch_size=1, shuffle=False,\n",
    "                                                  collate_fn=ChestnutBurSegmentation.collate_fn, \n",
    "                                                  num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary of test indices (as key) and tree_ids (as value)\n",
    "test_dict = {}\n",
    "for i in range(len(dataset_test)):\n",
    "    test_dict[i] = dataset_test.tree_ids[i]\n",
    "\n",
    "\n",
    "# save test_dict to text file just to be safe\n",
    "with open(f\"C:/Users/zack/Documents/GitHub/Savanna-Institute/Drone-based_Chestnut_Detection/MaskRCNN_test_dict_{time.strftime('%Y%m%d')}_{time.strftime('%H%M%S')}.txt\", 'w') as f:\n",
    "    for key, value in test_dict.items():\n",
    "        f.write('%s:%s\\n' % (key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_performance = engine.evaluate(model, data_loader_test, test_coco_ds, device=torch.device('cpu'), train_data_loader=None, train_coco_ds=None)\n",
    "print(f'Best trial test set mAP_50: {test_performance.coco_eval[\"segm\"].stats[0]}') \n",
    "print(f'Best trial test set mAR_100: {test_performance.coco_eval[\"segm\"].stats[8]}')\n",
    "print(f'Best trial test set f1 score: {calculate_f1_score(test_performance.coco_eval[\"segm\"].stats[0], test_performance.coco_eval[\"segm\"].stats[8])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate performance metrics on every image in test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "\n",
    "results = []\n",
    "\n",
    "metric = MeanAveragePrecision(iou_type=\"bbox\",\n",
    "                              class_metrics=True,\n",
    "                              max_detection_thresholds=[100, 1000, 10000]\n",
    "                              )\n",
    "\n",
    "model.to('cpu')\n",
    "model.eval()\n",
    "\n",
    "for images, targets in data_loader_test:\n",
    "    # use image_id to get image_name from image_names list\n",
    "    image_id = [target['image_id'].item() for target in targets]\n",
    "\n",
    "    # convert boxes in targets to tensors\n",
    "    targets = [{k: torch.tensor(v) if k == 'boxes' else v for k, v in t.items()} for t in targets]\n",
    "\n",
    "    # filter targets to only include boxes and labels keys\n",
    "    ground_truth = [{k: v for k, v in t.items() if k in ('boxes', 'labels')} for t in targets]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prediction = model(images, targets)\n",
    "\n",
    "    # calculate mAP and mAR from test dataset\n",
    "    metric.update(prediction, ground_truth)\n",
    "    mean_AP = metric.compute()\n",
    "\n",
    "    # append image name to mean_AP\n",
    "    mean_AP['tree_id'] = test_dict[image_id[0]]\n",
    "\n",
    "    # Append mean_AP and predictions to results list. \n",
    "    results.append(mean_AP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Store per-image test dataset metrics as dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pandas to create a dataframe of image names and mAP values\n",
    "img_results_df = pd.DataFrame()\n",
    "img_results_df['tree_id'] = [result['tree_id'] for result in results]\n",
    "img_results_df['mAP'] = [result['map'].item() for result in results]\n",
    "img_results_df['mAP_50'] = [result['map_50'].item() for result in results]\n",
    "img_results_df['mAP_75'] = [result['map_75'].item() for result in results]\n",
    "img_results_df['mAP_small'] = [result['map_small'].item() for result in results]\n",
    "img_results_df['mAP_medium'] = [result['map_medium'].item() for result in results]\n",
    "img_results_df['mAP_large'] = [result['map_large'].item() for result in results]\n",
    "img_results_df['mAR_1'] = [result['mar_1'].item() for result in results]\n",
    "img_results_df['mAR_10'] = [result['mar_10'].item() for result in results]\n",
    "img_results_df['mAR_100'] = [result['mar_100'].item() for result in results]\n",
    "img_results_df['mAR_small'] = [result['mar_small'].item() for result in results]\n",
    "img_results_df['mAR_medium'] = [result['mar_medium'].item() for result in results]\n",
    "img_results_df['mAR_large'] = [result['mar_large'].item() for result in results]\n",
    "\n",
    "# # if value is == -1.0, replace with NaN\n",
    "img_results_df = img_results_df.replace(-1.0, np.nan)\n",
    "\n",
    "# Metric values are running averages in torch metrics, so the last value is the final value.\n",
    "final_metrics = img_results_df.iloc[-1]\n",
    "final_metrics = final_metrics.drop('tree_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Print per-image metrics for test dataset as table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "# create a pretty table object\n",
    "x = PrettyTable()\n",
    "\n",
    "cols = ['Metric', 'Value']  \n",
    "\n",
    "# add column headers\n",
    "x.field_names = cols\n",
    "\n",
    "# values for column one in table are column names from final_metrics, column two are the column values. \n",
    "for i in range(len(final_metrics)):\n",
    "    x.add_row([final_metrics.index[i], f'{final_metrics[i]*100:.2f}%'])\n",
    "\n",
    "# print table\n",
    "print(x)\n",
    "\n",
    "# save table as txt file\n",
    "with open(f\"C:/Users/zack/Documents/GitHub/Savanna-Institute/Drone-based_Chestnut_Detection/MaskRCNN_test_dataset_table_{time.strftime('%Y%m%d')}_{time.strftime('%H%M%S')}.txt\", 'w') as f:\n",
    "    print(x, file = f)\n",
    "\n",
    "# save results_df to csv\n",
    "img_results_df.to_csv(f\"C:/Users/zack/Documents/GitHub/Savanna-Institute/Drone-based_Chestnut_Detection/MaskRCNN_test_dataset_results_{time.strftime('%Y%m%d')}_{time.strftime('%H%M%S')}.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load test dataset into single batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load entire test dataset into one batch\n",
    "data_loader_test_singleBatch = torch.utils.data.DataLoader(dataset_test, batch_size = len(dataset_test), shuffle = False,\n",
    "                                                collate_fn = ChestnutBurSegmentation.collate_fn, num_workers = 0)\n",
    "\n",
    "# run predictions on all images in the test dataset\n",
    "images, targets = next(iter(data_loader_test_singleBatch))\n",
    "\n",
    "images = list(image for image in images)\n",
    "targets = [{k: v for k, v in t.items()} for t in targets]\n",
    "\n",
    "# convert boxes in targets to tensors\n",
    "targets = [{k: torch.tensor(v) if k == 'boxes' else v for k, v in t.items()} for t in targets]\n",
    "\n",
    "model.to('cpu')\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = model(images, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Post-process model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each image in the batch, remove all predicted boxes with scores below 0.5\n",
    "for i in range(len(predictions)):\n",
    "    predictions[i]['boxes'] = predictions[i]['boxes'][predictions[i]['scores'] > 0.5]\n",
    "    predictions[i]['labels'] = predictions[i]['labels'][predictions[i]['scores'] > 0.5]\n",
    "    predictions[i]['scores'] = predictions[i]['scores'][predictions[i]['scores'] > 0.5]\n",
    "\n",
    "# resize boxes to original image shape\n",
    "for i in range(len(images)):\n",
    "    tran_w, tran_h = images[i].shape[1], images[i].shape[2]\n",
    "    \n",
    "    images[i] = Image.open(image_dir + test_dict[i] + \".png\")\n",
    "\n",
    "    orig_w, orig_h = images[i].size\n",
    "\n",
    "    predictions[i]['boxes'] = predictions[i]['boxes'] * torch.tensor([orig_w/tran_w, \n",
    "                                                                      orig_h/tran_h, \n",
    "                                                                      orig_w/tran_w,\n",
    "                                                                      orig_h/tran_h]).view(1, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot model predictions for images in test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {0: 'background', 1: 'chestnut bur'}\n",
    "label_color_map = {0: 'black', 1: 'red'}\n",
    "\n",
    "def plot_bbox_predicted(ax, box, label, score, mask=None):\n",
    "    # Add box to the image and use label_color_map to color-code by bounding box class if exists else 'black'\n",
    "    ax.add_patch(plt.Rectangle((box[0], box[1]), box[2] - box[0], box[3] - box[1],\n",
    "                               fill=False,\n",
    "                               color=label_color_map[label.item()] if label.item() in label_color_map else 'black', \n",
    "                               linewidth=1.5))\n",
    "    \n",
    "    # Add label and score to the bounding box. Concatenate label and score to one string. \n",
    "    # Use label_dict to replace class numbers with class names\n",
    "    ax.text(box[0], box[1] - 10,\n",
    "            s=f\"{label_dict[label.item()]} {score.item():.2f}\",\n",
    "            color='black',\n",
    "            fontsize=6,\n",
    "            verticalalignment='top',\n",
    "            bbox={'color': label_color_map[label.item()] if label.item() in label_color_map else 'black', 'pad': 0})\n",
    "    \n",
    "    # Plot the mask if provided\n",
    "    if mask is not None:\n",
    "        mask = mask.cpu().numpy()\n",
    "        mask = np.ma.masked_where(mask == 0, mask)\n",
    "        ax.imshow(mask, alpha=0.5, cmap='jet')\n",
    "\n",
    "    return ax\n",
    "\n",
    "# function for plotting image\n",
    "def img_show(image, ax = None, figsize = (6, 9)):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize = figsize)\n",
    "    ax.xaxis.tick_top()\n",
    "    ax.imshow(image)\n",
    "    return ax\n",
    "\n",
    "# Function for plotting all predictions on images\n",
    "def plot_predictions(image, boxes, labels, scores, masks=None, ax=None):\n",
    "    ax = img_show(image, ax=ax)\n",
    "    for i in range(len(boxes)):\n",
    "        box = boxes[i].cpu().numpy()\n",
    "        mask = masks[i] if masks is not None else None\n",
    "        plot_bbox_predicted(ax, box, labels[i], scores[i], mask)\n",
    "\n",
    "# Plot batch of images and predictions\n",
    "plt.figure(figsize=(24, 36))\n",
    "for i in range(0, len(dataset_test)):\n",
    "    ax = plt.subplot(8, 4, 1 + i)\n",
    "    plot_predictions(images[i], predictions[i]['boxes'], predictions[i]['labels'], predictions[i]['scores'], predictions[i]['masks'], ax=ax)\n",
    "    tree_id = dataset_test.tree_ids[i]  # Assuming dataset_test is available and has tree_ids\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Tree ID: {tree_id}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run inference on full dataset to calculate confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_all = ChestnutBurSegmentation(image_dir, df, get_transform(train = False))\n",
    "data_loader_all = torch.utils.data.DataLoader(dataset_all, batch_size=1, shuffle=False,\n",
    "                                                  collate_fn=ChestnutBurSegmentation.collate_fn, \n",
    "                                                  num_workers=0, pin_memory=True)\n",
    "\n",
    "# get model predictions for every image in data_loader_all\n",
    "model_predictions_all = []\n",
    "\n",
    "for images, targets in data_loader_all:\n",
    "    # use image_id to get image_name from image_names list\n",
    "    image_id = [target['image_id'].item() for target in targets]\n",
    "\n",
    "    # convert boxes in targets to tensors\n",
    "    targets = [{k: torch.tensor(v) if k == 'boxes' else v for k, v in t.items()} for t in targets]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prediction = model(images, targets)\n",
    "\n",
    "    # append image name to prediction\n",
    "    prediction['tree_id'] = test_dict[image_id[0]]\n",
    "\n",
    "    # Append mean_AP and predictions to results list. \n",
    "    model_predictions_all.append(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert model_predictions_all to a dataframe\n",
    "model_predictions_df = pd.DataFrame(model_predictions_all)\n",
    "\n",
    "# save csv for comparison with ground truth\n",
    "model_predictions_df.to_csv(f\"C:/Users/zack/Documents/GitHub/Savanna-Institute/Drone-based_Chestnut_Detection/MaskRCNN_full_dataset_results_{time.strftime('%Y%m%d')}_{time.strftime('%H%M%S')}.csv\", index = False), index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bohb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
