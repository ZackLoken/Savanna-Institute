{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Chestnut Bur Detection and Segmentation using MaskRCNN in PyTorch</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, Subset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms as _transforms, tv_tensors\n",
    "import torchvision.transforms.v2 as T\n",
    "from torchvision.transforms import functional as F\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import gc\n",
    "import os\n",
    "from PIL import Image, ImageDraw, ImageOps\n",
    "\n",
    "from segmentation_pytorch import engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <center> Load the image and annotation data </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load annotations from json file\n",
    "# annos = json.load(open(\"C:/Users/exx/Deep Learning/Chestnut_Bur_Instance_Segmentation/route9_orchard3/data/_annotations.coco.json\"))\n",
    "annos = json.load(open(\"S:/Zack/Deep Learning/Chestnut_Bur_Instance_Segmentation/route9_orchard3/data/_annotations.coco.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the annos dict to a df\n",
    "annos_df = pd.DataFrame(annos[\"annotations\"])\n",
    "df = pd.DataFrame()\n",
    "df[\"tree_id\"] = annos_df[\"image_id\"].apply(lambda x: annos[\"images\"][x][\"file_name\"].split(\"_\")[0])\n",
    "df[\"file_name\"] = annos_df[\"image_id\"].apply(lambda x: annos[\"images\"][x][\"file_name\"])\n",
    "df[\"file_name\"] = df[\"file_name\"].apply(lambda x: x.split(\"_\")[0] + \".png\")\n",
    "categories = [cat[\"name\"] for cat in annos[\"categories\"]]\n",
    "df[\"category_name\"] = annos_df[\"category_id\"].apply(lambda x: categories[x])\n",
    "df[\"bbox\"] = annos_df[\"bbox\"].apply(lambda x: torch.tensor(x))\n",
    "df[\"area\"] = annos_df[\"area\"].apply(lambda x: torch.tensor(x))\n",
    "df[\"segmentation\"] = annos_df[\"segmentation\"].apply(lambda x: torch.tensor(x))\n",
    "df[\"iscrowd\"] = annos_df[\"iscrowd\"]\n",
    "\n",
    "# Filter df to remove treeIds not reviewed. \n",
    "reviewed_trees = [\n",
    "    14, 44, 51, 60, 79, 91, 92, 117, 118, 146, 152, 171, \n",
    "    172, 210, 272, 276, 280, 286, 304, 309, 320, 329, \n",
    "    366, 369, 371, 392, 394, 405\n",
    "]\n",
    "\n",
    "df = df[df[\"tree_id\"].isin([str(tree_id) for tree_id in reviewed_trees])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = \"S:/Zack/Deep Learning/Chestnut_Bur_Instance_Segmentation/route9_orchard3/data/images\"\n",
    "image_names = df[\"file_name\"].unique()\n",
    "# mask_dir = \"C:/Users/exx/Deep Learning/Chestnut_Bur_Instance_Segmentation/route9_orchard3/data/masks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot each image with its canopy polygon mask and chestnut polygon masks\n",
    "\n",
    "# os.makedirs(\"C:/Users/exx/Deep Learning/Chestnut_Bur_Instance_Segmentation/route9_orchard3/annotated_images\", exist_ok=True)\n",
    "\n",
    "# for i, tree_id in enumerate(df[\"tree_id\"].unique()):\n",
    "#     image_file = Path(image_dir) / df[df[\"tree_id\"] == tree_id][\"file_name\"].values[0]\n",
    "#     image = Image.open(image_file)\n",
    "#     # get image width and height\n",
    "#     width, height = image.size\n",
    "\n",
    "#     # print(f\"tree {tree_id} has a width of {width} and height of {height} pixels\")\n",
    "\n",
    "#     canopy_poly = df[(df[\"tree_id\"] == tree_id) & (df[\"category_name\"] == \"Canopy\")][\"segmentation\"].values\n",
    "#     bur_poly = df[(df[\"tree_id\"] == tree_id) & (df[\"category_name\"] == \"Chestnut-burr\")][\"segmentation\"].values\n",
    "\n",
    "#     canopy_poly = [np.array(poly[0]).reshape(-1, 2).astype(np.int32) for poly in canopy_poly]\n",
    "#     bur_poly = [np.array(poly[0]).reshape(-1, 2).astype(np.int32) for poly in bur_poly]\n",
    "\n",
    "#     # Create a drawing context\n",
    "#     draw = ImageDraw.Draw(image)\n",
    "    \n",
    "#     for poly in canopy_poly:\n",
    "#         draw.polygon([tuple(point) for point in poly], outline=\"purple\", width=2)\n",
    "#     for poly in bur_poly:\n",
    "#         draw.polygon([tuple(point) for point in poly], outline=\"red\")\n",
    "\n",
    "#     # Save the image\n",
    "#     image.save(f\"C:/Users/exx/Deep Learning/Chestnut_Bur_Instance_Segmentation/route9_orchard3/annotated_images/{tree_id}_annotated.png\", format='PNG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <center> Pre-process and transform image and annotation data </center>\n",
    "\n",
    "##### Adapted from: https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html#an-instance-segmentation-model-for-pennfudan-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_fill(mask, polys, color):\n",
    "    from cv2 import fillPoly\n",
    "    for poly in polys:\n",
    "        fillPoly(mask, [poly], color)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChestnutBurSegmentation(Dataset):\n",
    "    \"\"\"Custom Dataset for Chestnut Bur Segmentation in UAV Images\"\"\"\n",
    "\n",
    "    def __init__(self, image_dir, df, transform=None, tile_size=512):\n",
    "        self.image_dir = image_dir\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        self.tile_size = tile_size\n",
    "        self.tiles = self.create_tile_indices()\n",
    "\n",
    "    def create_tile_indices(self):\n",
    "        tile_indices = []\n",
    "        for tree_id in self.df[\"tree_id\"].unique():\n",
    "            canopy_poly = self.df[(self.df[\"tree_id\"] == tree_id) & (self.df[\"category_name\"] == \"Canopy\")][\"segmentation\"].values\n",
    "            canopy_poly = [np.array(poly[0]).reshape(-1, 2).astype(np.int32) for poly in canopy_poly]\n",
    "\n",
    "            # get canopy_bbox coords [xmin, ymin, xmax, ymax]\n",
    "            xmin, ymin, xmax, ymax = min([poly[:, 0].min() for poly in canopy_poly]), min([poly[:, 1].min() for poly in canopy_poly]), max([poly[:, 0].max() for poly in canopy_poly]), max([poly[:, 1].max() for poly in canopy_poly])\n",
    "\n",
    "            # Calculate the number of tiles for the clipped image\n",
    "            clipped_height = ymax - ymin\n",
    "            clipped_width = xmax - xmin\n",
    "            num_tiles_y = (clipped_height + self.tile_size - 1) // self.tile_size\n",
    "            num_tiles_x = (clipped_width + self.tile_size - 1) // self.tile_size\n",
    "\n",
    "            for tile_y in range(num_tiles_y):\n",
    "                for tile_x in range(num_tiles_x):\n",
    "                    tile_indices.append((tree_id, tile_y, tile_x, xmin, ymin, xmax, ymax))\n",
    "        return tile_indices\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tree_id, tile_y, tile_x, xmin, ymin, xmax, ymax = self.tiles[idx]\n",
    "        row = self.df[self.df[\"tree_id\"] == tree_id].iloc[0]\n",
    "        image_file = Path(self.image_dir) / row[\"file_name\"]\n",
    "\n",
    "        # Calculate the tile coordinates\n",
    "        tile_xmin = xmin + tile_x * self.tile_size\n",
    "        tile_ymin = ymin + tile_y * self.tile_size\n",
    "        tile_xmax = min(tile_xmin + self.tile_size, xmax)\n",
    "        tile_ymax = min(tile_ymin + self.tile_size, ymax)\n",
    "\n",
    "        # Load only the required tile\n",
    "        with Image.open(image_file) as img:\n",
    "            img_tile = img.crop((tile_xmin, tile_ymin, tile_xmax, tile_ymax)).convert(\"RGB\")\n",
    "            img_tile = np.array(img_tile)\n",
    "\n",
    "        # Pad the tile if necessary\n",
    "        pad_height = self.tile_size - (tile_ymax - tile_ymin)\n",
    "        pad_width = self.tile_size - (tile_xmax - tile_xmin)\n",
    "\n",
    "        img_tile = np.pad(img_tile, ((0, pad_height), (0, pad_width), (0, 0)), mode='constant', constant_values=0)\n",
    "\n",
    "        # Filter the DataFrame once for the tree_id\n",
    "        tree_df = self.df[self.df[\"tree_id\"] == tree_id]\n",
    "        canopy_poly = tree_df[tree_df[\"category_name\"] == \"Canopy\"][\"segmentation\"].values\n",
    "        bur_poly = tree_df[tree_df[\"category_name\"] == \"Chestnut-burr\"][\"segmentation\"].values\n",
    "\n",
    "        canopy_poly = [np.array(poly[0]).reshape(-1, 2).astype(np.int32) for poly in canopy_poly]\n",
    "        bur_poly = [np.array(poly[0]).reshape(-1, 2).astype(np.int32) for poly in bur_poly]\n",
    "\n",
    "        # Create the canopy mask\n",
    "        canopy_mask = np.zeros((self.tile_size, self.tile_size), dtype=np.uint8)\n",
    "        canopy_poly_tile = [poly - [tile_xmin, tile_ymin] for poly in canopy_poly]\n",
    "        canopy_mask_tile = mask_fill(canopy_mask, canopy_poly_tile, 1)\n",
    "\n",
    "        # Check if the canopy mask covers a minimum area (e.g., 5% of the tile area)\n",
    "        min_coverage_area = 0.05 * self.tile_size * self.tile_size\n",
    "        if np.sum(canopy_mask) < min_coverage_area:\n",
    "            return None  # Exclude tiles with insufficient canopy coverage\n",
    "\n",
    "        # Apply canopy mask to the image\n",
    "        img_tile = img_tile * canopy_mask_tile[..., np.newaxis]\n",
    "\n",
    "        # Change the background color to pink\n",
    "        img_tile[canopy_mask == 0] = [255, 105, 180] # pink RGB\n",
    "\n",
    "        # Create the masks for the tile\n",
    "        bur_masks = []\n",
    "        for poly in bur_poly:\n",
    "            # Create the bur mask with the same coordinates as the tile\n",
    "            bur_mask = np.zeros((self.tile_size, self.tile_size), dtype=np.uint8)\n",
    "            bur_mask = mask_fill(bur_mask, [poly - [tile_xmin, tile_ymin]], 1)\n",
    "            bur_mask_tile = bur_mask[:tile_ymax - tile_ymin, :tile_xmax - tile_xmin]\n",
    "\n",
    "            # Pad the bur mask if necessary\n",
    "            bur_mask_tile = np.pad(bur_mask_tile, ((0, pad_height), (0, pad_width)), mode='constant', constant_values=0)\n",
    "            bur_masks.append(bur_mask_tile)\n",
    "\n",
    "        # Check if the tile contains any objects (chestnut burs)\n",
    "        if not bur_masks:\n",
    "            return None  # Exclude empty tiles\n",
    "\n",
    "        # Stack the bur masks\n",
    "        mask_image = np.stack(bur_masks, axis=0)\n",
    "\n",
    "        # store mask image as tensor and transpose to (C, H, W)\n",
    "        mask_image.transpose(1, 2, 0)\n",
    "        mask_image = tv_tensors.Mask(mask_image)\n",
    "\n",
    "        # Filter the target features for the tile\n",
    "        labels = tree_df[tree_df[\"category_name\"] == \"Chestnut-burr\"][\"category_name\"].values\n",
    "        labels = [categories.index(label) for label in labels]\n",
    "        bboxes = tree_df[tree_df[\"category_name\"] == \"Chestnut-burr\"][\"bbox\"].values\n",
    "        bboxes = [torch.tensor([bbox[0], bbox[1], bbox[0] + bbox[2], bbox[1] + bbox[3]], dtype=torch.float32) for bbox in bboxes]\n",
    "        bboxes = torch.stack([bbox for bbox in bboxes], dim=0)\n",
    "        area = tree_df[tree_df[\"category_name\"] == \"Chestnut-burr\"][\"area\"].values\n",
    "        iscrowd = tree_df[tree_df[\"category_name\"] == \"Chestnut-burr\"][\"iscrowd\"].values\n",
    "\n",
    "        # Adjust bounding boxes and labels for the tile\n",
    "        bbox_tile, label_tile, area_tile, iscrowd_tile, mask_tile = self.filter_bboxes_and_masks_for_tile(bboxes, labels, area, iscrowd, mask_image, tile_ymin, tile_xmin)\n",
    "\n",
    "        # Process the tile\n",
    "        img_tile = T.Compose([T.ToImage(), T.ToDtype(torch.float32, scale=True),\n",
    "                              T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])(img_tile)\n",
    "\n",
    "        # Create new image_id for the tile\n",
    "        image_id_tile = f\"{tree_id}_tile{tile_y}_{tile_x}\"  # Keep image_id as a string\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": bbox_tile,\n",
    "            \"labels\": label_tile,\n",
    "            \"image_id\": image_id_tile,\n",
    "            \"area\": area_tile,\n",
    "            \"iscrowd\": iscrowd_tile,\n",
    "            \"masks\": mask_tile\n",
    "        }\n",
    "\n",
    "        return img_tile, target\n",
    "\n",
    "    def filter_bboxes_and_masks_for_tile(self, bboxes, labels, areas, iscrowds, mask_tile, i, j):\n",
    "        \"\"\"Filter bounding boxes, labels, areas, iscrowds, and masks for a tile.\"\"\"\n",
    "        bbox_tile = []\n",
    "        label_tile = []\n",
    "        area_tile = []\n",
    "        iscrowd_tile = []\n",
    "        mask_tile_list = []\n",
    "\n",
    "        for idx, (bbox, label, area, iscrowd) in enumerate(zip(bboxes, labels, areas, iscrowds)):\n",
    "            x_min, y_min, x_max, y_max = bbox\n",
    "            if (x_min < j + self.tile_size and x_max > j and y_min < i + self.tile_size and y_max > i):\n",
    "                # Adjust coordinates to be relative to the tile\n",
    "                x_min = max(x_min - j, 0)\n",
    "                x_max = min(x_max - j, self.tile_size)\n",
    "                y_min = max(y_min - i, 0)\n",
    "                y_max = min(y_max - i, self.tile_size)\n",
    "                bbox_tile.append([x_min, y_min, x_max, y_max])\n",
    "                label_tile.append(label)\n",
    "                area_tile.append(area)\n",
    "                iscrowd_tile.append(iscrowd)\n",
    "                mask_tile_list.append(mask_tile[idx])  # Use the mask directly\n",
    "\n",
    "        if bbox_tile:\n",
    "            bbox_tile = torch.tensor(bbox_tile, dtype=torch.float32)\n",
    "            label_tile = torch.tensor(label_tile, dtype=torch.int64)\n",
    "            area_tile = torch.tensor(area_tile, dtype=torch.float32)\n",
    "            iscrowd_tile = torch.tensor(iscrowd_tile, dtype=torch.int64)\n",
    "            mask_tile = torch.stack(mask_tile_list)\n",
    "        else:\n",
    "            bbox_tile = torch.zeros((0, 4), dtype=torch.float32)\n",
    "            label_tile = torch.zeros((0,), dtype=torch.int64)\n",
    "            area_tile = torch.zeros((0,), dtype=torch.float32)\n",
    "            iscrowd_tile = torch.zeros((0,), dtype=torch.int64)\n",
    "            mask_tile = torch.zeros((0, self.tile_size, self.tile_size), dtype=torch.float32)\n",
    "\n",
    "        return bbox_tile, label_tile, area_tile, iscrowd_tile, mask_tile\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tiles)\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        batch = list(filter(lambda x: x is not None, batch))  # Exclude empty tiles\n",
    "        return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define transforms for the dataset\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    \n",
    "    if train:\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    transforms.append(T.ClampBoundingBoxes()) # for segmentations too\n",
    "    transforms.append(T.SanitizeBoundingBoxes()) ## for segmentations too\n",
    "\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ds = ChestnutBurSegmentation(image_dir, df, get_transform(train = False))\n",
    "print(len(sample_ds))\n",
    "print()\n",
    "sample_dl = DataLoader(sample_ds, \n",
    "                       batch_size = len(sample_ds), \n",
    "                       shuffle = True, \n",
    "                       collate_fn = ChestnutBurSegmentation.collate_fn)\n",
    "print(len(sample_dl))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, targets = next(iter(sample_dl))\n",
    "images = [img for img in images]\n",
    "targets = [{k: v for k, v in target.items()} for target in targets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <center> Plot sample transformed images, targets, and masks </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def plot_images_by_tree(images, targets, output_dir=\"S:/Zack/Deep Learning/Chestnut_Bur_Instance_Segmentation/route9_orchard3/data/transformed_image_checks\"):\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Group images and targets by tree ID\n",
    "    tree_dict = {}\n",
    "    for image, target in zip(images, targets):\n",
    "        tree_id = target[\"image_id\"].split('_')[0]\n",
    "        if tree_id not in tree_dict:\n",
    "            tree_dict[tree_id] = {\"images\": [], \"targets\": []}\n",
    "        tree_dict[tree_id][\"images\"].append(image)\n",
    "        tree_dict[tree_id][\"targets\"].append(target)\n",
    "\n",
    "    # Plot images, masks, and image with mask overlay for each tree\n",
    "    for tree_id, data in tree_dict.items():\n",
    "        images = data[\"images\"]\n",
    "        targets = data[\"targets\"]\n",
    "        fig, ax = plt.subplots(len(images), 3, figsize=(15, 5*len(images)))\n",
    "        fig.suptitle(f\"Tree {tree_id} Tiles\", fontsize=16)\n",
    "        for i, (image, target) in enumerate(zip(images, targets)):\n",
    "            # Check if the image and mask have the expected dimensions\n",
    "            if image.ndimension() != 3 or target[\"masks\"].ndimension() != 3:\n",
    "                print(f\"Skipping invalid sample for Tree ID: {tree_id}, Tile: {i}\")\n",
    "                print(f\"Image shape: {image.shape}, Mask shape: {target['masks'].shape}\")\n",
    "                continue\n",
    "\n",
    "            image = image.permute(1, 2, 0).cpu().numpy()\n",
    "            mask = target[\"masks\"].permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "            # Clip the image data to the valid range [0, 1] for floats\n",
    "            image = np.clip(image, 0, 1)\n",
    "\n",
    "            ax[i, 0].imshow(image)\n",
    "            ax[i, 0].set_title(f\"Tile {i} Transformed Image\")\n",
    "            ax[i, 1].imshow(mask.sum(axis=2), cmap=\"gray\")\n",
    "            ax[i, 1].set_title(f\"Tile {i} Chestnut Bur Masks\")\n",
    "            ax[i, 2].imshow(image)\n",
    "            ax[i, 2].imshow(mask.sum(axis=2), cmap='gray', alpha=0.75)\n",
    "            ax[i, 2].set_title(f\"Tile {i} Training Sample\")\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "        plt.savefig(os.path.join(output_dir, f\"tree_{tree_id}_tiles.png\"))\n",
    "        plt.close(fig)\n",
    "\n",
    "# Example usage\n",
    "plot_images_by_tree(images, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save each transformed image with its canopy mask and chestnut polygon masks to a folder. Name image treeID.png\n",
    "\n",
    "# os.makedirs(\"C:/Users/exx/Deep Learning/Chestnut_Bur_Instance_Segmentation/route9_orchard3/transformed_images\", exist_ok=True)\n",
    "\n",
    "# for i, (image, target) in enumerate(zip(images, targets)):\n",
    "\n",
    "#     tree_id = target[\"image_id\"]\n",
    "#     image = image.permute(1, 2, 0)\n",
    "#     mask = target[\"masks\"].permute(1, 2, 0)\n",
    "\n",
    "#     #plot mask on image and save to .png file\n",
    "#     plt.imshow(image)\n",
    "#     plt.imshow(mask.sum(axis=2), cmap='gray', alpha=0.75)\n",
    "#     plt.savefig(f\"C:/Users/exx/Deep Learning/Chestnut_Bur_Instance_Segmentation/route9_orchard3/transformed_images/{tree_id}_transformed.png\")\n",
    "#     plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <center> Construct MaskRCNN Model </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_instance_segmentation_model(num_classes):\n",
    "    # Load a Mask R-CNN instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(\n",
    "        weights=torchvision.models.detection.MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,\n",
    "        weights_backbone=torchvision.models.ResNet50_Weights.DEFAULT\n",
    "    )\n",
    "\n",
    "    # Get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "    # Replace the pre-trained head with a new one to reflect the number of classes\n",
    "    model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # Get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "\n",
    "    # Replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = torchvision.models.detection.mask_rcnn.MaskRCNNPredictor(\n",
    "        in_features_mask,\n",
    "        hidden_layer,\n",
    "        num_classes\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_instance_segmentation_model(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <center> Tune model hyperparameters using bayesian optimization algo and hyperband scheduler </center>\n",
    "\n",
    "##### adapted from: https://docs.ray.io/en/latest/tune/examples/bohb_example.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian Optimization HyperBand (BOHB) with HyperBand scheduler\n",
    "# # https://proceedings.mlr.press/v80/falkner18a.html\n",
    "\n",
    "import tempfile\n",
    "import time\n",
    "import ray\n",
    "from ray import train, tune\n",
    "from ray.tune.schedulers.hb_bohb import HyperBandForBOHB\n",
    "from ray.tune.search.bohb import TuneBOHB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ChestnutBurSegmentation(search_space, indices):\n",
    "    train_ds = ray.get(search_space[\"train_ds_ref\"])\n",
    "    val_dl = ray.get(search_space[\"val_dl_ref\"])\n",
    "\n",
    "    model = get_instance_segmentation_model(num_classes = 2) # background, chestnut bur\n",
    "    \n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model = nn.DataParallel(model) # train on multiple gpus if available\n",
    "    model.to(device)\n",
    "\n",
    "    # construct an optimizer\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, \n",
    "                                lr = search_space[\"lr\"], \n",
    "                                momentum = search_space[\"momentum\"], \n",
    "                                weight_decay = search_space[\"weight_decay\"])\n",
    "\n",
    "    warmup_factor = 1.0 / 1000\n",
    "    warmup_iters = min(1000, int(len(indices)*0.8) - 1)\n",
    "\n",
    "    # construct a learning rate scheduler\n",
    "    lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "            optimizer, start_factor=warmup_factor, total_iters=warmup_iters\n",
    "        )\n",
    "    \n",
    "    # load existing checkpoint if available\n",
    "    if train.get_checkpoint():\n",
    "        loaded_checkpoint = train.get_checkpoint()\n",
    "        with loaded_checkpoint.as_directory() as loaded_checkpoint_dir:\n",
    "            model_state, optimizer_state = torch.load(\n",
    "                os.path.join(loaded_checkpoint_dir, \"checkpoint.pt\")\n",
    "            )\n",
    "            model.load_state_dict(model_state)\n",
    "            optimizer.load_state_dict(optimizer_state)\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "\n",
    "    # load data from object store references\n",
    "    train_dl = DataLoader(train_ds, # first 80% of dataset for training\n",
    "                           batch_size = search_space[\"batch_size\"],\n",
    "                           shuffle = True,\n",
    "                           collate_fn = ChestnutBurSegmentation.collate_fn,\n",
    "                           num_workers=0, \n",
    "                           pin_memory=True)\n",
    "\n",
    "    print_freq = 2\n",
    "\n",
    "    # Main training function\n",
    "    for epoch in range(start_epoch, search_space[\"epochs\"]):\n",
    "        # train for one epoch, printing every 10 iterations\n",
    "        train_logger, val_logger = engine.train_one_epoch(model, optimizer, train_dl, device, epoch, print_freq, val_dl, scaler=None)\n",
    "        # update the learning rate\n",
    "        lr_scheduler.step()\n",
    "        # evaluate on the test dataset\n",
    "        train_metrics, val_metrics = engine.evaluate(model, val_dl, device, train_dl)\n",
    "\n",
    "        # save checkpoint\n",
    "         # Here we save a checkpoint. It is automatically registered with Ray Tune\n",
    "        with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n",
    "            path = os.path.join(temp_checkpoint_dir, \"checkpoint.pt\")\n",
    "            torch.save(\n",
    "                (model.state_dict(), optimizer.state_dict()), path\n",
    "            )\n",
    "            checkpoint = train.Checkpoint.from_directory(temp_checkpoint_dir)\n",
    "            train.report(\n",
    "                {\"train_loss\": train_logger.loss.avg, # train loss,\n",
    "                \"val_loss\": val_logger.loss.avg, # val loss\n",
    "                \"train_mAP_50\": train_metrics.coco_eval['segm'].stats[1], # train mAP@50\n",
    "                \"val_mAP_50\": val_metrics.coco_eval['segm'].stats[1], # val mAP@50\n",
    "                \"train_mAR_100\": train_metrics.coco_eval['segm'].stats[8], # train mAR@100\n",
    "                \"val_mAR_100\": val_metrics.coco_eval['segm'].stats[8], # val mAR@100\n",
    "                \"epoch\": epoch}, \n",
    "                checkpoint = checkpoint\n",
    "            )\n",
    "    \n",
    "    print(\"Tuning Trial Complete!\")\n",
    "\n",
    "\n",
    "def test_best_model(best_trial, indices, image_dir, df):\n",
    "    best_model = get_instance_segmentation_model(num_classes = 2) # background, chestnut bur\n",
    "\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    best_model.to(device)\n",
    "\n",
    "    checkpoint_path = os.path.join(best_trial.checkpoint.to_directory(), \"checkpoint.pt\")\n",
    "\n",
    "    model_state, _ = torch.load(checkpoint_path)\n",
    "    best_model.load_state_dict(model_state)\n",
    "\n",
    "    test_dl = ray.get(best_trial.config[\"test_dl_ref\"])\n",
    "\n",
    "    test_results = engine.evaluate(best_model, test_dl, device, train_data_loader=None)\n",
    "\n",
    "    print(f'Best trial test set mAP_50: {test_results.coco_eval[\"segm\"].stats[1]} and mAR_100: {test_results.coco_eval[\"segm\"].stats[8]}')\n",
    "\n",
    "    return test_results\n",
    "\n",
    "def trial_dirname_creator(trial):\n",
    "    return f\"train_MAVdroneDataset_{trial.trial_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# store indices in random order list for subsetting\n",
    "indices = torch.randperm(len(sample_ds)).tolist()\n",
    "\n",
    "def main(num_samples, indices):\n",
    "    train_ds = Subset(ChestnutBurSegmentation(image_dir, \n",
    "                                                df, \n",
    "                                                get_transform(train = True)\n",
    "                                                ), \n",
    "                    indices[:-int(len(indices)*0.2)] # first 80% of dataset for training\n",
    "                    ) \n",
    "\n",
    "    val_dl = DataLoader(Subset(ChestnutBurSegmentation(image_dir, \n",
    "                                                        df, \n",
    "                                                        get_transform(train = False)\n",
    "                                                        ), \n",
    "                                indices[-int(len(indices)*0.2):-int(len(indices)*0.05)] # next 15% of dataset for validation\n",
    "                                ),\n",
    "                        batch_size = 1,\n",
    "                        shuffle = False,\n",
    "                        collate_fn = ChestnutBurSegmentation.collate_fn,\n",
    "                        num_workers=0,\n",
    "                        pin_memory=True\n",
    "                        )\n",
    "\n",
    "    test_dl = DataLoader(Subset(ChestnutBurSegmentation(image_dir, \n",
    "                                                        df, \n",
    "                                                        get_transform(train = False)\n",
    "                                                        ), \n",
    "                                indices[-int(len(indices)*0.05):] # last 5% of dataset for testing\n",
    "                                ), \n",
    "                         batch_size = 1, \n",
    "                         shuffle = False, \n",
    "                         collate_fn = ChestnutBurSegmentation.collate_fn, \n",
    "                         num_workers=0, \n",
    "                         pin_memory=True\n",
    "                         )\n",
    "\n",
    "    # put large objects in object store; reference in search_space\n",
    "    train_ds_ref = ray.put(train_ds)\n",
    "    val_dl_ref = ray.put(val_dl)\n",
    "    test_dl_ref = ray.put(test_dl)\n",
    "\n",
    "    search_space = {\n",
    "        \"epochs\": 10,\n",
    "        \"lr\": tune.uniform(0.00009, 0.05),\n",
    "        \"momentum\": tune.uniform(0.00001, 0.99),\n",
    "        \"weight_decay\": tune.uniform(0.00001, 0.99),\n",
    "        \"batch_size\": tune.choice([2, 4, 8]),\n",
    "        \"train_ds_ref\": train_ds_ref,\n",
    "        \"val_dl_ref\": val_dl_ref,\n",
    "        \"test_dl_ref\": test_dl_ref\n",
    "    }\n",
    "\n",
    "    algo = TuneBOHB(\n",
    "        metric=\"val_mAP_50\",\n",
    "        mode=\"max\",\n",
    "        points_to_evaluate=[\n",
    "            {\"lr\": 0.005, \n",
    "             \"momentum\": 0.9, \n",
    "             \"weight_decay\": 0.0005, \n",
    "             \"batch_size\": 2},\n",
    "        ]\n",
    "    )\n",
    "    algo = tune.search.ConcurrencyLimiter(algo, max_concurrent=1)\n",
    "    scheduler = HyperBandForBOHB(\n",
    "        time_attr=\"training_iteration\",\n",
    "        max_t=int(search_space[\"epochs\"]),\n",
    "        reduction_factor=4,\n",
    "        stop_last_trials=False,\n",
    "    )\n",
    "\n",
    "    tuner = tune.Tuner(\n",
    "        tune.with_resources(\n",
    "            tune.with_parameters(train_ChestnutBurSegmentation, indices=indices),\n",
    "            resources = {\"cpu\": 24.0, \"gpu\": 1.0},\n",
    "        ),\n",
    "        run_config=train.RunConfig(\n",
    "            name=\"bohb_exp\",\n",
    "            storage_path='C:/Users/exx/Documents/GitHub/Savanna-Institute/Drone-based_Chestnut_Detection/ray_results',\n",
    "            stop={\"training_iteration\": int(search_space[\"epochs\"])},\n",
    "        ),\n",
    "        tune_config = tune.TuneConfig(\n",
    "            metric = \"val_mAP_50\",\n",
    "            mode = \"max\",\n",
    "            search_alg = algo,\n",
    "            scheduler = scheduler,\n",
    "            num_samples = int(num_samples),\n",
    "            time_budget_s=600000,\n",
    "            trial_dirname_creator=trial_dirname_creator\n",
    "        ),\n",
    "        param_space=search_space\n",
    "    )\n",
    "\n",
    "    results = tuner.fit()\n",
    "\n",
    "    best_trial = results.get_best_result(metric = \"val_mAP_50\", mode = \"max\", scope = \"all\", filter_nan_and_inf=False)\n",
    "\n",
    "    print(\"Best trial config: {}\".format(best_trial.config))\n",
    "    print(\"Best trial final training loss: {}\".format(best_trial.metrics[\"train_loss\"]))\n",
    "    print(\"Best trial final validation loss: {}\".format(best_trial.metrics[\"val_loss\"]))\n",
    "    print(\"Best trial final training mAP_50: {}\".format(best_trial.metrics[\"train_mAP_50\"]))\n",
    "    print(\"Best trial final validation mAP_50: {}\".format(best_trial.metrics[\"val_mAP_50\"]))\n",
    "    print(\"Best trial final training mAR_100: {}\".format(best_trial.metrics[\"train_mAR_100\"]))\n",
    "    print(\"Best trial final validation mAR_100: {}\".format(best_trial.metrics[\"val_mAR_100\"]))\n",
    "\n",
    "    test_performance = test_best_model(best_trial, indices, image_dir, df)\n",
    "\n",
    "    return best_trial, test_performance\n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "    best_trial = main(num_samples = 20, indices = indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on the GPU or on the CPU, if a GPU is not available\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "num_classes = 2 # (background = 0, chestnut bur = 1)\n",
    "\n",
    "train_dl = DataLoader(ray.get(best_trial.config[\"train_ds_ref\"]), \n",
    "                      batch_size = best_trial.config[\"batch_size\"], \n",
    "                      shuffle = True, \n",
    "                      collate_fn = ChestnutBurSegmentation.collate_fn,\n",
    "                      num_workers=4,\n",
    "                      pin_memory=True)\n",
    "\n",
    "valid_dl = ray.get(best_trial.config[\"val_dl_ref\"])\n",
    "\n",
    "# get the model using our helper function\n",
    "model = get_instance_segmentation_model(num_classes)\n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(\n",
    "    params,\n",
    "    lr=best_trial.config[\"lr\"],\n",
    "    momentum=best_trial.config[\"momentum\"],\n",
    "    weight_decay=best_trial.config[\"weight_decay\"]\n",
    ")\n",
    "\n",
    "warmup_factor = 1.0 / 1000\n",
    "warmup_iters = min(1000, int(len(indices)*0.8) - 1)\n",
    "\n",
    "# construct a learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "        optimizer, start_factor=warmup_factor, total_iters=warmup_iters\n",
    "    )\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    train_logger, val_logger = engine.train_one_epoch(model, optimizer, train_dl, valid_dl, device, epoch, print_freq=2, scaler=None)\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the test dataset\n",
    "    train_performance, val_performance = engine.evaluate(model, valid_dl, device, train_dl)\n",
    "\n",
    "print(\"That's it!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model and weights to .pth\n",
    "torch.save(model.state_dict(), f\"C:/Users/exx/Documents/GitHub/Savanna-Institute/Drone-based_Chestnut_Detection/MaskRCNN_{time.strftime('%Y%m%d')}_{time.strftime('%H%M%S')}.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bohb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
