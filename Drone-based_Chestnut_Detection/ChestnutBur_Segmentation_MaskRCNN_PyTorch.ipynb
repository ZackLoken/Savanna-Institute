{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Chestnut Bur Detection and Segmentation using MaskRCNN in PyTorch</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, Subset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms as _transforms, tv_tensors\n",
    "from torchvision.ops import FrozenBatchNorm2d, roi_align\n",
    "import torchvision.transforms.v2 as T\n",
    "from torchvision.models.detection import MaskRCNN, rpn\n",
    "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
    "import json\n",
    "from matplotlib import patches\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import cv2\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import gc\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <center> Load the image and annotation data </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load annotations from json file\n",
    "annos = json.load(open(\"S:/Zack/Deep Learning/Chestnut_Bur_Instance_Segmentation/route9_orchard3/data/_annotations.coco.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the annos dict to a df\n",
    "annos_df = pd.DataFrame(annos[\"annotations\"])\n",
    "df = pd.DataFrame()\n",
    "df[\"tree_id\"] = annos_df[\"image_id\"].apply(lambda x: annos[\"images\"][x][\"file_name\"].split(\"_\")[0])\n",
    "df[\"file_name\"] = annos_df[\"image_id\"].apply(lambda x: annos[\"images\"][x][\"file_name\"])\n",
    "df[\"file_name\"] = df[\"file_name\"].apply(lambda x: x.split(\"_\")[0] + \".png\")\n",
    "categories = [cat[\"name\"] for cat in annos[\"categories\"]]\n",
    "df[\"category_name\"] = annos_df[\"category_id\"].apply(lambda x: categories[x])\n",
    "df[\"bbox\"] = annos_df[\"bbox\"].apply(lambda x: torch.tensor(x))\n",
    "df[\"area\"] = annos_df[\"area\"].apply(lambda x: torch.tensor(x))\n",
    "df[\"segmentation\"] = annos_df[\"segmentation\"].apply(lambda x: torch.tensor(x))\n",
    "df[\"iscrowd\"] = annos_df[\"iscrowd\"]\n",
    "\n",
    "# reviewed_trees = [329, 369, 115, 117, 286, 171, 320, 280, 309, 172, 92, \n",
    "#                   79, 146, 152, 51, 371, 272, 91, 392, 44, 12, 60, 304]\n",
    "\n",
    "reviewed_trees = [392, 60, 44, 91, 272, 51, 152, 329, 286, 320, 280, 366, 304] # subset for testing \n",
    "\n",
    "df = df[df[\"tree_id\"].isin([str(tree_id) for tree_id in reviewed_trees])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = \"S:/Zack/Deep Learning/Chestnut_Bur_Instance_Segmentation/route9_orchard3/data/images\"\n",
    "image_names = df[\"file_name\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <center> Pre-process image and annotation data </center>\n",
    "\n",
    "##### Adapted from: https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html#an-instance-segmentation-model-for-pennfudan-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_polygon_mask(mask, polys, color):\n",
    "    for poly in polys:\n",
    "        # Create a PIL Image object to use ImageDraw\n",
    "        poly_mask = Image.new(\"L\", (mask.shape[1], mask.shape[0]), 0)\n",
    "        ImageDraw.Draw(poly_mask).polygon(poly, fill=color)  # Directly pass the list of tuples\n",
    "        poly_mask = np.array(poly_mask)\n",
    "        mask = np.maximum(mask, poly_mask)\n",
    "    return mask\n",
    "\n",
    "# Define the output directories\n",
    "preprocessed_image_dir = Path('S:\\\\Zack\\\\Deep Learning\\\\Chestnut_Bur_Instance_Segmentation\\\\route9_orchard3\\\\data\\\\preprocessed\\\\images')\n",
    "bbox_image_dir = Path('S:\\\\Zack\\\\Deep Learning\\\\Chestnut_Bur_Instance_Segmentation\\\\route9_orchard3\\\\data\\\\preprocessed\\\\images_with_boxes')\n",
    "mask_output_dir = Path('S:\\\\Zack\\\\Deep Learning\\\\Chestnut_Bur_Instance_Segmentation\\\\route9_orchard3\\\\data\\\\preprocessed\\\\masks')\n",
    "\n",
    "# Ensure the output directories exist\n",
    "preprocessed_image_dir.mkdir(parents=True, exist_ok=True)\n",
    "bbox_image_dir.mkdir(parents=True, exist_ok=True)\n",
    "mask_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Loop through all file names in the DataFrame\n",
    "for file_name in df[\"file_name\"].unique():\n",
    "    try:\n",
    "        print(f\"Processing file: {file_name}\")\n",
    "        \n",
    "        # Filter the DataFrame for the current file name\n",
    "        row = df[df[\"file_name\"] == file_name]\n",
    "        image_path = Path(image_dir) / row[\"file_name\"].values[0]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image_np = np.array(image)  # Convert image to [H, W, 3]\n",
    "\n",
    "        # Process Canopy Polygon\n",
    "        canopy_poly = row[row[\"category_name\"] == \"Canopy\"][\"segmentation\"].values\n",
    "        canopy_poly = [np.array(poly[0]).reshape(-1, 2).tolist() for poly in canopy_poly]\n",
    "        canopy_poly = [[tuple(p) for p in poly] for poly in canopy_poly]  # Convert to list of tuples for PIL\n",
    "        canopy_mask = np.zeros((image_np.shape[0], image_np.shape[1]), dtype=np.uint8)\n",
    "        canopy_mask = create_polygon_mask(canopy_mask, canopy_poly, 1)\n",
    "\n",
    "        # Process Chestnut-burr Polygons\n",
    "        bur_poly = row[row[\"category_name\"] == \"Chestnut-burr\"][\"segmentation\"].values\n",
    "        bur_poly = [np.array(poly[0]).reshape(-1, 2).tolist() for poly in bur_poly]\n",
    "        bur_poly = [[tuple(p) for p in poly] for poly in bur_poly]  # Convert to list of tuples for PIL\n",
    "        bur_masks = []\n",
    "        for poly in bur_poly:\n",
    "            mask = np.zeros((image_np.shape[0], image_np.shape[1]), dtype=np.uint8)\n",
    "            mask = create_polygon_mask(mask, [poly], 1)\n",
    "            bur_masks.append(mask)\n",
    "        mask_image = np.stack(bur_masks, axis=0)\n",
    "\n",
    "        # Ensure the image and masks have compatible shapes\n",
    "        mask_image = mask_image.transpose(1, 2, 0)  # Convert masks to [H, W, N]\n",
    "\n",
    "        # Crop the image using NumPy slicing\n",
    "        canopy_bbox = row[row[\"category_name\"] == \"Canopy\"][\"bbox\"].values[0]\n",
    "        padding = 50.0\n",
    "        padded_bbox = [\n",
    "            max(0, int(canopy_bbox[0] - padding)),\n",
    "            max(0, int(canopy_bbox[1] - padding)),\n",
    "            min(image_np.shape[1], int(canopy_bbox[0] + canopy_bbox[2] + padding)),\n",
    "            min(image_np.shape[0], int(canopy_bbox[1] + canopy_bbox[3] + padding))\n",
    "        ]\n",
    "        image_cropped = image_np[padded_bbox[1]:padded_bbox[3], padded_bbox[0]:padded_bbox[2]]\n",
    "\n",
    "        # Crop the masks using NumPy slicing\n",
    "        mask_cropped = mask_image[padded_bbox[1]:padded_bbox[3], padded_bbox[0]:padded_bbox[2], :]\n",
    "\n",
    "        # Sum the masks along the last axis\n",
    "        mask_cropped_sum = np.sum(mask_cropped, axis=2)\n",
    "\n",
    "        # Fill the background with black where the canopy mask is not present\n",
    "        fill_color = [0, 0, 0]  # RGB for black\n",
    "        canopy_mask_cropped = canopy_mask[padded_bbox[1]:padded_bbox[3], padded_bbox[0]:padded_bbox[2]].astype(bool)\n",
    "        for c in range(3):\n",
    "            image_cropped[:, :, c][~canopy_mask_cropped] = fill_color[c]\n",
    "\n",
    "        # Convert the image to a tensor and normalize\n",
    "        transform = T.Compose([T.ToImage(), T.ToDtype(torch.float32, scale=True), T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "        image_cropped = transform(image_cropped).permute(1, 2, 0).numpy()  # Normalize and convert back to [H, W, C]\n",
    "\n",
    "        # Save the preprocessed image as a .npy file\n",
    "        preprocessed_image_path = preprocessed_image_dir / f'{Path(file_name).stem}.npy'\n",
    "        np.save(preprocessed_image_path, image_cropped)\n",
    "\n",
    "        # Save the multi-channel mask as a .npz file\n",
    "        mask_image_path = mask_output_dir / f'{Path(file_name).stem}.npz'\n",
    "        np.savez_compressed(mask_image_path, mask_cropped)\n",
    "\n",
    "        # Update the bounding box coordinates in the DataFrame\n",
    "        for i, bbox in enumerate(row[row[\"category_name\"] == \"Chestnut-burr\"][\"bbox\"].values):\n",
    "            new_bbox = [\n",
    "                bbox[0] - padded_bbox[0],\n",
    "                bbox[1] - padded_bbox[1],\n",
    "                bbox[2] - padded_bbox[0],\n",
    "                bbox[3] - padded_bbox[1]\n",
    "            ]\n",
    "            df.at[row.index[i], \"bbox\"] = new_bbox\n",
    "\n",
    "        # Plot the bounding boxes on the preprocessed image at 100% scale and save\n",
    "        fig, ax = plt.subplots(1, figsize=(image_cropped.shape[1] / 100, image_cropped.shape[0] / 100), dpi=1)\n",
    "        ax.imshow(image_cropped)\n",
    "        ax.set_title('Preprocessed Image with Bounding Boxes')\n",
    "\n",
    "        # Add bounding boxes\n",
    "        for bbox in row[row[\"category_name\"] == \"Chestnut-burr\"][\"bbox\"].values:\n",
    "            rect = patches.Rectangle(\n",
    "                (bbox[0] - padded_bbox[0], bbox[1] - padded_bbox[1]),\n",
    "                bbox[2] - bbox[0],\n",
    "                bbox[3] - bbox[1],\n",
    "                linewidth=0.5,\n",
    "                edgecolor='r',\n",
    "                facecolor='none'\n",
    "            )\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "        plt.axis('off')\n",
    "        bbox_image_path = bbox_image_dir / f'{file_name}'\n",
    "        plt.savefig(bbox_image_path, bbox_inches='tight', pad_inches=0)\n",
    "        plt.close(fig)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_name}: {e}\")\n",
    "\n",
    "# Filter the DataFrame to include only the necessary columns and rows where the category name is \"Chestnut-burr\"\n",
    "df = df[df[\"category_name\"] == \"Chestnut-burr\"]\n",
    "df = df[[\"tree_id\", \"file_name\", \"category_name\", \"bbox\", \"iscrowd\"]]\n",
    "\n",
    "# Convert bounding boxes to lists of floats\n",
    "df[\"bbox\"] = df[\"bbox\"].apply(lambda x: [float(coord) for coord in x.strip('[]').split(',')])\n",
    "\n",
    "# Save the updated DataFrame to a CSV file\n",
    "updated_csv_path = 'S:\\\\Zack\\\\Deep Learning\\\\Chestnut_Bur_Instance_Segmentation\\\\route9_orchard3\\\\data\\\\preprocessed\\\\preprocessed_annotations.csv'\n",
    "df.to_csv(updated_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChestnutBurSegmentation(Dataset):\n",
    "    def __init__(self, images_dir, masks_dir, annotations_csv, transform=None):\n",
    "        self.images_dir = Path(images_dir)\n",
    "        self.masks_dir = Path(masks_dir)\n",
    "        self.annotations = pd.read_csv(annotations_csv)\n",
    "        self.transform = transform\n",
    "        self.file_names = self.annotations[\"file_name\"].unique()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.file_names[idx]\n",
    "        image_path = self.images_dir / f'{Path(file_name).stem}.npy'\n",
    "        mask_path = self.masks_dir / f'{Path(file_name).stem}.npz'\n",
    "\n",
    "        # Load the preprocessed image\n",
    "        image = np.load(image_path)\n",
    "\n",
    "        # Load the preprocessed mask\n",
    "        mask = np.load(mask_path)['arr_0']\n",
    "\n",
    "        # Get the annotations for the current file\n",
    "        annotations = self.annotations[self.annotations[\"file_name\"] == file_name]\n",
    "\n",
    "        # Extract bounding boxes, labels, and iscrowd from annotations\n",
    "        bboxes = annotations[\"bbox\"].apply(eval).tolist()\n",
    "        labels = annotations[\"category_name\"].apply(lambda x: 1 if x == \"Chestnut-burr\" else 0).tolist()\n",
    "        iscrowd = annotations[\"iscrowd\"].tolist()\n",
    "\n",
    "        # Convert bounding boxes to tensors and calculate area\n",
    "        bboxes = torch.tensor(bboxes, dtype=torch.float32)\n",
    "        area = (bboxes[:, 2] - bboxes[:, 0]) * (bboxes[:, 3] - bboxes[:, 1])\n",
    "\n",
    "        # Create the target dictionary\n",
    "        target = {\n",
    "            \"boxes\": bboxes,\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.int64),\n",
    "            \"image_id\": torch.tensor([idx]),\n",
    "            \"area\": area,\n",
    "            \"iscrowd\": torch.tensor(iscrowd, dtype=torch.uint8),\n",
    "            \"masks\": torch.tensor(mask, dtype=torch.uint8)\n",
    "        }\n",
    "\n",
    "        if self.transform:\n",
    "            image, target = self.transform(image, target)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define transforms for the dataset\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    if train:\n",
    "        transforms.append(T.RandomApply([T.RandomRotation(degrees=[-15, 15], fill=defaultdict(lambda: 0, {tv_tensors.Image: (0, 0, 0)}))], p=0.75)) # fill padded area in black\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "        transforms.append(T.RandomIoUCrop(min_scale=0.9, max_scale=1.1))\n",
    "        transforms.append(T.RandomApply([T.ColorJitter(brightness=0.05, contrast=0.1, saturation=0.1, hue=0)], p=0.5))\n",
    "    transforms.append(T.Resize(size=(540,), max_size=960, interpolation=T.InterpolationMode.BICUBIC))\n",
    "    transforms.append(T.ClampBoundingBoxes()) # for segmentations too\n",
    "    transforms.append(T.SanitizeBoundingBoxes()) ## for segmentations too\n",
    "    transforms.append(T.ToDtype(dtype = torch.float32, scale = True))\n",
    "    # transforms.append(T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]))\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_dir = 'S:\\\\Zack\\\\Deep Learning\\\\Chestnut_Bur_Instance_Segmentation\\\\route9_orchard3\\\\data\\\\preprocessed\\\\images'\n",
    "masks_dir = 'S:\\\\Zack\\\\Deep Learning\\\\Chestnut_Bur_Instance_Segmentation\\\\route9_orchard3\\\\data\\\\preprocessed\\\\masks'\n",
    "annotations_csv = 'S:\\\\Zack\\\\Deep Learning\\\\Chestnut_Bur_Instance_Segmentation\\\\route9_orchard3\\\\data\\\\preprocessed\\\\updated_annotations.csv'\n",
    "\n",
    "sample_ds = ChestnutBurSegmentation(images_dir, \n",
    "                                  masks_dir, \n",
    "                                  annotations_csv,\n",
    "                                  transform = get_transform(train=True))\n",
    "\n",
    "sample_dl = DataLoader(sample_ds, \n",
    "                       batch_size = 1, \n",
    "                       shuffle = True, \n",
    "                       collate_fn = ChestnutBurSegmentation.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resized_bounding_boxes = []\n",
    "\n",
    "# for images, targets in sample_dl:\n",
    "#     for target in targets:\n",
    "#         for box in target['boxes']:\n",
    "#             resized_bounding_boxes.append(box)\n",
    "\n",
    "# # Convert to numpy array\n",
    "# resized_bounding_boxes = np.array(resized_bounding_boxes)\n",
    "\n",
    "# # Print the resized bounding box dimensions\n",
    "# print(resized_bounding_boxes[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cluster import KMeans\n",
    "\n",
    "# # Convert to [width, height] format\n",
    "# widths = resized_bounding_boxes[:, 2] - resized_bounding_boxes[:, 0]\n",
    "# heights = resized_bounding_boxes[:, 3] - resized_bounding_boxes[:, 1]\n",
    "# bounding_boxes_wh = np.stack((widths, heights), axis=1)\n",
    "\n",
    "# # Perform k-means clustering to find anchor sizes\n",
    "# num_clusters = 5  # Number of anchor sizes\n",
    "# kmeans = KMeans(n_clusters=num_clusters, random_state=666).fit(bounding_boxes_wh)\n",
    "# anchor_sizes = kmeans.cluster_centers_\n",
    "\n",
    "# # Print the anchor sizes\n",
    "# print(\"Anchor Sizes (width, height):\")\n",
    "# print(anchor_sizes)\n",
    "\n",
    "# # Determine aspect ratios from the anchor sizes\n",
    "# anchor_aspect_ratios = anchor_sizes[:, 0] / anchor_sizes[:, 1]\n",
    "\n",
    "# # Print the aspect ratios\n",
    "# print(\"Anchor Aspect Ratios:\")\n",
    "# print(anchor_aspect_ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m images, targets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msample_dl\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m images \u001b[38;5;241m=\u001b[39m [img \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[0;32m      3\u001b[0m targets \u001b[38;5;241m=\u001b[39m [{k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m target\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m target \u001b[38;5;129;01min\u001b[39;00m targets]\n",
      "File \u001b[1;32mc:\\Users\\zack\\anaconda3\\envs\\bohb_pt\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\zack\\anaconda3\\envs\\bohb_pt\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\zack\\anaconda3\\envs\\bohb_pt\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\zack\\anaconda3\\envs\\bohb_pt\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[6], line 27\u001b[0m, in \u001b[0;36mChestnutBurSegmentation.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     24\u001b[0m annotations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mannotations[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mannotations[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile_name\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m file_name]\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Extract bounding boxes, labels, and iscrowd from annotations\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m bboxes \u001b[38;5;241m=\u001b[39m \u001b[43mannotations\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbbox\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43meval\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     28\u001b[0m labels \u001b[38;5;241m=\u001b[39m annotations[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategory_name\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChestnut-burr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     29\u001b[0m iscrowd \u001b[38;5;241m=\u001b[39m annotations[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miscrowd\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[1;32mc:\\Users\\zack\\anaconda3\\envs\\bohb_pt\\lib\\site-packages\\pandas\\core\\series.py:4917\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\zack\\anaconda3\\envs\\bohb_pt\\lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\zack\\anaconda3\\envs\\bohb_pt\\lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\zack\\anaconda3\\envs\\bohb_pt\\lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\zack\\anaconda3\\envs\\bohb_pt\\lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m<string>:1\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tensor' is not defined"
     ]
    }
   ],
   "source": [
    "images, targets = next(iter(sample_dl))\n",
    "images = [img for img in images]\n",
    "targets = [{k: v for k, v in target.items()} for target in targets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <center> Plot sample transformed images, targets, and masks </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sample_images_and_targets(images, targets):\n",
    "    batch_size = len(images)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        # Get the image and target from the batch\n",
    "        image = images[i]\n",
    "        target = targets[i]\n",
    "\n",
    "        # Convert image tensor to numpy array for plotting\n",
    "        image_np = image.permute(1, 2, 0).cpu().numpy()  # Convert from [C, H, W] to [H, W, C]\n",
    "\n",
    "        # Plot the image\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(10, 10))  # Increase DPI for higher resolution\n",
    "        ax.imshow(image_np)\n",
    "        ax.set_title(f\"Image {i}\")\n",
    "\n",
    "        # Plot bounding boxes\n",
    "        bboxes = target['boxes'].cpu().numpy()\n",
    "        for bbox in bboxes:\n",
    "            rect = plt.Rectangle((bbox[0], bbox[1]), bbox[2] - bbox[0], bbox[3] - bbox[1], fill=False, color='red', linewidth=0.75)\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "plot_sample_images_and_targets(images, targets)\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot each transformed image with its chestnut burr masks\n",
    "\n",
    "for i, (image, target) in enumerate(zip(images, targets)):\n",
    "\n",
    "    tree_id = target[\"image_id\"]\n",
    "    image = image.permute(1, 2, 0)\n",
    "    mask = target[\"masks\"].permute(1, 2, 0)\n",
    "\n",
    "    #plot mask on image and save to .png file\n",
    "    plt.imshow(image)\n",
    "    plt.imshow(mask.sum(axis=2), cmap='gray', alpha=0.75)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <center> Construct MaskRCNN Model </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_instance_segmentation_model(depth=18, trainable_backbone_layers=3, min_size=540, max_size=960, image_mean=[0, 0, 0], \n",
    "                                    image_std=[1, 1, 1], num_classes=2, box_score_thresh=0.5, box_nms_thresh=0.3,\n",
    "                                    box_detections_per_img=1000):\n",
    "    # Create the backbone with FPN\n",
    "    if depth == 18:\n",
    "        backbone = resnet_fpn_backbone(backbone_name='resnet18', \n",
    "                                       weights=torchvision.models.ResNet18_Weights.DEFAULT, \n",
    "                                       trainable_layers=trainable_backbone_layers\n",
    "                                       )\n",
    "    elif depth == 34:\n",
    "        backbone = resnet_fpn_backbone(backbone_name='resnet34', \n",
    "                                       weights=torchvision.models.ResNet34_Weights.DEFAULT,\n",
    "                                       trainable_layers=trainable_backbone_layers\n",
    "                                       )\n",
    "    elif depth == 50:\n",
    "        backbone = resnet_fpn_backbone(backbone_name='resnet50', \n",
    "                                       weights=torchvision.models.ResNet50_Weights.DEFAULT,\n",
    "                                       trainable_layers=trainable_backbone_layers\n",
    "                                       )\n",
    "    elif depth == 101:\n",
    "        backbone = resnet_fpn_backbone(backbone_name='resnet101', \n",
    "                                       weights=torchvision.models.ResNet101_Weights.DEFAULT, \n",
    "                                       trainable_layers=trainable_backbone_layers\n",
    "                                       )\n",
    "    elif depth == 152:\n",
    "        backbone = resnet_fpn_backbone(backbone_name='resnet152', \n",
    "                                       weights=torchvision.models.ResNet152_Weights.DEFAULT, \n",
    "                                       trainable_layers=trainable_backbone_layers\n",
    "                                       )\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model depth\")\n",
    "    \n",
    "    # anchor sizes and aspect ratios determined using Kmeans clustering on resized bounding boxes\n",
    "    anchor_sizes = [(1,), (2,), (4,), (8,), (16,)] \n",
    "    anchor_aspect_ratios = [(0.99, 1.0, 1.01), (0.99, 1.0, 1.01), (0.99, 1.0, 1.01), (0.99, 1.0, 1.01), (0.99, 1.0, 1.01)]\n",
    "\n",
    "    # Create the Mask R-CNN model with the custom backbone\n",
    "    model = MaskRCNN(backbone, \n",
    "                     num_classes=num_classes,\n",
    "                     min_size=min_size,\n",
    "                     max_size=max_size,\n",
    "                     image_mean=image_mean,\n",
    "                     image_std=image_std,\n",
    "                     rpn_anchor_generator=rpn.AnchorGenerator(sizes=anchor_sizes, aspect_ratios=anchor_aspect_ratios),\n",
    "                     box_score_thresh=box_score_thresh,\n",
    "                     box_nms_thresh=box_nms_thresh,\n",
    "                     box_detections_per_img=box_detections_per_img\n",
    "                     )\n",
    "\n",
    "    # Get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "    # Replace the pre-trained head with a new one to reflect the number of classes\n",
    "    model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # Get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "\n",
    "    # Replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = torchvision.models.detection.mask_rcnn.MaskRCNNPredictor(\n",
    "        in_features_mask,\n",
    "        hidden_layer,\n",
    "        num_classes\n",
    "    )\n",
    "\n",
    "    # replace FrozenBatchNorm2d in the last two backbone layers with trainable batchNorm2d\n",
    "    for name, module in model.backbone.body.named_modules():\n",
    "        if 'layer3' in name or 'layer4' in name:\n",
    "            if isinstance(module, FrozenBatchNorm2d):\n",
    "                # Extract the number of features, mean, and variance from the FrozenBatchNorm2d\n",
    "                num_features = module.weight.shape[0]  # Get the number of channels (features)\n",
    "                running_mean = module.running_mean.clone()  # Extract running mean\n",
    "                running_var = module.running_var.clone()    # Extract running variance\n",
    "\n",
    "                # Create a new BatchNorm2d layer with the same number of features\n",
    "                batch_norm = nn.BatchNorm2d(num_features)\n",
    "\n",
    "                # Initialize BatchNorm2d with the extracted running mean and variance\n",
    "                batch_norm.running_mean = running_mean\n",
    "                batch_norm.running_var = running_var\n",
    "\n",
    "                # Initialize the weights with a normal distribution (mean=1, std=0.02) for stability\n",
    "                nn.init.normal_(batch_norm.weight, mean=1.0, std=0.02)  # Small normal distribution\n",
    "\n",
    "                # Initialize the biases to zero (standard for BatchNorm2d)\n",
    "                nn.init.constant_(batch_norm.bias, 0)\n",
    "\n",
    "                # Replace the FrozenBatchNorm2d layer with BatchNorm2d\n",
    "                parent_module_name = '.'.join(name.split('.')[:-1])  # Get the parent module name\n",
    "                module_name = name.split('.')[-1]  # Extract the last part of the name (the module name)\n",
    "\n",
    "                # Retrieve the parent module and replace the FrozenBatchNorm2d with BatchNorm2d\n",
    "                parent_module = dict(model.backbone.body.named_modules())[parent_module_name]\n",
    "                setattr(parent_module, module_name, batch_norm)\n",
    "\n",
    "                # Set requires_grad to True for the new BatchNorm2d layer\n",
    "                for param in batch_norm.parameters():\n",
    "                    param.requires_grad = True\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_instance_segmentation_model())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Tune model hyperparameters using Ray Tune</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian Optimization HyperBand (BOHB) with HyperBand scheduler\n",
    "# # https://proceedings.mlr.press/v80/falkner18a.html\n",
    "\n",
    "from torch_lr_finder import LRFinder, TrainDataLoaderIter\n",
    "from datetime import datetime\n",
    "import gc\n",
    "import time\n",
    "import ray\n",
    "from ray import train, tune\n",
    "from ray.tune import JupyterNotebookReporter\n",
    "from ray.tune.search import ConcurrencyLimiter\n",
    "from ray.tune.schedulers.hb_bohb import HyperBandForBOHB\n",
    "from ray.tune.search.bohb import TuneBOHB\n",
    "import ray.cloudpickle as pickle\n",
    "from segmentation_pytorch import engine\n",
    "from segmentation_pytorch.coco_utils import get_coco_api_from_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ray Tune Trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducible training\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def calculate_f1_score(precision, recall):\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "def train_ChestnutBurSegmentation(config):\n",
    "    import ray\n",
    "    import torch\n",
    "    from segmentation_pytorch import engine\n",
    "    import tempfile\n",
    "    from pathlib import Path\n",
    "    import ray.cloudpickle as pickle\n",
    "\n",
    "     # Set random seed for reproducible training\n",
    "    set_seed(666)\n",
    "\n",
    "    # Get dataset references directly from config\n",
    "    dataset_train = ray.get(config[\"dataset_train_ref\"])\n",
    "    data_loader_val = ray.get(config[\"data_loader_val_ref\"])\n",
    "    train_coco_ds = ray.get(config[\"train_coco_ds_ref\"])\n",
    "    val_coco_ds = ray.get(config[\"val_coco_ds_ref\"])\n",
    "\n",
    "    # Use gradient accumulation due to memory constraints (batch_size=16 maxes out GPU)\n",
    "    training_steps = [\n",
    "        {\"step\": 0, \"batch_size\": config[\"batch_size\"], \"epochs\": 10, \"print_freq\": 1, \"accumulation_steps\": 1},\n",
    "        {\"step\": 1, \"batch_size\": config[\"batch_size\"], \"epochs\": 10, \"print_freq\": 1, \"accumulation_steps\": 2},\n",
    "        {\"step\": 2, \"batch_size\": config[\"batch_size\"], \"epochs\": 5, \"print_freq\": 1, \"accumulation_steps\": 4}\n",
    "    ]\n",
    "\n",
    "    # Load checkpoint if available\n",
    "    checkpoint = train.get_checkpoint()\n",
    "    if checkpoint:\n",
    "        with checkpoint.as_directory() as checkpoint_dir:\n",
    "            data_path = Path(checkpoint_dir) / \"data.pkl\"\n",
    "            with open(data_path, \"rb\") as fp:\n",
    "                checkpoint_state = pickle.load(fp)\n",
    "            start_epoch = checkpoint_state[\"epoch\"] + 1\n",
    "            current_step = checkpoint_state[\"current_step\"]\n",
    "            batch_size = checkpoint_state[\"batch_size\"]\n",
    "            accumulation_steps = checkpoint_state[\"accumulation_steps\"]\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        current_step = 0\n",
    "        batch_size = config[\"batch_size\"]\n",
    "        accumulation_steps = training_steps[0][\"accumulation_steps\"]\n",
    "\n",
    "    # Initialize step index\n",
    "    step_index = current_step\n",
    "\n",
    "    # Loop through training_steps during training to increase batch size\n",
    "    while step_index < len(training_steps):\n",
    "        step = training_steps[step_index]\n",
    "\n",
    "        batch_size = step['batch_size']\n",
    "        total_epochs = step['epochs']\n",
    "        print_freq = step['print_freq']\n",
    "        accumulation_steps = step['accumulation_steps']\n",
    "\n",
    "        model = get_instance_segmentation_model(depth=config[\"resnet\"],\n",
    "                                                trainable_backbone_layers=config[\"backbone_lyrs\"],\n",
    "                                                image_mean=[0, 0, 0],\n",
    "                                                image_std=[1, 1, 1],\n",
    "                                                num_classes=2, # background, chestnut bur\n",
    "                                                box_score_thresh=config[\"score_thresh\"],\n",
    "                                                box_nms_thresh=config[\"nms_thresh\"],\n",
    "                                                box_detections_per_img=1000) # match maxDets in COCO evaluation\n",
    "        \n",
    "        device = \"cpu\"\n",
    "        if torch.cuda.is_available():\n",
    "            device = \"cuda:0\"\n",
    "        model.to(device)\n",
    "\n",
    "        # construct an optimizer\n",
    "        params = [p for p in model.parameters() if p.requires_grad]\n",
    "        optimizer = torch.optim.SGD(params, \n",
    "                                    lr = config[\"lr\"], \n",
    "                                    momentum = config[\"momentum\"], \n",
    "                                    weight_decay = config[\"weight_decay\"])\n",
    "\n",
    "        lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min')\n",
    "                                                                  \n",
    "        # Restore model and optimizer state if checkpoint is available\n",
    "        if checkpoint:\n",
    "            model.load_state_dict(checkpoint_state[\"model_state_dict\"])\n",
    "            optimizer.load_state_dict(checkpoint_state[\"optimizer_state_dict\"])\n",
    "\n",
    "        # Calculate the remaining epochs for the current step\n",
    "        remaining_epochs = total_epochs - (start_epoch % total_epochs)\n",
    "\n",
    "        data_loader = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size,\n",
    "                                                  shuffle=True, \n",
    "                                                  collate_fn=ChestnutBurSegmentation.collate_fn,\n",
    "                                                  num_workers=0, pin_memory=True)\n",
    "\n",
    "        print(f'Training step {step[\"step\"]}... batch size: {batch_size * accumulation_steps}')\n",
    "        print()\n",
    "\n",
    "        for epoch in range(start_epoch, start_epoch + remaining_epochs):\n",
    "            train_metric_logger, val_metric_logger = engine.train_one_epoch(model, optimizer, data_loader, device,\n",
    "                                                                     epoch, print_freq, accumulation_steps,\n",
    "                                                                     data_loader_val)\n",
    "\n",
    "            # Evaluate on the val dataset\n",
    "            train_coco_evaluator, val_coco_evaluator = engine.evaluate(model, data_loader_val, val_coco_ds, device, data_loader, train_coco_ds)\n",
    "\n",
    "            # Calculate F1 score for the current epoch\n",
    "            val_f1 = calculate_f1_score(val_coco_evaluator.coco_eval['segm'].stats[0], val_coco_evaluator.coco_eval['segm'].stats[8])\n",
    "\n",
    "            # Update the learning rate\n",
    "            lr_scheduler.step(metrics=val_f1)\n",
    "\n",
    "            checkpoint_data = {\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"current_step\": step[\"step\"],\n",
    "                \"batch_size\": batch_size,\n",
    "                \"accumulation_steps\": accumulation_steps,\n",
    "            }\n",
    "\n",
    "            with tempfile.TemporaryDirectory() as checkpoint_dir:\n",
    "                data_path = Path(checkpoint_dir) / \"data.pkl\"\n",
    "                with open(data_path, \"wb\") as fp:\n",
    "                    pickle.dump(checkpoint_data, fp)\n",
    "                train.report(\n",
    "                    {\"epoch\": epoch,\n",
    "                     \"train_loss\": train_metric_logger.loss.avg,  # loss averaged across all batches in epoch\n",
    "                     \"val_loss\": val_metric_logger.loss.avg,\n",
    "                     \"train_mAP\": train_coco_evaluator.coco_eval['segm'].stats[0],  # mAP (IoU=0.50:0.95)\n",
    "                     \"val_mAP\": val_coco_evaluator.coco_eval['segm'].stats[0],\n",
    "                     \"train_mAR\": train_coco_evaluator.coco_eval['segm'].stats[8],  # mAR (IoU=0.50:0.95)\n",
    "                     \"val_mAR\": val_coco_evaluator.coco_eval['segm'].stats[8],\n",
    "                     \"train_f1\": calculate_f1_score(train_coco_evaluator.coco_eval['segm'].stats[0],  # AP (IoU=0.50:0.95)\n",
    "                                                    train_coco_evaluator.coco_eval['segm'].stats[8]  # AR (IoU=0.50:0.95)\n",
    "                                                    ),\n",
    "                     \"val_f1\": val_f1},\n",
    "                    checkpoint=train.Checkpoint.from_directory(checkpoint_dir),\n",
    "                )\n",
    "\n",
    "        # Set start_epoch to the next epoch for the next training step\n",
    "        start_epoch += remaining_epochs\n",
    "        step_index += 1\n",
    "\n",
    "    print('Tuning Trial Complete!')\n",
    "\n",
    "\n",
    "def test_best_model(best_trial, best_checkpoint):\n",
    "    best_model = get_instance_segmentation_model(depth=best_trial.config[\"resnet\"],\n",
    "                                                trainable_backbone_layers=best_trial.config[\"backbone_lyrs\"],\n",
    "                                                image_mean=[0, 0, 0],\n",
    "                                                image_std=[1, 1, 1],\n",
    "                                                num_classes=2, # background, chestnut bur\n",
    "                                                box_score_thresh=best_trial.config[\"score_thresh\"],\n",
    "                                                box_nms_thresh=best_trial.config[\"nms_thresh\"],\n",
    "                                                box_detections_per_img=1000)\n",
    "\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        \n",
    "    best_model.to(device)\n",
    "\n",
    "    with best_checkpoint.as_directory() as checkpoint_dir:\n",
    "        data_path = Path(checkpoint_dir) / \"data.pkl\"\n",
    "        with open(data_path, \"rb\") as fp:\n",
    "            best_checkpoint_data = pickle.load(fp)\n",
    "\n",
    "        best_model.load_state_dict(best_checkpoint_data[\"model_state_dict\"])\n",
    "\n",
    "    data_loader_test = ray.get(best_trial.config[\"data_loader_test_ref\"])\n",
    "    test_coco_ds = ray.get(best_trial.config[\"test_coco_ds_ref\"]) \n",
    "    \n",
    "    test_results = engine.evaluate(best_model, data_loader_test, test_coco_ds, device, train_data_loader=None, train_coco_ds=None)\n",
    "\n",
    "    print(f'Best trial test set mAP: {test_results.coco_eval[\"segm\"].stats[0]}') # IoU=0.50:0.95\n",
    "    print(f'Best trial test set mAR: {test_results.coco_eval[\"segm\"].stats[8]}') # IoU=0.50:0.95\n",
    "    print(f'Best trial test set f1-score: {calculate_f1_score(test_results.coco_eval[\"segm\"].stats[0], test_results.coco_eval[\"segm\"].stats[8])}') # IoU=0.50:0.95\n",
    "\n",
    "\n",
    "def trial_dirname_creator(trial):\n",
    "    return f\"{trial.trial_id}\"\n",
    "\n",
    "\n",
    "def train_lr_finder(config):\n",
    "\n",
    "    class CustomTrainDataLoaderIter(TrainDataLoaderIter):\n",
    "        def inputs_labels_from_batch(self, batch_data):\n",
    "            inputs = [image.to('cuda:0') for image in batch_data[0]]\n",
    "            labels = [{k: v.to('cuda:0') for k, v in t.items()} for t in batch_data[1]]\n",
    "            return inputs, labels\n",
    "\n",
    "    dataset_train = ray.get(config[\"dataset_train_ref\"])\n",
    "    accumulation_steps = 1  ## FIXME: hardcoded for now\n",
    "\n",
    "    data_loader_train = torch.utils.data.DataLoader(dataset_train, batch_size=config[\"batch_size\"],\n",
    "                                                    shuffle=True,\n",
    "                                                    collate_fn=ChestnutBurSegmentation.collate_fn,\n",
    "                                                    num_workers=0, pin_memory=False)\n",
    "\n",
    "    model = get_instance_segmentation_model(depth=config[\"resnet\"],\n",
    "                                            trainable_backbone_layers=config[\"backbone_lyrs\"],\n",
    "                                            image_mean=[0, 0, 0],\n",
    "                                            image_std=[1, 1, 1],\n",
    "                                            num_classes=2, # background, chestnut bur\n",
    "                                            box_score_thresh=config[\"score_thresh\"],\n",
    "                                            box_nms_thresh=config[\"nms_thresh\"],\n",
    "                                            box_detections_per_img=1000).to('cuda:0')\n",
    "\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(\n",
    "        params, lr=1e-7, momentum=config[\"momentum\"], weight_decay=config[\"weight_decay\"]\n",
    "    )\n",
    "\n",
    "    train_iter = CustomTrainDataLoaderIter(data_loader_train)\n",
    "    grad_scaler = torch.GradScaler()\n",
    "\n",
    "    class CustomLRFinder(LRFinder):\n",
    "        def __init__(self, model, optimizer, criterion, device=None, amp_backend=\"native\", amp_config=None, grad_scaler=None):\n",
    "            super().__init__(model, optimizer, criterion, device)\n",
    "            self.amp_backend = amp_backend\n",
    "            self.amp_config = amp_config\n",
    "            self.grad_scaler = grad_scaler or torch.GradScaler()\n",
    "\n",
    "        def _train_batch(self, train_iter, accumulation_steps, non_blocking_transfer=True):\n",
    "            self.model.train()\n",
    "            total_loss = 0\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            for _ in range(accumulation_steps):\n",
    "                inputs, labels = next(train_iter)\n",
    "                inputs, labels = self._move_to_device(inputs, labels, non_blocking=non_blocking_transfer)\n",
    "\n",
    "                with torch.autocast(device_type=\"cuda:0\"):\n",
    "                    outputs = self.model(inputs, labels)\n",
    "                    loss = sum(loss for loss in outputs.values())\n",
    "\n",
    "                loss /= accumulation_steps\n",
    "                self.grad_scaler.scale(loss).backward()\n",
    "                total_loss += loss\n",
    "\n",
    "            self.grad_scaler.step(self.optimizer)\n",
    "            self.grad_scaler.update()\n",
    "\n",
    "            return total_loss.item()\n",
    "\n",
    "    lr_finder = CustomLRFinder(model, optimizer, None, device='cuda:0', amp_backend='torch', amp_config=None, grad_scaler=grad_scaler)\n",
    "    lr_finder.range_test(train_iter, end_lr=0.01, num_iter=100, step_mode='exp', accumulation_steps=accumulation_steps)\n",
    "    suggested_lr = lr_finder.plot(suggest_lr=True)\n",
    "\n",
    "    lr_finder.reset()\n",
    "\n",
    "    # Ensure consistent return value\n",
    "    try:\n",
    "        if isinstance(suggested_lr, tuple):\n",
    "            axes, suggested_lr_value = suggested_lr\n",
    "            return suggested_lr_value\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected return type from plot method: {type(suggested_lr)}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Error during learning rate finding: {e}\")\n",
    "        # Return a default learning rate if an error occurs\n",
    "        return 5e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Main tuning program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store indices in random order list for subsetting\n",
    "indices = torch.randperm(len(sample_ds)).tolist()\n",
    "\n",
    "def main(num_samples, max_num_epochs, restore_path=\"\"):\n",
    "    ray.shutdown()\n",
    "    ray.init(include_dashboard=True)\n",
    "\n",
    "    dataset_train = ChestnutBurSegmentation(image_dir, df, get_transform(train = True))\n",
    "    dataset_val = ChestnutBurSegmentation(image_dir, df, get_transform(train = False))\n",
    "    dataset_test = ChestnutBurSegmentation(image_dir, df, get_transform(train = False))\n",
    "\n",
    "    # dataset_train = Subset(dataset_train, \n",
    "    #                     indices[:-int(len(indices)*0.2)]) # first 80% of dataset for training     \n",
    "     \n",
    "    dataset_train = Subset(dataset_train, indices[:10])                       \n",
    "\n",
    "    # dataset_val = Subset(dataset_val, \n",
    "    #                             indices[-int(len(indices)*0.2):-int(len(indices)*0.05)]) # next 15% of dataset for validation                                \n",
    "\n",
    "    dataset_val = Subset(dataset_val, indices[10:12]) \n",
    "\n",
    "    # dataset_test = Subset(dataset_test, \n",
    "    #                             indices[-int(len(indices)*0.05):]) # last 5% of dataset for testing                            \n",
    "    \n",
    "    dataset_test = Subset(dataset_test, indices[12:]) \n",
    "\n",
    "    data_loader_val = torch.utils.data.DataLoader(\n",
    "        dataset_val, batch_size=1, shuffle=False,\n",
    "        collate_fn=ChestnutBurSegmentation.collate_fn, num_workers=0, pin_memory=False\n",
    "    )\n",
    "\n",
    "    data_loader_test = torch.utils.data.DataLoader(\n",
    "        dataset_test, batch_size=1, shuffle=False,\n",
    "        collate_fn=ChestnutBurSegmentation.collate_fn, num_workers=0, pin_memory=False\n",
    "    )\n",
    "\n",
    "    # Create ObjectRefs\n",
    "    dataset_train_ref = ray.put(dataset_train)\n",
    "    data_loader_val_ref = ray.put(data_loader_val)\n",
    "    data_loader_test_ref = ray.put(data_loader_test)\n",
    "\n",
    "\n",
    "    train_coco_ds = get_coco_api_from_dataset(dataset_train)\n",
    "    train_coco_ds_ref = ray.put(train_coco_ds)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    val_coco_ds = get_coco_api_from_dataset(dataset_val)\n",
    "    val_coco_ds_ref = ray.put(val_coco_ds)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    test_coco_ds = get_coco_api_from_dataset(dataset_test)\n",
    "    test_coco_ds_ref = ray.put(test_coco_ds)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "        \n",
    "\n",
    "    config = {\n",
    "        # \"lr\": tune.sample_from(lambda config: train_lr_finder(config)),\n",
    "        \"lr\": tune.loguniform(1e-5, 1e-2),\n",
    "        \"resnet\": tune.choice([18, 34]),\n",
    "        \"momentum\": tune.uniform(0.4, 0.9),\n",
    "        \"weight_decay\": tune.loguniform(0.00001, 0.01),\n",
    "        \"backbone_lyrs\": tune.choice([2, 3, 4]),\n",
    "        \"batch_size\": tune.choice([1, 2]),\n",
    "        \"score_thresh\": tune.uniform(0.1, 0.5),\n",
    "        \"nms_thresh\": tune.uniform(0.05, 0.4),\n",
    "        \"dataset_train_ref\": dataset_train_ref,\n",
    "        \"data_loader_val_ref\": data_loader_val_ref,\n",
    "        \"data_loader_test_ref\": data_loader_test_ref,\n",
    "        \"train_coco_ds_ref\": train_coco_ds_ref,\n",
    "        \"val_coco_ds_ref\": val_coco_ds_ref,\n",
    "        \"test_coco_ds_ref\": test_coco_ds_ref\n",
    "    }\n",
    "\n",
    "    if tune.Tuner.can_restore(os.path.abspath(restore_path)):\n",
    "        tuner = tune.Tuner.restore(\n",
    "            os.path.abspath(restore_path),\n",
    "            trainable=ChestnutBurSegmentation,\n",
    "            param_space=config,  # pass same config with new ObjectRefs\n",
    "            resume_unfinished=True,\n",
    "            resume_errored=False\n",
    "        )\n",
    "        print(f\"Tuner Restored from {restore_path}\")\n",
    "    else:\n",
    "        algo = TuneBOHB(seed=666)  # set for identical initial configurations\n",
    "        \n",
    "        algo = ConcurrencyLimiter(algo, max_concurrent=1)\n",
    "\n",
    "        scheduler = HyperBandForBOHB(\n",
    "            time_attr=\"training_iteration\",\n",
    "            max_t=int(max_num_epochs),\n",
    "            reduction_factor=4,\n",
    "            stop_last_trials=False,\n",
    "        )\n",
    "\n",
    "        reporter = JupyterNotebookReporter(overwrite=True,\n",
    "            metric_columns=[\"epoch\", \"train_loss\", \"val_loss\", \"train_mAP\", \"val_mAP\", \"train_mAR\", \"val_mAR\", \"train_f1\", \"val_f1\"],\n",
    "            parameter_columns=[\"lr\", \"resnet\", \"momentum\", \"weight_decay\", \"backbone_lyrs\", \"batch_size\", \"score_thresh\", \"nms_thresh\"],\n",
    "            print_intermediate_tables=True,\n",
    "            sort_by_metric=True\n",
    "        )\n",
    "\n",
    "        tuner = tune.Tuner(\n",
    "            tune.with_resources(\n",
    "                train_ChestnutBurSegmentation,\n",
    "                resources={\"cpu\": 24.0, \"gpu\": 1.0} \n",
    "            ),\n",
    "            run_config=train.RunConfig(\n",
    "                name=f\"BOHB_MaskRCNN_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
    "                failure_config=train.FailureConfig(max_failures=1),\n",
    "                progress_reporter=reporter,\n",
    "            ),\n",
    "            tune_config=tune.TuneConfig(\n",
    "                mode=\"min\",\n",
    "                metric=\"val_loss\",\n",
    "                search_alg=algo,\n",
    "                scheduler=scheduler,\n",
    "                num_samples=int(num_samples),\n",
    "                trial_dirname_creator=trial_dirname_creator\n",
    "            ),\n",
    "            param_space=config\n",
    "        )\n",
    "    results = tuner.fit()\n",
    "\n",
    "    best_trial = results.get_best_result(\"val_f1\", \"max\")\n",
    "\n",
    "    print(\"Best trial config: {}\".format(best_trial.config))\n",
    "    print()\n",
    "    print(\"Best trial final training loss: {}\".format(best_trial.metrics[\"train_loss\"]))\n",
    "    print(\"Best trial final validation loss: {}\".format(best_trial.metrics[\"val_loss\"]))\n",
    "    print(\"Best trial final training mAP: {}\".format(best_trial.metrics[\"train_mAP\"]))\n",
    "    print(\"Best trial final validation mAP: {}\".format(best_trial.metrics[\"val_mAP\"]))\n",
    "    print(\"Best trial final training mAR: {}\".format(best_trial.metrics[\"train_mAR\"]))\n",
    "    print(\"Best trial final validation mAR: {}\".format(best_trial.metrics[\"val_mAR\"]))\n",
    "    print(\"Best trial final training f1-score: {}\".format(best_trial.metrics[\"train_f1\"]))\n",
    "    print(\"Best trial final validation f1-score: {}\".format(best_trial.metrics[\"val_f1\"]))\n",
    "    print()\n",
    "\n",
    "    best_checkpoint = best_trial.get_best_checkpoint(metric=\"val_f1\", mode=\"max\")\n",
    "\n",
    "    test_best_model(best_trial, best_checkpoint)\n",
    "\n",
    "    return train_coco_ds, val_coco_ds, test_coco_ds, results, best_trial\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    train_coco_ds, val_coco_ds, test_coco_ds, results, best_trial = main(num_samples=30,\n",
    "                                                                         max_num_epochs=25,\n",
    "                                                                         restore_path=\"C:/Users/zack/ray_results/FALSE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train Model Using Tuned Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def main(train_coco_ds, val_coco_ds, best_trial):\n",
    "    # Set seed\n",
    "    set_seed(666)\n",
    "\n",
    "    training_steps = [\n",
    "        {\"step\": 0, \"batch_size\": best_trial.config[\"batch_size\"], \"epochs\": 10, \"print_freq\": 25, \"accumulation_steps\": 2},\n",
    "        {\"step\": 1, \"batch_size\": best_trial.config[\"batch_size\"], \"epochs\": 10, \"print_freq\": 25, \"accumulation_steps\": 4},\n",
    "        {\"step\": 2, \"batch_size\": best_trial.config[\"batch_size\"], \"epochs\": 5, \"print_freq\": 25, \"accumulation_steps\": 8}\n",
    "    ]\n",
    "\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "\n",
    "    # Initialize the tensorboard writer\n",
    "    current_datetime = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    writer = SummaryWriter(log_dir=f'C:/Users/exx/Documents/GitHub/SSD_VGG_PyTorch/runs/RetinaNet/{current_datetime}')\n",
    "\n",
    "    # Store one checkpoint dictionary for each epoch in a list of dictionaries. \n",
    "    checkpoints = []\n",
    "\n",
    "    dataset = ChestnutBurSegmentation(image_dir, df, get_transform(train = True))\n",
    "    dataset_val = ChestnutBurSegmentation(image_dir, df, get_transform(train = False))\n",
    "\n",
    "    # dataset = Subset(dataset, indices[:-int(len(indices)*0.2)]) # first 80% of dataset for training                   \n",
    "\n",
    "    # dataset_val = Subset(dataset_val, indices[-int(len(indices)*0.2):-int(len(indices)*0.05)]) # next 15% of dataset for validation\n",
    "\n",
    "    dataset = Subset(dataset, indices[:10])\n",
    "\n",
    "    dataset_val = Subset(dataset_val, indices[10:12])                                          \n",
    "    \n",
    "    data_loader_val = torch.utils.data.DataLoader(\n",
    "        dataset_val, batch_size=1, shuffle=False,\n",
    "        collate_fn=ChestnutBurSegmentation.collate_fn, num_workers=0, pin_memory=True\n",
    "    )\n",
    "    \n",
    "    start_epoch = 0\n",
    "\n",
    "    # Loop through training_steps during training to increase batch size and decrease learning rate\n",
    "    for step in training_steps:\n",
    "        batch_size = step['batch_size']\n",
    "        num_epochs = step['epochs']\n",
    "        print_freq = step['print_freq']\n",
    "        accumulation_steps = step['accumulation_steps']\n",
    "\n",
    "        # Reinitialize the model with the current hyperparameters\n",
    "        model = get_instance_segmentation_model(depth=best_trial.config[\"resnet\"],\n",
    "                                                trainable_backbone_layers=best_trial.config[\"backbone_lyrs\"],\n",
    "                                                image_mean=[0, 0, 0],\n",
    "                                                image_std=[1, 1, 1],\n",
    "                                                num_classes=2, # background, chestnut bur\n",
    "                                                box_score_thresh=best_trial.config[\"score_thresh\"],\n",
    "                                                box_nms_thresh=best_trial.config[\"nms_thresh\"],\n",
    "                                                box_detections_per_img=1000) \n",
    "\n",
    "        model.to(device)\n",
    "\n",
    "        # Construct an optimizer with the suggested learning rate\n",
    "        params = [p for p in model.parameters() if p.requires_grad]\n",
    "        optimizer = torch.optim.SGD(params, lr=best_trial.config[\"lr\"],\n",
    "                                    momentum=best_trial.config[\"momentum\"], \n",
    "                                    weight_decay=best_trial.config[\"weight_decay\"])\n",
    "        \n",
    "        # And a learning rate scheduler\n",
    "        lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min')\n",
    "\n",
    "        # Define training and validation data loaders\n",
    "        data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, \n",
    "                                                  shuffle=True, \n",
    "                                                  collate_fn=ChestnutBurSegmentation.collate_fn, \n",
    "                                                  num_workers=0,\n",
    "                                                  pin_memory=True)\n",
    "        \n",
    "        print(f'Beginning training step {step[\"step\"]}... batch size: {batch_size * accumulation_steps}')\n",
    "\n",
    "        #########################################################\n",
    "        ##               The main training loop                ##\n",
    "        #########################################################\n",
    "        for epoch in range(start_epoch, num_epochs + start_epoch):\n",
    "            # Monitor memory usage at the start of the epoch\n",
    "            print(f\"Epoch {epoch} - Memory allocated: {torch.cuda.memory_allocated(device)} bytes\")\n",
    "\n",
    "            train_metric_logger, val_metric_logger = engine.train_one_epoch(model, optimizer, data_loader, device, \n",
    "                                                                            epoch, print_freq, accumulation_steps,\n",
    "                                                                            data_loader_val)\n",
    "\n",
    "            # Evaluate on the validation dataset\n",
    "            train_coco_evaluator, val_coco_evaluator = engine.evaluate(model, data_loader_val, val_coco_ds, device,\n",
    "                                                                        data_loader, train_coco_ds)\n",
    "            \n",
    "            # Update the learning rate\n",
    "            lr_scheduler.step()\n",
    "\n",
    "            # Store training and validation metrics in checkpoint dictionary. \n",
    "            checkpoint = {\n",
    "                \"epoch\": epoch,\n",
    "                \"train_loss\": train_metric_logger.loss.avg, # average across entire training epoch\n",
    "                \"val_loss\": val_metric_logger.loss.avg,\n",
    "                \"train_mAP\": train_coco_evaluator.coco_eval['segm'].stats[0],# IoU=0.50:0.95\n",
    "                \"train_mAR\": train_coco_evaluator.coco_eval['segm'].stats[8],# IoU=0.50:0.95\n",
    "                \"val_mAP\": val_coco_evaluator.coco_eval['segm'].stats[0],\n",
    "                \"val_mAR\": val_coco_evaluator.coco_eval['segm'].stats[8],\n",
    "                \"train_f1\": calculate_f1_score(train_coco_evaluator.coco_eval['segm'].stats[0], \n",
    "                                                train_coco_evaluator.coco_eval['segm'].stats[8]\n",
    "                                                ),\n",
    "                \"val_f1\": calculate_f1_score(val_coco_evaluator.coco_eval['segm'].stats[0],\n",
    "                                                val_coco_evaluator.coco_eval['segm'].stats[8]\n",
    "                                                ),\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict()\n",
    "            }\n",
    "\n",
    "            # Append checkpoint to checkpoints list\n",
    "            checkpoints.append(checkpoint)\n",
    "\n",
    "            # Report training and validation scalars to tensorboard\n",
    "            writer.add_scalar('Loss/Train', np.array(float(checkpoint[\"train_loss\"])), epoch) # use tags to group scalars\n",
    "            writer.add_scalar('Loss/Val', np.array(float(checkpoint[\"val_loss\"])), epoch)\n",
    "            writer.add_scalar('mAP/Train', np.array(float(checkpoint[\"train_mAP\"])), epoch)\n",
    "            writer.add_scalar('mAP/Val', np.array(float(checkpoint[\"val_mAP\"])), epoch)\n",
    "            writer.add_scalar('mAR/Train', np.array(float(checkpoint[\"train_mAR\"])), epoch)\n",
    "            writer.add_scalar('mAR/Val', np.array(float(checkpoint[\"val_mAR\"])), epoch)\n",
    "            writer.add_scalar('F1/Train', np.array(float(checkpoint[\"train_f1\"])), epoch)\n",
    "            writer.add_scalar('F1/Val', np.array(float(checkpoint[\"val_f1\"])), epoch)\n",
    "\n",
    "            # Clear CUDA cache and collect garbage \n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect() \n",
    "\n",
    "            # Monitor memory usage at the end of the epoch\n",
    "            print(f\"Epoch {epoch} - Max memory allocated: {torch.cuda.max_memory_allocated(device)} bytes\")\n",
    "\n",
    "        # Set start_epoch to current epoch for next training step\n",
    "        start_epoch += num_epochs\n",
    "\n",
    "    print('All Training Steps Complete!')\n",
    "\n",
    "    # Close tensorboard writer\n",
    "    writer.close()\n",
    "\n",
    "    return checkpoints\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    checkpoints = main(train_coco_ds, val_coco_ds, best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best train epoch is dictionary in checkpoints with highest val_mAP_50 value\n",
    "best_train_epoch = max(checkpoints, key = lambda x: x['val_mAP_50'])\n",
    "\n",
    "model = get_instance_segmentation_model(depth=best_trial.config[\"resnet\"],\n",
    "                                                trainable_backbone_layers=best_trial.config[\"backbone_lyrs\"],\n",
    "                                                image_mean=[0, 0, 0],\n",
    "                                                image_std=[1, 1, 1],\n",
    "                                                num_classes=2, # background, chestnut bur\n",
    "                                                box_score_thresh=0.2, # from last training step\n",
    "                                                box_nms_thresh=0.4,\n",
    "                                                box_detections_per_img=1000)\n",
    "\n",
    "# load model weights from best_train_epoch\n",
    "model.load_state_dict(best_train_epoch[\"model_state_dict\"])\n",
    "\n",
    "# save model weights to .pth file\n",
    "torch.save(model.state_dict(), f\"C:/Users/zack/Documents/GitHub/Savanna-Institute/Drone-based_Chestnut_Detection/BurrSegmentation_MaskRCNN_{time.strftime('%Y%m%d')}_{time.strftime('%H%M%S')}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy checkpoints and remove model and optimizer state dicts\n",
    "checkpoints_copy = checkpoints.copy()\n",
    "for c in checkpoints_copy:\n",
    "    del c[\"model_state_dict\"]\n",
    "    del c[\"optimizer_state_dict\"]\n",
    "\n",
    "# save checkpoints list to text file\n",
    "with open(f\"C:/Users/zack/Documents/GitHub/Savanna-Institute/Drone-based_Chestnut_Detection/MaskRCNN_checkpoints_{time.strftime('%Y%m%d')}_{time.strftime('%H%M%S')}.txt\", 'w') as f:\n",
    "    for item in checkpoints_copy:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = ChestnutBurSegmentation(image_dir, df, get_transform(train = False))\n",
    "# dataset_test = Subset(dataset_test, indices[-int(len(indices)*0.05):]) # last 5% of dataset for testing\n",
    "dataset_test = Subset(dataset_test, indices[12:])\n",
    "data_loader_test = torch.utils.data.DataLoader(dataset_test, batch_size=1, shuffle=False,\n",
    "                                                  collate_fn=ChestnutBurSegmentation.collate_fn, \n",
    "                                                  num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary of test indices (as key) and tree_ids (as value)\n",
    "test_dict = {}\n",
    "for i in range(len(dataset_test)):\n",
    "    test_dict[i] = dataset_test.tree_ids[i]\n",
    "\n",
    "\n",
    "# save test_dict to text file just to be safe\n",
    "with open(f\"C:/Users/zack/Documents/GitHub/Savanna-Institute/Drone-based_Chestnut_Detection/MaskRCNN_test_dict_{time.strftime('%Y%m%d')}_{time.strftime('%H%M%S')}.txt\", 'w') as f:\n",
    "    for key, value in test_dict.items():\n",
    "        f.write('%s:%s\\n' % (key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_coco_ds = get_coco_api_from_dataset(dataset_test)\n",
    "test_performance = engine.evaluate(model, data_loader_test, test_coco_ds, device=torch.device('cpu'), train_data_loader=None, train_coco_ds=None)\n",
    "print(f'Best trial test set mAP_50: {test_performance.coco_eval[\"segm\"].stats[0]}') \n",
    "print(f'Best trial test set mAR_100: {test_performance.coco_eval[\"segm\"].stats[8]}')\n",
    "print(f'Best trial test set f1 score: {calculate_f1_score(test_performance.coco_eval[\"segm\"].stats[0], test_performance.coco_eval[\"segm\"].stats[8])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate performance metrics on every image in test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "\n",
    "results = []\n",
    "\n",
    "metric = MeanAveragePrecision(iou_type=\"segm\",\n",
    "                              class_metrics=True,\n",
    "                              max_detection_thresholds=[50, 500, 5000]\n",
    "                              )\n",
    "\n",
    "model.to('cpu')\n",
    "model.eval()\n",
    "\n",
    "for images, targets in data_loader_test:\n",
    "    # use image_id to get image_name from image_names list\n",
    "    image_id = [target['image_id'].item() for target in targets]\n",
    "\n",
    "    # convert boxes in targets to tensors\n",
    "    targets = [{k: torch.tensor(v) if k == 'boxes' else v for k, v in t.items()} for t in targets]\n",
    "\n",
    "    # filter targets to only include boxes and labels keys\n",
    "    ground_truth = [{k: v for k, v in t.items() if k in ('boxes', 'labels')} for t in targets]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prediction = model(images, targets)\n",
    "\n",
    "    # calculate mAP and mAR from test dataset\n",
    "    metric.update(prediction, ground_truth)\n",
    "    mean_AP = metric.compute()\n",
    "\n",
    "    # append image name to mean_AP\n",
    "    mean_AP['tree_id'] = test_dict[image_id[0]]\n",
    "\n",
    "    # Append mean_AP and predictions to results list. \n",
    "    results.append(mean_AP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Store per-image test dataset metrics as dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pandas to create a dataframe of image names and mAP values\n",
    "img_results_df = pd.DataFrame()\n",
    "img_results_df['tree_id'] = [result['tree_id'] for result in results]\n",
    "img_results_df['mAP'] = [result['map'].item() for result in results]\n",
    "img_results_df['mAP_50'] = [result['map_50'].item() for result in results]\n",
    "img_results_df['mAP_75'] = [result['map_75'].item() for result in results]\n",
    "img_results_df['mAP_small'] = [result['map_small'].item() for result in results]\n",
    "img_results_df['mAP_medium'] = [result['map_medium'].item() for result in results]\n",
    "img_results_df['mAP_large'] = [result['map_large'].item() for result in results]\n",
    "img_results_df['mAR_1'] = [result['mar_1'].item() for result in results]\n",
    "img_results_df['mAR_10'] = [result['mar_10'].item() for result in results]\n",
    "img_results_df['mAR_100'] = [result['mar_100'].item() for result in results]\n",
    "img_results_df['mAR_small'] = [result['mar_small'].item() for result in results]\n",
    "img_results_df['mAR_medium'] = [result['mar_medium'].item() for result in results]\n",
    "img_results_df['mAR_large'] = [result['mar_large'].item() for result in results]\n",
    "\n",
    "# # if value is == -1.0, replace with NaN\n",
    "img_results_df = img_results_df.replace(-1.0, np.nan)\n",
    "\n",
    "# Metric values are running averages in torch metrics, so the last value is the final value.\n",
    "final_metrics = img_results_df.iloc[-1]\n",
    "final_metrics = final_metrics.drop('tree_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Print per-image metrics for test dataset as table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "# create a pretty table object\n",
    "x = PrettyTable()\n",
    "\n",
    "cols = ['Metric', 'Value']  \n",
    "\n",
    "# add column headers\n",
    "x.field_names = cols\n",
    "\n",
    "# values for column one in table are column names from final_metrics, column two are the column values. \n",
    "for i in range(len(final_metrics)):\n",
    "    x.add_row([final_metrics.index[i], f'{final_metrics[i]*100:.2f}%'])\n",
    "\n",
    "# print table\n",
    "print(x)\n",
    "\n",
    "# save table as txt file\n",
    "with open(f\"C:/Users/zack/Documents/GitHub/Savanna-Institute/Drone-based_Chestnut_Detection/MaskRCNN_test_dataset_table_{time.strftime('%Y%m%d')}_{time.strftime('%H%M%S')}.txt\", 'w') as f:\n",
    "    print(x, file = f)\n",
    "\n",
    "# save results_df to csv\n",
    "img_results_df.to_csv(f\"C:/Users/zack/Documents/GitHub/Savanna-Institute/Drone-based_Chestnut_Detection/MaskRCNN_test_dataset_results_{time.strftime('%Y%m%d')}_{time.strftime('%H%M%S')}.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load test dataset into single batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load entire test dataset into one batch\n",
    "data_loader_test_singleBatch = torch.utils.data.DataLoader(dataset_test, batch_size = len(dataset_test), shuffle = False,\n",
    "                                                collate_fn = ChestnutBurSegmentation.collate_fn, num_workers = 0)\n",
    "\n",
    "# run predictions on all images in the test dataset\n",
    "images, targets = next(iter(data_loader_test_singleBatch))\n",
    "\n",
    "images = list(image for image in images)\n",
    "targets = [{k: v for k, v in t.items()} for t in targets]\n",
    "\n",
    "# convert boxes in targets to tensors\n",
    "targets = [{k: torch.tensor(v) if k == 'boxes' else v for k, v in t.items()} for t in targets]\n",
    "\n",
    "model.to('cpu')\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = model(images, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Post-process model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each image in the batch, remove all predicted boxes with scores below 0.5\n",
    "for i in range(len(predictions)):\n",
    "    predictions[i]['boxes'] = predictions[i]['boxes'][predictions[i]['scores'] > 0.5]\n",
    "    predictions[i]['labels'] = predictions[i]['labels'][predictions[i]['scores'] > 0.5]\n",
    "    predictions[i]['scores'] = predictions[i]['scores'][predictions[i]['scores'] > 0.5]\n",
    "\n",
    "# resize boxes to original image shape\n",
    "for i in range(len(images)):\n",
    "    tran_w, tran_h = images[i].shape[1], images[i].shape[2]\n",
    "    \n",
    "    images[i] = Image.open(image_dir + test_dict[i] + \".png\")\n",
    "\n",
    "    orig_w, orig_h = images[i].size\n",
    "\n",
    "    predictions[i]['boxes'] = predictions[i]['boxes'] * torch.tensor([orig_w/tran_w, \n",
    "                                                                      orig_h/tran_h, \n",
    "                                                                      orig_w/tran_w,\n",
    "                                                                      orig_h/tran_h]).view(1, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot model predictions for images in test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {0: 'background', 1: 'chestnut bur'}\n",
    "label_color_map = {0: 'black', 1: 'red'}\n",
    "\n",
    "def plot_bbox_predicted(ax, box, label, score, mask=None):\n",
    "    # Add box to the image and use label_color_map to color-code by bounding box class if exists else 'black'\n",
    "    ax.add_patch(plt.Rectangle((box[0], box[1]), box[2] - box[0], box[3] - box[1],\n",
    "                               fill=False,\n",
    "                               color=label_color_map[label.item()] if label.item() in label_color_map else 'black', \n",
    "                               linewidth=1.5))\n",
    "    \n",
    "    # Add label and score to the bounding box. Concatenate label and score to one string. \n",
    "    # Use label_dict to replace class numbers with class names\n",
    "    ax.text(box[0], box[1] - 10,\n",
    "            s=f\"{label_dict[label.item()]} {score.item():.2f}\",\n",
    "            color='black',\n",
    "            fontsize=6,\n",
    "            verticalalignment='top',\n",
    "            bbox={'color': label_color_map[label.item()] if label.item() in label_color_map else 'black', 'pad': 0})\n",
    "    \n",
    "    # Plot the mask if provided\n",
    "    if mask is not None:\n",
    "        mask = mask.cpu().numpy()\n",
    "        mask = np.ma.masked_where(mask == 0, mask)\n",
    "        ax.imshow(mask, alpha=0.5, cmap='jet')\n",
    "\n",
    "    return ax\n",
    "\n",
    "# function for plotting image\n",
    "def img_show(image, ax = None, figsize = (6, 9)):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize = figsize)\n",
    "    ax.xaxis.tick_top()\n",
    "    ax.imshow(image)\n",
    "    return ax\n",
    "\n",
    "# Function for plotting all predictions on images\n",
    "def plot_predictions(image, boxes, labels, scores, masks=None, ax=None):\n",
    "    ax = img_show(image, ax=ax)\n",
    "    for i in range(len(boxes)):\n",
    "        box = boxes[i].cpu().numpy()\n",
    "        mask = masks[i] if masks is not None else None\n",
    "        plot_bbox_predicted(ax, box, labels[i], scores[i], mask)\n",
    "\n",
    "# Plot batch of images and predictions\n",
    "plt.figure(figsize=(24, 36))\n",
    "for i in range(0, len(dataset_test)):\n",
    "    ax = plt.subplot(8, 4, 1 + i)\n",
    "    plot_predictions(images[i], predictions[i]['boxes'], predictions[i]['labels'], predictions[i]['scores'], predictions[i]['masks'], ax=ax)\n",
    "    tree_id = dataset_test.tree_ids[i]  # Assuming dataset_test is available and has tree_ids\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Tree ID: {tree_id}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run inference on full dataset to calculate confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_all = ChestnutBurSegmentation(image_dir, df, get_transform(train = False))\n",
    "data_loader_all = torch.utils.data.DataLoader(dataset_all, batch_size=1, shuffle=False,\n",
    "                                                  collate_fn=ChestnutBurSegmentation.collate_fn, \n",
    "                                                  num_workers=0, pin_memory=True)\n",
    "\n",
    "# get model predictions for every image in data_loader_all\n",
    "model_predictions_all = []\n",
    "\n",
    "for images, targets in data_loader_all:\n",
    "    # use image_id to get image_name from image_names list\n",
    "    image_id = [target['image_id'].item() for target in targets]\n",
    "\n",
    "    # convert boxes in targets to tensors\n",
    "    targets = [{k: torch.tensor(v) if k == 'boxes' else v for k, v in t.items()} for t in targets]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prediction = model(images, targets)\n",
    "\n",
    "    # append image name to prediction\n",
    "    prediction['tree_id'] = test_dict[image_id[0]]\n",
    "\n",
    "    # Append mean_AP and predictions to results list. \n",
    "    model_predictions_all.append(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert model_predictions_all to a dataframe\n",
    "model_predictions_df = pd.DataFrame(model_predictions_all)\n",
    "\n",
    "# save csv for comparison with ground truth\n",
    "model_predictions_df.to_csv(f\"C:/Users/zack/Documents/GitHub/Savanna-Institute/Drone-based_Chestnut_Detection/MaskRCNN_full_dataset_results_{time.strftime('%Y%m%d')}_{time.strftime('%H%M%S')}.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bohb_pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
