{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Chestnut Bur Detection and Segmentation using MaskRCNN in PyTorch</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, Subset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms as _transforms, tv_tensors\n",
    "import torchvision.transforms.v2 as T\n",
    "from cv2 import fillPoly\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from matplotlib.patches import Polygon\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <center> Load the image and annotation data </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load annotations from json file here: \"C:\\Users\\exx\\Downloads\\Route 9 Orchard 4.v1-test_dataset.coco-segmentation\\train\\_annotations.coco.json\"\n",
    "annos = json.load(open(\"C:/Users/exx/Downloads/Route 9 Orchard 4.v1-test_dataset.coco-segmentation/train/_annotations.coco.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in annos.keys():\n",
    "    print(annos[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the annos dict to a df, where each row is an image and the columns are the file name, the category name,\n",
    "# the polygon coords, bbox, area, and iscrowd\n",
    "annos_df = pd.DataFrame(annos[\"annotations\"])\n",
    "df = pd.DataFrame()\n",
    "df[\"tree_id\"] = annos_df[\"image_id\"].apply(lambda x: annos[\"images\"][x][\"file_name\"].split(\"_\")[0])\n",
    "df[\"file_name\"] = annos_df[\"image_id\"].apply(lambda x: annos[\"images\"][x][\"file_name\"])\n",
    "df[\"file_name\"] = df[\"file_name\"].apply(lambda x: x.split(\"_\")[0] + \".png\")\n",
    "categories = [cat[\"name\"] for cat in annos[\"categories\"]]\n",
    "df[\"category_name\"] = annos_df[\"category_id\"].apply(lambda x: categories[x])\n",
    "df[\"bbox\"] = annos_df[\"bbox\"].apply(lambda x: torch.tensor(x))\n",
    "df[\"area\"] = annos_df[\"area\"].apply(lambda x: torch.tensor(x))\n",
    "df[\"segmentation\"] = annos_df[\"segmentation\"].apply(lambda x: torch.tensor(x))\n",
    "df[\"iscrowd\"] = annos_df[\"iscrowd\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = \"C:\\\\Users\\\\exx\\\\EasyIDP\\\\Route9_Orchard4\\\\Outputs\\\\Roboflow\\\\images\"\n",
    "\n",
    "image_names = df[\"file_name\"].unique()\n",
    "\n",
    "filtered_image_dir = \"C:\\\\Users\\\\exx\\\\Deep Learning\\\\Chestnut_Bur_Instance_Segmentation\\\\filtered_images\"\n",
    "Path(filtered_image_dir).mkdir(exist_ok=True)\n",
    "for image_name in image_names:\n",
    "    shutil.copy(image_dir + \"\\\\\" + image_name, filtered_image_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <center> Plot sample image and annotation data </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_fill(mask, polys, color):\n",
    "    for poly in polys:\n",
    "        fillPoly(mask, [poly], color)\n",
    "    return mask\n",
    "\n",
    "def plot_images_with_masks(image_dir: Path, tree_id_list, df: pd.DataFrame):\n",
    "    fig, axs = plt.subplots(1, len(tree_id_list), figsize=(20, 10))\n",
    "    for i, tree_id in enumerate(tree_id_list):\n",
    "        image_name = df[df[\"tree_id\"] == tree_id][\"file_name\"].values[0]\n",
    "        image = Image.open(Path(image_dir) / image_name)\n",
    "\n",
    "        canopy_poly = df[(df[\"tree_id\"] == tree_id) & (df[\"category_name\"] == \"canopy\")][\"segmentation\"].values\n",
    "        bur_poly = df[(df[\"tree_id\"] == tree_id) & (df[\"category_name\"] == \"chestnut bur\")][\"segmentation\"].values\n",
    "\n",
    "        canopy_poly = [np.array(poly[0]).reshape(-1, 2).astype(np.int32) for poly in canopy_poly]\n",
    "        bur_poly = [np.array(poly[0]).reshape(-1, 2).astype(np.int32) for poly in bur_poly]\n",
    "\n",
    "        bur_masks = []\n",
    "        for poly in bur_poly:\n",
    "            bur_mask = np.zeros((image.height, image.width), dtype=np.uint8)\n",
    "            bur_mask = mask_fill(bur_mask, [poly], 2)\n",
    "            bur_masks.append(bur_mask)\n",
    "\n",
    "        # Stack all masks together\n",
    "        if bur_masks:\n",
    "            background_mask = np.zeros((image.height, image.width, 1), dtype=np.uint8)\n",
    "            canopy_mask = np.zeros((image.height, image.width, 1), dtype=np.uint8)\n",
    "            canopy_mask = mask_fill(canopy_mask, canopy_poly, 1)\n",
    "            bur_masks_stacked = np.stack(bur_masks, axis=-1)\n",
    "            mask_image = np.concatenate([background_mask, canopy_mask, bur_masks_stacked], axis=-1)\n",
    "        else:\n",
    "            background_mask = np.zeros((image.height, image.width), dtype=np.uint8)\n",
    "            canopy_mask = np.zeros((image.height, image.width), dtype=np.uint8)\n",
    "            canopy_mask = mask_fill(canopy_mask, canopy_poly, 1)\n",
    "            mask_image = np.stack((background_mask, canopy_mask), axis=-1)\n",
    "\n",
    "        axs[i].imshow(image)\n",
    "        axs[i].imshow(mask_image.sum(axis=-1), alpha=0.5)\n",
    "        axs[i].axis(\"off\")\n",
    "        axs[i].set_title(f'Tree ID: {tree_id}')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_images_with_masks(image_dir, [\"100\", \"11\"], df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <center> Pre-process and transform image and annotation data </center>\n",
    "\n",
    "##### Adapted from: https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html#an-instance-segmentation-model-for-pennfudan-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset loader (PyTorch) for loading images and annotation data\n",
    "class ChestnutBurSegmentation(Dataset):\n",
    "    \"\"\"Custom Dataset for Chestnut Bur Segmentation in UAV Images\"\"\"\n",
    "\n",
    "    def __init__(self, image_dir, df, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        self.unique_tree_ids = self.df[\"tree_id\"].unique()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        tree_id = self.unique_tree_ids[idx]\n",
    "\n",
    "        row = self.df[self.df[\"tree_id\"] == tree_id].iloc[0]\n",
    "\n",
    "        image_file = Path(self.image_dir) / row[\"file_name\"]\n",
    "\n",
    "        image = tv_tensors.Image(Image.open(image_file))\n",
    "\n",
    "        height, width = image.shape[-2:]\n",
    "\n",
    "        canopy_poly = self.df[(self.df[\"tree_id\"] == tree_id) & (self.df[\"category_name\"] == \"canopy\")][\"segmentation\"].values\n",
    "        bur_poly = self.df[(self.df[\"tree_id\"] == tree_id) & (self.df[\"category_name\"] == \"chestnut bur\")][\"segmentation\"].values\n",
    "\n",
    "        canopy_poly = [np.array(poly[0]).reshape(-1, 2).astype(np.int32) for poly in canopy_poly]\n",
    "        bur_poly = [np.array(poly[0]).reshape(-1, 2).astype(np.int32) for poly in bur_poly]\n",
    "\n",
    "        canopy_bbox = self.df[(self.df[\"tree_id\"] == tree_id) & (self.df[\"category_name\"] == \"canopy\")][\"bbox\"].values\n",
    "        canopy_bbox = [torch.tensor([canopy_bbox[0][0], canopy_bbox[0][1], canopy_bbox[0][2], canopy_bbox[0][3]], dtype = torch.float32)]\n",
    "        canopy_bbox = torch.stack([bbox for bbox in canopy_bbox], dim=0) # (n_objects, 4)\n",
    "\n",
    "        canopy_mask = np.zeros((height, width), dtype=np.uint8)\n",
    "        canopy_mask = mask_fill(canopy_mask, canopy_poly, 1)\n",
    "\n",
    "        bur_masks = []\n",
    "        # One mask per bur. Store each bur mask in a list to stack later. \n",
    "        for poly in bur_poly:\n",
    "            bur_mask = np.zeros((height, width), dtype=np.uint8)\n",
    "            bur_mask = mask_fill(bur_mask, [poly], 2)\n",
    "            bur_masks.append(bur_mask)\n",
    "\n",
    "        if bur_masks:\n",
    "            background_mask = np.zeros((height, width, 1), dtype=np.uint8)\n",
    "            bur_masks_stacked = np.stack(bur_masks, axis=-1)\n",
    "            mask_image = np.concatenate([background_mask, bur_masks_stacked], axis=-1).transpose(2, 0, 1)\n",
    "            labels = self.df[(self.df[\"tree_id\"] == tree_id) & (self.df[\"category_name\"] == \"chestnut bur\")][\"category_name\"].values\n",
    "            labels = [categories.index(label) for label in labels]\n",
    "            bboxes = self.df[(self.df[\"tree_id\"] == tree_id) & (self.df[\"category_name\"] == \"chestnut bur\")][\"bbox\"].values\n",
    "            bboxes = [torch.tensor([bbox[0], bbox[1], bbox[0] + bbox[2], bbox[1] + bbox[3]], dtype = torch.float32) for bbox in bboxes] # convert to xyxy format\n",
    "            bboxes = torch.stack([bbox for bbox in bboxes], dim=0) # (n_objects, 4)\n",
    "            area = self.df[(self.df[\"tree_id\"] == tree_id) & (self.df[\"category_name\"] == \"chestnut bur\")][\"area\"].values\n",
    "            iscrowd = self.df[(self.df[\"tree_id\"] == tree_id) & (self.df[\"category_name\"] == \"chestnut bur\")][\"iscrowd\"].values\n",
    "        else:\n",
    "            background_mask = np.zeros((height, width), dtype=np.uint8)\n",
    "            mask_image = np.stack((background_mask, canopy_mask), axis=-1).transpose(2, 0, 1)\n",
    "            labels = self.df[(self.df[\"tree_id\"] == tree_id) & (self.df[\"category_name\"] == \"canopy\")][\"category_name\"].values\n",
    "            labels = [categories.index(label) for label in labels]\n",
    "            bboxes = canopy_bbox\n",
    "            area = self.df[(self.df[\"tree_id\"] == tree_id) & (self.df[\"category_name\"] == \"canopy\")][\"area\"].values\n",
    "            iscrowd = self.df[(self.df[\"tree_id\"] == tree_id) & (self.df[\"category_name\"] == \"canopy\")][\"iscrowd\"].values\n",
    "\n",
    "        # fill image background (outside of tree canopy)\n",
    "        image = image * np.array(canopy_mask).astype(np.uint8)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = tv_tensors.BoundingBoxes(bboxes, format=tv_tensors.BoundingBoxFormat.XYXY, canvas_size=(height, width)) \n",
    "        target[\"labels\"] = torch.as_tensor(labels, dtype = torch.int64) # (n_objects)\n",
    "        target[\"image_id\"] = int(tree_id)\n",
    "        target[\"area\"] = torch.stack([a.clone().detach().float() for a in area], dim=0) # (n_objects)\n",
    "        target[\"iscrowd\"] = torch.stack([torch.tensor(ic, dtype = torch.int64) for ic in iscrowd], dim=0) # (n_objects)\n",
    "            \n",
    "        if self.transform is not None:\n",
    "            # normalize image using imagenet stats\n",
    "            image = T.Compose([T.ToDtype(torch.float32, scale=True),\n",
    "                               T.ColorJitter(), \n",
    "                               T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])(image)\n",
    "            image_and_mask = torch.cat((image, torch.tensor(mask_image, dtype=torch.float32)), dim=0)\n",
    "            image_and_mask, target = self.transform(image_and_mask, target)\n",
    "            image = image_and_mask[:image.shape[0], :, :] # slice out the transformed image\n",
    "            mask_image = image_and_mask[image.shape[0]:, :].clone().detach().to(torch.uint8) # slice out the transformed mask\n",
    "            target[\"masks\"] = mask_image\n",
    "        else:\n",
    "            image = image.to(torch.float32)\n",
    "            target[\"masks\"] = torch.tensor(mask_image, dtype=torch.uint8)\n",
    "\n",
    "        return image, target\n",
    "        \n",
    "    def collate_fn(batch):\n",
    "        return tuple(zip(*batch))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.unique_tree_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define transforms for the dataset\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    if train:\n",
    "        transforms.append(T.RandomZoomOut(fill = {tv_tensors.Image: (255, 20, 147), tv_tensors.Mask: (0,0,0)},\n",
    "                            p = 0.5,\n",
    "                            side_range = (1.0, 1.4)))\n",
    "        transforms.append(T.Resize((448, 448), interpolation=T.InterpolationMode.NEAREST, antialias = True)) # no maintain aspect ratio\n",
    "        transforms.append(T.RandomRotation(degrees = 45, expand = False))\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "        transforms.append(T.RandomVerticalFlip(0.5))\n",
    "    else:\n",
    "        transforms.append(T.Resize((448, 448), interpolation=T.InterpolationMode.NEAREST, antialias = True)) # no maintain aspect ratio\n",
    "    transforms.append(T.ClampBoundingBoxes())\n",
    "    transforms.append(T.SanitizeBoundingBoxes())\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ds = ChestnutBurSegmentation(filtered_image_dir, df, get_transform(train = True))\n",
    "sample_dl = DataLoader(sample_ds, batch_size = 24, shuffle = True, collate_fn = ChestnutBurSegmentation.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, targets = next(iter(sample_dl))\n",
    "images = [img for img in images]\n",
    "targets = [{k: v for k, v in target.items()} for target in targets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <center> Plot sample transformed images, targets, and masks </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "tree_ids = [target[\"image_id\"] for target in targets]\n",
    "\n",
    "image = images[tree_ids.index(11)].permute(1, 2, 0) if 11 in tree_ids else images[0].permute(1, 2, 0)\n",
    "mask = targets[tree_ids.index(11)][\"masks\"].permute(1, 2, 0) if 11 in tree_ids else targets[0][\"masks\"].permute(1, 2, 0)\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.title(\"transformed image\")\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(mask.sum(axis=2), cmap=\"gray\")\n",
    "plt.title(\"chestnut bur masks\")\n",
    "plt.show()\n",
    "\n",
    "# plot mask on image\n",
    "plt.imshow(image)\n",
    "plt.imshow(mask.sum(axis=2), cmap='gray', alpha=0.7)\n",
    "plt.title(\"training sample\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <center> Construct MaskRCNN Model </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_instance_segmentation_model(num_classes):\n",
    "    # Load a Mask R-CNN instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(\n",
    "        weights=torchvision.models.detection.MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,\n",
    "        weights_backbone=torchvision.models.ResNet50_Weights.DEFAULT\n",
    "    )\n",
    "\n",
    "    # Get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "    # Replace the pre-trained head with a new one to reflect the number of classes\n",
    "    model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # Get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "\n",
    "    # Replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = torchvision.models.detection.mask_rcnn.MaskRCNNPredictor(\n",
    "        in_features_mask,\n",
    "        hidden_layer,\n",
    "        num_classes\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_instance_segmentation_model(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <center> Tune model hyperparameters using bayesian optimization algo and hyperband scheduler </center>\n",
    "\n",
    "##### adapted from: https://docs.ray.io/en/latest/tune/examples/bohb_example.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian Optimization HyperBand (BOHB) with HyperBand scheduler\n",
    "# # https://proceedings.mlr.press/v80/falkner18a.html\n",
    "\n",
    "import tempfile\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import ray\n",
    "from ray import train, tune\n",
    "from ray.tune.schedulers.hb_bohb import HyperBandForBOHB\n",
    "from ray.tune.search.bohb import TuneBOHB\n",
    "import ConfigSpace as CS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(step, width, height, activation):\n",
    "    time.sleep(0.1)\n",
    "    activation_boost = 10 if activation==\"relu\" else 1\n",
    "    return (0.1 + width * step / 100) ** (-1) + height * 0.1 + activation_boost\n",
    "\n",
    "def objective(config):\n",
    "    start = 0\n",
    "    if train.get_checkpoint():\n",
    "        with train.get_checkpoint().as_directory() as checkpoint_dir:\n",
    "            start = int((Path(checkpoint_dir) / \"data.ckpt\").read_text())\n",
    "\n",
    "    for step in range(start, config[\"steps\"]):\n",
    "        score = evaluate(step, config[\"width\"], config[\"height\"], config[\"activation\"])\n",
    "        with tempfile.TemporaryDirectory() as checkpoint_dir:\n",
    "            (Path(checkpoint_dir) / \"data.ckpt\").write_text(str(step))\n",
    "            train.report(\n",
    "                {\"iterations\": step, \"mean_loss\": score},\n",
    "                checkpoint=train.Checkpoint.from_directory(checkpoint_dir)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    \"steps\": 100,\n",
    "    \"width\": tune.uniform(0, 20),\n",
    "    \"height\": tune.uniform(-100, 100),\n",
    "    \"activation\": tune.choice([\"relu\", \"tanh\"]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = TuneBOHB()\n",
    "algo = tune.search.ConcurrencyLimiter(algo, max_concurrent=4)\n",
    "scheduler = HyperBandForBOHB(\n",
    "    time_attr=\"training_iteration\",\n",
    "    max_t=100,\n",
    "    reduction_factor=4,\n",
    "    stop_last_trials=False,\n",
    ")\n",
    "\n",
    "num_samples = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = tune.Tuner(\n",
    "    objective,\n",
    "    tune_config=tune.TuneConfig(\n",
    "        metric=\"mean_loss\",\n",
    "        mode=\"min\",\n",
    "        search_alg=algo,\n",
    "        scheduler=scheduler,\n",
    "        num_samples=num_samples,\n",
    "    ),\n",
    "    run_config=train.RunConfig(\n",
    "        name=\"bohb_exp\",\n",
    "        stop={\"training_iteration\": 100},\n",
    "    ),\n",
    "    param_space=search_space,\n",
    ")\n",
    "results = tuner.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <center> Train the model using tuned hyperparameters </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detection_pytorch import engine\n",
    "\n",
    "# train on the GPU or on the CPU, if a GPU is not available\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "num_classes = 3 # (background = 0, canopy = 1, chestnut bur = 2)\n",
    "\n",
    "# use our dataset and defined transformations\n",
    "train_ds = ChestnutBurSegmentation(filtered_image_dir, df, get_transform(train = True))\n",
    "valid_ds = ChestnutBurSegmentation(filtered_image_dir, df, get_transform(train = False))\n",
    "\n",
    "# store training indices in random order list\n",
    "indices = torch.randperm(len(train_ds)).tolist()\n",
    "splits = [int(len(indices) * 0.8), len(indices) - int(len(indices) * 0.8)]\n",
    "\n",
    "train_ds = Subset(train_ds, splits[0])\n",
    "valid_ds = Subset(valid_ds, splits[1])\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size = 8, shuffle = True, collate_fn = ChestnutBurSegmentation.collate_fn)\n",
    "valid_dl = DataLoader(valid_ds, batch_size = 1, shuffle = False, collate_fn = ChestnutBurSegmentation.collate_fn)\n",
    "\n",
    "# get the model using our helper function\n",
    "model = get_instance_segmentation_model(num_classes)\n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(\n",
    "    params,\n",
    "    lr=0.005,\n",
    "    momentum=0.9,\n",
    "    weight_decay=0.0005\n",
    ")\n",
    "\n",
    "# and a learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=3,\n",
    "    gamma=0.1\n",
    ")\n",
    "\n",
    "num_epochs = 2\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    engine.train_one_epoch(model, optimizer, train_dl, device, epoch, print_freq=10)\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the test dataset\n",
    "    engine.evaluate(model, valid_dl, device=device)\n",
    "\n",
    "print(\"That's it!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
