{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, Subset, DataLoader\n",
    "from torchvision import transforms as _transforms, tv_tensors\n",
    "import torchvision.transforms.v2 as T\n",
    "import json\n",
    "from matplotlib import patches\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import gc\n",
    "import random\n",
    "import os\n",
    "from torch_lr_finder import LRFinder, TrainDataLoaderIter\n",
    "from datetime import datetime\n",
    "import time\n",
    "import timm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <center> Load Images and Annotations </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load annotations from json file\n",
    "annos = json.load(open(\"S:/Zack/Deep Learning/Chestnut_Bur_Instance_Segmentation/route9_orchard3/data/_annotations.coco.json\"))\n",
    "\n",
    "# convert the annos dict to a df\n",
    "annos_df = pd.DataFrame(annos[\"annotations\"])\n",
    "df = pd.DataFrame()\n",
    "df[\"tree_id\"] = annos_df[\"image_id\"].apply(lambda x: annos[\"images\"][x][\"file_name\"].split(\"_\")[0])\n",
    "df[\"file_name\"] = annos_df[\"image_id\"].apply(lambda x: annos[\"images\"][x][\"file_name\"])\n",
    "df[\"file_name\"] = df[\"file_name\"].apply(lambda x: x.split(\"_\")[0] + \".png\")\n",
    "categories = [cat[\"name\"] for cat in annos[\"categories\"]]\n",
    "df[\"category_name\"] = annos_df[\"category_id\"].apply(lambda x: categories[x])\n",
    "df[\"bbox\"] = annos_df[\"bbox\"].apply(lambda x: torch.tensor(x))\n",
    "df[\"area\"] = annos_df[\"area\"].apply(lambda x: torch.tensor(x))\n",
    "df[\"segmentation\"] = annos_df[\"segmentation\"].apply(lambda x: torch.tensor(x))\n",
    "df[\"iscrowd\"] = annos_df[\"iscrowd\"]\n",
    "\n",
    "reviewed_trees = [392, 60, 44, 91, 272, 51, 152, 329, 286, 320, 280, 366, 304] # subset for testing \n",
    "\n",
    "df = df[df[\"tree_id\"].isin([str(tree_id) for tree_id in reviewed_trees])]\n",
    "\n",
    "image_names = df[\"file_name\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <center> Image Pre-processing </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_polygon_mask(mask, polys, color):\n",
    "#     for poly in polys:\n",
    "#         poly_mask = Image.new(\"L\", (mask.shape[1], mask.shape[0]), 0)\n",
    "#         ImageDraw.Draw(poly_mask).polygon(poly, fill=color)\n",
    "#         poly_mask = np.array(poly_mask)\n",
    "#         mask = np.maximum(mask, poly_mask)\n",
    "#     return mask\n",
    "\n",
    "# def transform_bbox_to_tile_coords(bbox, tile_left, tile_top, global_left, global_top, tile_width, tile_height):\n",
    "#     x, y, w, h = bbox\n",
    "#     x_padded = x - global_left\n",
    "#     y_padded = y - global_top\n",
    "#     x_in_tile = x_padded - tile_left\n",
    "#     y_in_tile = y_padded - tile_top\n",
    "#     x2_in_tile = x_in_tile + w\n",
    "#     y2_in_tile = y_in_tile + h\n",
    "#     x_clamped = max(0, min(tile_width, x_in_tile))\n",
    "#     y_clamped = max(0, min(tile_height, y_in_tile))\n",
    "#     x2_clamped = max(0, min(tile_width, x2_in_tile))\n",
    "#     y2_clamped = max(0, min(tile_height, y2_in_tile))\n",
    "#     clipped_w = x2_clamped - x_clamped\n",
    "#     clipped_h = y2_clamped - y_clamped\n",
    "#     if clipped_w <= 0 or clipped_h <= 0:\n",
    "#         return None\n",
    "#     return (x_clamped, y_clamped, clipped_w, clipped_h)\n",
    "\n",
    "# # Define output directories\n",
    "# base_output_dir = Path('S:/Zack/Deep Learning/Chestnut_Bur_Instance_Segmentation/U-Net/data')\n",
    "# preprocessed_image_dir = base_output_dir / 'images'\n",
    "# mask_output_dir = base_output_dir / 'masks'\n",
    "\n",
    "# preprocessed_image_dir.mkdir(parents=True, exist_ok=True)\n",
    "# mask_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# image_dir = 'S:/Zack/Deep Learning/Chestnut_Bur_Instance_Segmentation/route9_orchard3/data/images'\n",
    "# preprocessed_data = []\n",
    "\n",
    "# # Overlap settings\n",
    "# tile_height, tile_width = 224, 224\n",
    "# overlap_pct = 0.20  # 20% overlap\n",
    "# stride_y = int(tile_height * (1 - overlap_pct))\n",
    "# stride_x = int(tile_width * (1 - overlap_pct))\n",
    "\n",
    "# for file_name in df[\"file_name\"].unique():\n",
    "#     try:\n",
    "#         print(f\"Processing file: {file_name}\")\n",
    "#         row = df[df[\"file_name\"] == file_name]\n",
    "#         image_path = Path(image_dir) / row[\"file_name\"].values[0]\n",
    "#         image = Image.open(image_path).convert('RGB')\n",
    "#         image_np = np.array(image)\n",
    "\n",
    "#         # Process canopy polygon\n",
    "#         canopy_poly = row[row[\"category_name\"] == \"Canopy\"][\"segmentation\"].values\n",
    "#         canopy_poly = [np.array(poly[0]).reshape(-1, 2).tolist() for poly in canopy_poly]\n",
    "#         canopy_poly = [[tuple(p) for p in poly] for poly in canopy_poly]\n",
    "#         canopy_mask = np.zeros((image_np.shape[0], image_np.shape[1]), dtype=np.uint8)\n",
    "#         canopy_mask = create_polygon_mask(canopy_mask, canopy_poly, 1)\n",
    "\n",
    "#         # Process Chestnut-burr polygons\n",
    "#         bur_poly = row[row[\"category_name\"] == \"Chestnut-burr\"][\"segmentation\"].values\n",
    "#         bur_poly = [np.array(poly[0]).reshape(-1, 2).tolist() for poly in bur_poly]\n",
    "#         bur_poly = [[tuple(p) for p in poly] for poly in bur_poly]\n",
    "#         bur_masks = []\n",
    "#         for poly in bur_poly:\n",
    "#             mask = np.zeros((image_np.shape[0], image_np.shape[1]), dtype=np.uint8)\n",
    "#             mask = create_polygon_mask(mask, [poly], 1)\n",
    "#             bur_masks.append(mask)\n",
    "#         mask_image = np.stack(bur_masks, axis=0)\n",
    "#         mask_image = mask_image.transpose(1, 2, 0)\n",
    "\n",
    "#         # Crop the image using canopy bounding box with padding\n",
    "#         canopy_bbox = row[row[\"category_name\"] == \"Canopy\"][\"bbox\"].values[0]\n",
    "#         padding = 50.0\n",
    "#         padded_bbox = [\n",
    "#             max(0, int(canopy_bbox[0] - padding)),\n",
    "#             max(0, int(canopy_bbox[1] - padding)),\n",
    "#             min(image_np.shape[1], int(canopy_bbox[0] + canopy_bbox[2] + padding)),\n",
    "#             min(image_np.shape[0], int(canopy_bbox[1] + canopy_bbox[3] + padding))\n",
    "#         ]\n",
    "#         image_cropped = image_np[padded_bbox[1]:padded_bbox[3], padded_bbox[0]:padded_bbox[2]]\n",
    "#         mask_cropped = mask_image[padded_bbox[1]:padded_bbox[3], padded_bbox[0]:padded_bbox[2], :]\n",
    "\n",
    "#         # Fill background with black outside of canopy mask\n",
    "#         canopy_mask_cropped = canopy_mask[padded_bbox[1]:padded_bbox[3], padded_bbox[0]:padded_bbox[2]].astype(bool)\n",
    "#         fill_color = [0, 0, 0]\n",
    "#         for c in range(3):\n",
    "#             image_cropped[..., c][~canopy_mask_cropped] = fill_color[c]\n",
    "\n",
    "#         transform = T.Compose([\n",
    "#             T.ToImage(),\n",
    "#             T.ToDtype(torch.float32, scale=True),\n",
    "#             T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "#         ])\n",
    "#         image_cropped = transform(image_cropped).permute(1, 2, 0).numpy()  # Normalize and convert back to [H, W, C]\n",
    "\n",
    "#         image_height, image_width, _ = image_cropped.shape\n",
    "#         padded_height = ((image_height + tile_height - 1) // tile_height) * tile_height\n",
    "#         padded_width = ((image_width + tile_width - 1) // tile_width) * tile_width\n",
    "\n",
    "#         padded_image = np.zeros((padded_height, padded_width, 3), dtype=np.float32)\n",
    "#         padded_image[:image_height, :image_width, :] = image_cropped\n",
    "\n",
    "#         padded_mask = np.zeros((padded_height, padded_width, mask_cropped.shape[2]), dtype=np.uint8)\n",
    "#         padded_mask[:image_height, :image_width, :] = mask_cropped\n",
    "\n",
    "#         for tile_y in range(0, padded_height - tile_height + 1, stride_y):\n",
    "#             for tile_x in range(0, padded_width - tile_width + 1, stride_x):\n",
    "#                 tile_image = padded_image[tile_y:tile_y+tile_height, tile_x:tile_x+tile_width, :]\n",
    "#                 tile_mask = padded_mask[tile_y:tile_y+tile_height, tile_x:tile_x+tile_width, :]\n",
    "#                 # Only remove tiles that are entirely padding (i.e., tile_image is all zeros)\n",
    "#                 if np.all(tile_image == 0):\n",
    "#                     continue\n",
    "\n",
    "#                 tile_bboxes = []\n",
    "#                 tile_masks = []\n",
    "\n",
    "#                 bur_rows = row[row[\"category_name\"] == \"Chestnut-burr\"]\n",
    "#                 for bbox_arr, mask_arr in zip(bur_rows[\"bbox\"].values, bur_rows[\"segmentation\"].values):\n",
    "#                     tile_bbox = transform_bbox_to_tile_coords(\n",
    "#                         bbox_arr,\n",
    "#                         tile_x, tile_y,\n",
    "#                         padded_bbox[0], padded_bbox[1],\n",
    "#                         tile_width, tile_height\n",
    "#                     )\n",
    "#                     if tile_bbox is None:\n",
    "#                         continue\n",
    "#                     bx, by, bw, bh = tile_bbox\n",
    "#                     tile_bboxes.append({\n",
    "#                         \"tree_id\": bur_rows[\"tree_id\"].values[0],\n",
    "#                         \"file_name\": f\"{Path(file_name).stem}_{tile_y}_{tile_x}\",\n",
    "#                         \"bbox_x\": float(bx),\n",
    "#                         \"bbox_y\": float(by),\n",
    "#                         \"bbox_w\": float(bw),\n",
    "#                         \"bbox_h\": float(bh),\n",
    "#                         \"category_name\": \"Chestnut-burr\",\n",
    "#                         \"iscrowd\": int(bur_rows[\"iscrowd\"].values[0]),\n",
    "#                         \"tile_x\": tile_x,\n",
    "#                         \"tile_y\": tile_y\n",
    "#                     })\n",
    "\n",
    "#                     # Create mask for the bounding box\n",
    "#                     mask = np.zeros((tile_height, tile_width), dtype=np.uint8)\n",
    "#                     poly = np.array(mask_arr[0]).reshape(-1, 2).tolist()\n",
    "#                     poly = [(p[0] - padded_bbox[0] - tile_x, p[1] - padded_bbox[1] - tile_y) for p in poly]\n",
    "#                     mask = create_polygon_mask(mask, [poly], 1)\n",
    "#                     tile_masks.append(mask)\n",
    "\n",
    "#                 if not tile_bboxes:\n",
    "#                     continue\n",
    "\n",
    "#                 # Stack the masks to create the mask image\n",
    "#                 tile_mask = np.stack(tile_masks, axis=2)\n",
    "\n",
    "#                 tile_image_path = preprocessed_image_dir / f\"{Path(file_name).stem}_{tile_y}_{tile_x}.npy\"\n",
    "#                 tile_mask_path = mask_output_dir / f\"{Path(file_name).stem}_{tile_y}_{tile_x}.npz\"\n",
    "#                 np.save(tile_image_path, tile_image)\n",
    "#                 np.savez_compressed(tile_mask_path, tile_mask)\n",
    "\n",
    "#                 preprocessed_data.extend(tile_bboxes)\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing file {file_name}: {e}\")\n",
    "\n",
    "# preprocessed_df = pd.DataFrame(preprocessed_data)\n",
    "# updated_csv_path = base_output_dir / 'annotations.csv'\n",
    "# preprocessed_df.to_csv(updated_csv_path, index=False)\n",
    "\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <center> PyTorch Dataset </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChestnutBurSegmentation(Dataset):\n",
    "    def __init__(self, images_dir, masks_dir, annotations_csv, transform=None):\n",
    "        self.images_dir = Path(images_dir)\n",
    "        self.masks_dir = Path(masks_dir)\n",
    "        self.annotations = pd.read_csv(annotations_csv)\n",
    "        self.transform = transform\n",
    "        self.file_names = self.annotations[\"file_name\"].unique()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        file_name = self.file_names[idx]\n",
    "        image_path = self.images_dir / f'{Path(file_name).stem}.npy'\n",
    "        mask_path = self.masks_dir / f'{Path(file_name).stem}.npz'\n",
    "\n",
    "        # Load the preprocessed image and mask\n",
    "        image = np.load(image_path)\n",
    "        mask = np.load(mask_path)['arr_0']\n",
    "\n",
    "        # Convert image and mask to tv_tensors\n",
    "        image = tv_tensors.Image(torch.tensor(image).permute(2, 0, 1))  # [C, H, W]\n",
    "        instance_masks = tv_tensors.Mask(torch.tensor(mask).permute(2, 0, 1))  # [N, H, W] where N is instances\n",
    "        \n",
    "        # Combine instance masks into binary mask\n",
    "        binary_mask = torch.max(instance_masks, dim=0)[0].unsqueeze(0)  # [1, H, W]\n",
    "        binary_mask = tv_tensors.Mask(binary_mask)\n",
    "\n",
    "        # Get the annotations for the current file\n",
    "        annotations = self.annotations[self.annotations[\"file_name\"] == file_name]\n",
    "\n",
    "        labels = [1] * len(annotations)  # All labels are \"Chestnut-burr\"\n",
    "        bboxes = annotations[[\"bbox_x\", \"bbox_y\", \"bbox_w\", \"bbox_h\"]].values\n",
    "        bboxes = [torch.tensor([float(bbox[0]), float(bbox[1]), \n",
    "                              float(bbox[0] + bbox[2]), float(bbox[1] + bbox[3])], \n",
    "                              dtype=torch.float32) for bbox in bboxes]\n",
    "        bboxes = torch.stack(bboxes, dim=0)\n",
    "        iscrowd = annotations[\"iscrowd\"].values\n",
    "        area = (bboxes[:, 2] - bboxes[:, 0]) * (bboxes[:, 3] - bboxes[:, 1])\n",
    "\n",
    "        # Create the target dictionary\n",
    "        target = {\n",
    "            \"boxes\": tv_tensors.BoundingBoxes(bboxes, \n",
    "                                            format=tv_tensors.BoundingBoxFormat.XYXY, \n",
    "                                            canvas_size=(image.shape[1], image.shape[2])),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.int64),\n",
    "            \"image_id\": idx,\n",
    "            \"area\": area.clone().detach(),\n",
    "            \"iscrowd\": torch.tensor(iscrowd, dtype=torch.uint8),\n",
    "            \"masks\": binary_mask,  # Use binary mask for SmallObjectUNet\n",
    "        }\n",
    "\n",
    "        if self.transform:\n",
    "            image, target = self.transform(image, target)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <center> Transformation Pipeline </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    if train:\n",
    "        transforms.extend([\n",
    "            T.RandomHorizontalFlip(0.5),\n",
    "            T.RandomVerticalFlip(0.5),\n",
    "            T.RandomRotation(degrees=(-45, 45)),\n",
    "            \n",
    "            # Add scale augmentation for small objects\n",
    "            T.RandomAffine(\n",
    "                degrees=0, \n",
    "                scale=(0.8, 1.2),\n",
    "                translate=(0.1, 0.1)\n",
    "            ),\n",
    "            # Add noise resilience\n",
    "            T.RandomApply([\n",
    "                T.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0))\n",
    "            ], p=0.3),\n",
    "        ])\n",
    "    transforms.append(T.ClampBoundingBoxes())\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <center> Plot Sample Batch </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_dir = 'S:/Zack/Deep Learning/Chestnut_Bur_Instance_Segmentation/U-Net/data/images'\n",
    "masks_dir = 'S:/Zack/Deep Learning/Chestnut_Bur_Instance_Segmentation/U-Net/data/masks'\n",
    "annotations_csv = 'S:/Zack/Deep Learning/Chestnut_Bur_Instance_Segmentation/U-Net/data/annotations.csv'\n",
    "\n",
    "sample_ds = ChestnutBurSegmentation(images_dir, \n",
    "                                    masks_dir, \n",
    "                                    annotations_csv,\n",
    "                                    transform = get_transform(train=True))\n",
    "\n",
    "sample_dl = DataLoader(sample_ds, \n",
    "                       batch_size = 4, \n",
    "                       shuffle = True, \n",
    "                       collate_fn = ChestnutBurSegmentation.collate_fn)\n",
    "\n",
    "images, targets = next(iter(sample_dl))\n",
    "images = [img for img in images]\n",
    "targets = [{k: v for k, v in target.items()} for target in targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image_with_annotations(image, target):\n",
    "    \"\"\"\n",
    "    Plot an image with bounding boxes (left) and with an overlayed mask (right).\n",
    "\n",
    "    Parameters:\n",
    "      image: tv_tensors.Image wrapping a torch.Tensor of shape [C, H, W]\n",
    "      target: dictionary with keys \"boxes\" (BoundingBoxes) and \"masks\" (Mask)\n",
    "    \"\"\"\n",
    "    # Convert image tensor to numpy image in HxWxC format\n",
    "    # (Assumes image tensor is already in the proper format, note: normalization is not reversed)\n",
    "    img_np = image.permute(1, 2, 0).cpu().numpy()\n",
    "    img_np = np.clip(img_np, 0, 1)\n",
    "\n",
    "    # Extract bounding boxes from target.\n",
    "    # Depending on your tv_tensors version, access the underlying tensor via .data or .tensor.\n",
    "    if hasattr(target[\"boxes\"], \"data\"):\n",
    "        boxes = target[\"boxes\"].data\n",
    "    else:\n",
    "        boxes = target[\"boxes\"].tensor\n",
    "    boxes = boxes.cpu().numpy()\n",
    "\n",
    "    # Extract mask from target.\n",
    "    if hasattr(target[\"masks\"], \"data\"):\n",
    "        mask_tensor = target[\"masks\"].data\n",
    "    else:\n",
    "        mask_tensor = target[\"masks\"].tensor\n",
    "    mask_np = mask_tensor.cpu().numpy()  # shape: (C, H, W)\n",
    "    # Combine channels if necessary (assumes binary masks per instance)\n",
    "    if mask_np.ndim == 3:\n",
    "        combined_mask = np.sum(mask_np, axis=0)\n",
    "    else:\n",
    "        combined_mask = mask_np\n",
    "\n",
    "    # Create figure with two subplots\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    # Left: Plot image with bounding boxes\n",
    "    ax = axes[0]\n",
    "    ax.imshow(img_np)\n",
    "    for box in boxes:\n",
    "        x1, y1, x2, y2 = box\n",
    "        rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1,\n",
    "                                 linewidth=2, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "    ax.set_title(\"Image with Bounding Boxes\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    # Right: Plot image with mask overlay\n",
    "    ax = axes[1]\n",
    "    ax.imshow(img_np)\n",
    "    ax.imshow(combined_mask, cmap='jet', alpha=0.5)\n",
    "    ax.set_title(\"Image with Mask Overlay\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "for i in range(len(images)):\n",
    "    plot_image_with_annotations(images[i], targets[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <center> Model Definition </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_norm(num_channels):\n",
    "    return nn.BatchNorm2d(num_channels)\n",
    "\n",
    "def calculate_pos_weight(dataset):\n",
    "    \"\"\"Calculate positive weight based on class distribution in dataset\"\"\"\n",
    "    total_pixels = 0\n",
    "    positive_pixels = 0\n",
    "    \n",
    "    for i in range(len(dataset)):\n",
    "        _, target = dataset[i]\n",
    "        mask = target['masks']\n",
    "        total_pixels += mask.numel()\n",
    "        positive_pixels += mask.sum()\n",
    "    \n",
    "    neg_pos_ratio = (total_pixels - positive_pixels) / positive_pixels\n",
    "    return torch.tensor([neg_pos_ratio])\n",
    "\n",
    "def calculate_confidence(binary_mask, target_mask):\n",
    "    \"\"\"Calculate confidence using IoU for semantic segmentation\"\"\"\n",
    "    intersection = (binary_mask * target_mask).sum() # overlapping pixels\n",
    "    union = binary_mask.sum() + target_mask.sum() - intersection # combined pixels\n",
    "    confidence = intersection / (union + 1e-6) # IoU\n",
    "    return confidence\n",
    "\n",
    "def find_optimal_threshold(pred_probs, target_masks, thresholds=None):\n",
    "    \"\"\"\n",
    "    Find optimal threshold based on IoU scores\n",
    "    pred_probs: [B, 1, H, W] sigmoid probabilities\n",
    "    target_masks: [B, 1, H, W] binary ground truth\n",
    "    \"\"\"\n",
    "    if thresholds is None:\n",
    "        thresholds = torch.linspace(0.2, 0.9, 15, device=pred_probs.device)\n",
    "    \n",
    "    best_iou = 0\n",
    "    best_threshold = 0.5\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        binary_masks = (pred_probs > threshold).float()\n",
    "        # Calculate batch IoU\n",
    "        intersection = (binary_masks * target_masks).sum((2,3))\n",
    "        union = binary_masks.sum((2,3)) + target_masks.sum((2,3)) - intersection\n",
    "        iou = (intersection / (union + 1e-6)).mean()\n",
    "        \n",
    "        if iou > best_iou:\n",
    "            best_iou = iou\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    return best_threshold\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Squeeze-and-Excitation Block for channel attention\n",
    "    Particularly useful for small objects by:\n",
    "    - Weighting important channels more heavily\n",
    "    - Suppressing less informative channels\n",
    "    - Helping focus on subtle features in low contrast areas\n",
    "    \n",
    "    Args:\n",
    "        channels (int): Number of input channels\n",
    "        reduction (int): Channel reduction factor, default 16\n",
    "                        Lower values (8-16) preserve more channel information\n",
    "                        important for small object features\n",
    "    \"\"\"\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.squeeze = nn.AdaptiveAvgPool2d(1)\n",
    "        self.excitation = nn.Sequential(\n",
    "            nn.Linear(channels, channels // reduction),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channels // reduction, channels),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.squeeze(x).view(b, c)\n",
    "        y = self.excitation(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"\n",
    "    Double Convolution block with residual connection and channel attention\n",
    "    Optimized for small object detection by:\n",
    "    - Residual connections preserving fine-grained features\n",
    "    - Channel attention highlighting important feature channels\n",
    "    - Group normalization for stable training with small objects\n",
    "    \n",
    "    Args:\n",
    "        in_channels (int): Number of input channels\n",
    "        out_channels (int): Number of output channels\n",
    "        dropout_rate (float): Dropout rate for regularization\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, dropout_rate=0.0):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.downsample = None\n",
    "        if in_channels != out_channels:\n",
    "            self.downsample = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "            \n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            get_norm(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(p=dropout_rate),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            get_norm(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(p=dropout_rate)\n",
    "        )\n",
    "        \n",
    "        # Add SE block for channel attention\n",
    "        self.se = SEBlock(out_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        identity = x if self.downsample is None else self.downsample(x)\n",
    "        out = self.double_conv(x)\n",
    "        out = self.se(out)  # Apply channel attention\n",
    "        return out + identity  # Residual connection\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Spatial attention block for skip connections\n",
    "    Enhances small object detection by:\n",
    "    - Focusing on relevant spatial locations\n",
    "    - Suppressing background noise\n",
    "    - Preserving fine details from encoder features\n",
    "    \n",
    "    Args:\n",
    "        F_g (int): Number of channels from decoder\n",
    "        F_l (int): Number of channels from encoder skip connection\n",
    "        F_int (int): Number of intermediate channels for attention\n",
    "    \"\"\"\n",
    "    def __init__(self, F_g, F_l, F_int):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        self.W_g = nn.Sequential(\n",
    "            nn.Conv2d(F_g, F_int, kernel_size=1),\n",
    "            get_norm(F_int)\n",
    "        )\n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.Conv2d(F_l, F_int, kernel_size=1),\n",
    "            get_norm(F_int)\n",
    "        )\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(F_int, 1, kernel_size=1),\n",
    "            get_norm(1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, g, x):\n",
    "        g1 = self.W_g(g)\n",
    "        x1 = self.W_x(x)\n",
    "        psi = self.relu(g1 + x1)\n",
    "        psi = self.psi(psi)\n",
    "        return x * psi\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Upsampling block with transposed convolution and attention\n",
    "    Designed for small object preservation by:\n",
    "    - Using transposed convolution for learnable upsampling\n",
    "    - Incorporating spatial attention on skip connections\n",
    "    - Maintaining high-resolution feature maps\n",
    "    \n",
    "    Args:\n",
    "        in_channels (int): Number of input channels\n",
    "        out_channels (int): Number of output channels\n",
    "        skip_in (int, optional): Number of skip connection channels\n",
    "        dropout_rate (float): Dropout rate for regularization\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, skip_in=None, dropout_rate=0.0):\n",
    "        super(UpBlock, self).__init__()\n",
    "        if skip_in is None:\n",
    "            skip_in = out_channels\n",
    "        # Replace upsampling with transposed conv\n",
    "        self.up = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),\n",
    "            nn.Dropout2d(p=dropout_rate)\n",
    "        )\n",
    "        self.skip_reduce = nn.Sequential(\n",
    "            nn.Conv2d(skip_in, out_channels, kernel_size=1),\n",
    "            nn.Dropout2d(p=dropout_rate)\n",
    "        )\n",
    "        self.attention = AttentionBlock(F_g=out_channels, F_l=out_channels, F_int=out_channels // 2)\n",
    "        self.conv = DoubleConv(out_channels * 2, out_channels, dropout_rate)\n",
    "    \n",
    "    def forward(self, x, skip):\n",
    "        x = self.up(x)\n",
    "        skip = self.skip_reduce(skip)\n",
    "        # Use bicubic interpolation for better resolution preservation\n",
    "        if x.size() != skip.size():\n",
    "            x = F.interpolate(x, size=skip.size()[2:], mode='bicubic', align_corners=True)\n",
    "        skip = self.attention(x, skip)\n",
    "        x = torch.cat([skip, x], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "class SmallObjectUNet(nn.Module):\n",
    "    \"\"\"\n",
    "    UNet architecture optimized for small object detection in low contrast images\n",
    "    Key features:\n",
    "    - Swin Transformer backbone for hierarchical feature extraction\n",
    "    - Deep supervision for better gradient flow\n",
    "    - Channel and spatial attention mechanisms\n",
    "    - Group normalization for batch-independent normalization\n",
    "    - Dynamic thresholding for optimal segmentation\n",
    "    \n",
    "    Args:\n",
    "        in_channels (int): Number of input channels (default: 3)\n",
    "        out_channels (int): Number of output channels (default: 1)\n",
    "        base_filters (int): Number of base filters (default: 64)\n",
    "        dataset: Dataset for calculating class weights\n",
    "        grad_clip (float): Gradient clipping threshold (default: 1.0)\n",
    "        l2_lambda (float): L2 regularization strength (default: 0.01)\n",
    "        pos_weight (float): Class weight multiplier (default: 1.0)\n",
    "        dice_weight (float): Weight of Dice loss vs BCE (default: 0.5)\n",
    "        deep_supervision (bool): Enable deep supervision (default: True)\n",
    "        dropout_rate (float): Dropout rate (default: 0.0)\n",
    "        sup_weights (dict): Deep supervision layer weights\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=3, out_channels=1, base_filters=64, dataset=None,\n",
    "                 grad_clip=1.0, l2_lambda=0.01, pos_weight=1.0, dice_weight=0.5,\n",
    "                 deep_supervision=True, dropout_rate=0.0, sup_weights={'sup4_weight': 0.5, 'sup3_weight': 0.3, \n",
    "                                                                        'sup2_weight': 0.2, 'sup1_weight': 0.1}):\n",
    "        super().__init__()\n",
    "        self.encoder = timm.create_model('swin_tiny_patch4_window7_224', pretrained=True, features_only=True)\n",
    "        encoder_channels = self.encoder.feature_info.channels()\n",
    "        self.grad_clip = grad_clip\n",
    "        self.l2_lambda = l2_lambda\n",
    "        self.dice_weight = dice_weight \n",
    "        self.sup_weights = sup_weights\n",
    "        self.threshold = 0.5 # Default threshold for binary mask\n",
    "        \n",
    "        # Calculate positive class weight with multiplier\n",
    "        base_pos_weight = calculate_pos_weight(dataset) if dataset is not None else torch.tensor([2.0])\n",
    "        self.pos_weight = base_pos_weight * pos_weight\n",
    "        \n",
    "        self.center = DoubleConv(encoder_channels[-1], encoder_channels[-1], dropout_rate)\n",
    "        self.dec4 = UpBlock(encoder_channels[-1], encoder_channels[-2], encoder_channels[-1], dropout_rate)\n",
    "        self.dec3 = UpBlock(encoder_channels[-2], encoder_channels[-3], encoder_channels[-2], dropout_rate)\n",
    "        self.dec2 = UpBlock(encoder_channels[-3], encoder_channels[-4], encoder_channels[-3], dropout_rate)\n",
    "        self.dec1 = UpBlock(encoder_channels[-4], base_filters, encoder_channels[-4], dropout_rate)\n",
    "        \n",
    "        # Add final upsampling to match input resolution\n",
    "        self.out_conv = nn.Sequential(\n",
    "            nn.UpsamplingBilinear2d(scale_factor=2),  # First 2x upsampling\n",
    "            nn.Conv2d(base_filters, base_filters, kernel_size=3, padding=1),\n",
    "            get_norm(base_filters),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.UpsamplingBilinear2d(scale_factor=2),  # Second 2x upsampling\n",
    "            nn.Conv2d(base_filters, base_filters, kernel_size=3, padding=1),\n",
    "            get_norm(base_filters),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(base_filters, out_channels, kernel_size=1)\n",
    "        )\n",
    "        \n",
    "        self._init_weights()\n",
    "\n",
    "        # Add deep supervision outputs\n",
    "        self.deep_supervision = deep_supervision\n",
    "        if deep_supervision:\n",
    "            self.deep_sup4 = nn.Conv2d(encoder_channels[-2], out_channels, 1)\n",
    "            self.deep_sup3 = nn.Conv2d(encoder_channels[-3], out_channels, 1)\n",
    "            self.deep_sup2 = nn.Conv2d(encoder_channels[-4], out_channels, 1)\n",
    "            self.deep_sup1 = nn.Conv2d(base_filters, out_channels, 1)\n",
    "\n",
    "    def forward(self, images, targets=None):\n",
    "        if isinstance(images, list):\n",
    "            images = torch.stack(images, dim=0)\n",
    "\n",
    "        features = self.encoder(images)\n",
    "        e1, e2, e3, e4 = features[0], features[1], features[2], features[3]\n",
    "        \n",
    "        # Ensure correct feature shapes\n",
    "        e1 = self.ensure_channel_first(e1)\n",
    "        e2 = self.ensure_channel_first(e2)\n",
    "        e3 = self.ensure_channel_first(e3)\n",
    "        e4 = self.ensure_channel_first(e4)\n",
    "        \n",
    "        center = self.center(e4)\n",
    "        d4 = self.dec4(center, e4)\n",
    "        d3 = self.dec3(d4, e3)\n",
    "        d2 = self.dec2(d3, e2)\n",
    "        d1 = self.dec1(d2, e1)\n",
    "        main_pred = self.out_conv(d1)  # Returns logits [batch_size, 1, H, W]\n",
    "\n",
    "        if self.training:\n",
    "            if self.deep_supervision:\n",
    "                # Get deep supervision predictions (logits)\n",
    "                sup4 = F.interpolate(self.deep_sup4(d4), size=main_pred.shape[2:], mode='bicubic')\n",
    "                sup3 = F.interpolate(self.deep_sup3(d3), size=main_pred.shape[2:], mode='bicubic')\n",
    "                sup2 = F.interpolate(self.deep_sup2(d2), size=main_pred.shape[2:], mode='bicubic')\n",
    "                sup1 = F.interpolate(self.deep_sup1(d1), size=main_pred.shape[2:], mode='bicubic')\n",
    "                \n",
    "                # Calculate losses using logits\n",
    "                main_loss_dict = self._compute_loss(main_pred, targets)\n",
    "                sup4_loss_dict = self._compute_loss(sup4, targets)\n",
    "                sup3_loss_dict = self._compute_loss(sup3, targets)\n",
    "                sup2_loss_dict = self._compute_loss(sup2, targets)\n",
    "                sup1_loss_dict = self._compute_loss(sup1, targets)\n",
    "                \n",
    "                # Calculate total loss with supervision weights\n",
    "                total_loss = (main_loss_dict['loss'] + \n",
    "                        self.sup_weights['sup4_weight'] * sup4_loss_dict['loss'] +\n",
    "                        self.sup_weights['sup3_weight'] * sup3_loss_dict['loss'] +\n",
    "                        self.sup_weights['sup2_weight'] * sup2_loss_dict['loss'] +\n",
    "                        self.sup_weights['sup1_weight'] * sup1_loss_dict['loss'])\n",
    "                \n",
    "                return {\n",
    "                    'total_loss': total_loss,\n",
    "                    'bce_loss': main_loss_dict['bce_loss'],\n",
    "                    'dice_loss': main_loss_dict['dice_loss'],\n",
    "                    'sup4_loss': sup4_loss_dict['loss'],\n",
    "                    'sup3_loss': sup3_loss_dict['loss'],\n",
    "                    'sup2_loss': sup2_loss_dict['loss'],\n",
    "                    'sup1_loss': sup1_loss_dict['loss']\n",
    "                }\n",
    "            else:\n",
    "                return self._compute_loss(main_pred, targets)\n",
    "        else:\n",
    "            # Convert logits to probabilities for inference\n",
    "            pred_probs = torch.sigmoid(main_pred)\n",
    "            \n",
    "            # Update threshold during validation\n",
    "            if targets is not None:\n",
    "                self.threshold = find_optimal_threshold(pred_probs, \n",
    "                                                     torch.stack([t['masks'] for t in targets]))\n",
    "            \n",
    "            results = []\n",
    "            for i in range(len(pred_probs)):\n",
    "                probs = pred_probs[i]\n",
    "                \n",
    "                # Convert probabilities to binary mask\n",
    "                binary_mask = (probs > self.threshold).float()\n",
    "                \n",
    "                # Calculate confidence score\n",
    "                binary_mask = (probs > self.threshold).float()\n",
    "                target_mask = targets[i]['masks'] if targets is not None else torch.zeros_like(binary_mask)\n",
    "                confidence = calculate_confidence(binary_mask, target_mask)\n",
    "                \n",
    "                result = {\n",
    "                    'masks': binary_mask.unsqueeze(0),  # [1, H, W]\n",
    "                    'scores': confidence.unsqueeze(0),   # [1]\n",
    "                    'labels': torch.tensor([1], device=main_pred.device)\n",
    "                }\n",
    "                results.append(result)\n",
    "            return results\n",
    "\n",
    "    def dice_coefficient(self, pred, target, smooth=1.0):\n",
    "            \"\"\"\n",
    "            Calculate Dice coefficient\n",
    "            pred: [batch_size, 1, H, W] - probabilities after sigmoid\n",
    "            target: [batch_size, 1, H, W] - binary ground truth masks\n",
    "            \"\"\"\n",
    "            pred = pred.contiguous().view(-1)\n",
    "            target = target.contiguous().view(-1)\n",
    "            \n",
    "            intersection = (pred * target).sum()\n",
    "            pred_sum = pred.sum()\n",
    "            target_sum = target.sum()\n",
    "            \n",
    "            dice = (2. * intersection + smooth) / (pred_sum + target_sum + smooth)\n",
    "            return dice\n",
    "\n",
    "    def _compute_loss(self, predictions, targets):\n",
    "        \"\"\"\n",
    "        Compute combined BCE and Dice loss with L2 regularization\n",
    "        predictions: [batch_size, 1, height, width] - raw logits\n",
    "        targets: list of dicts containing 'masks' tensor\n",
    "        \"\"\"\n",
    "        # Process target masks\n",
    "        combined_masks = []\n",
    "        for target in targets:\n",
    "            mask = target['masks'].to(predictions.device)  # Move mask to same device as predictions\n",
    "            if mask.dim() == 3 and mask.shape[0] > 1:\n",
    "                mask = torch.max(mask, dim=0, keepdim=True)[0]\n",
    "            elif mask.dim() == 2:\n",
    "                mask = mask.unsqueeze(0)\n",
    "            elif mask.shape[0] == 0:\n",
    "                mask = torch.zeros((1, mask.shape[1], mask.shape[2]), \n",
    "                                device=predictions.device)\n",
    "            combined_masks.append(mask)\n",
    "        \n",
    "        # Stack masks and ensure correct format\n",
    "        masks = torch.stack(combined_masks, dim=0)  # Already on correct device\n",
    "        masks = masks.float()\n",
    "        \n",
    "        if masks.shape[1] != 1:\n",
    "            masks = masks[:, 0:1, :, :]\n",
    "        \n",
    "        masks = F.interpolate(masks, size=predictions.shape[2:], \n",
    "                            mode='bicubic', align_corners=True)\n",
    "        \n",
    "        # Ensure pos_weight is on correct device\n",
    "        if hasattr(self, 'pos_weight'):\n",
    "            self.pos_weight = self.pos_weight.to(predictions.device)\n",
    "        \n",
    "        # BCE loss with logits\n",
    "        bce_loss = F.binary_cross_entropy_with_logits(\n",
    "            predictions, \n",
    "            masks,\n",
    "            pos_weight=self.pos_weight\n",
    "        )\n",
    "        \n",
    "        # Dice loss\n",
    "        pred_probs = torch.sigmoid(predictions)\n",
    "        dice_loss = 1 - self.dice_coefficient(pred_probs, masks)\n",
    "        \n",
    "        # Combine losses\n",
    "        combined_loss = (1 - self.dice_weight) * bce_loss + self.dice_weight * dice_loss\n",
    "        \n",
    "        # L2 regularization\n",
    "        l2_reg = torch.tensor(0., device=predictions.device)\n",
    "        for param in self.parameters():\n",
    "            l2_reg += torch.norm(param)\n",
    "        \n",
    "        total_loss = combined_loss + self.l2_lambda * l2_reg\n",
    "        \n",
    "        if self.training:\n",
    "            torch.nn.utils.clip_grad_norm_(self.parameters(), self.grad_clip)\n",
    "        \n",
    "        return {\n",
    "            'loss': total_loss,\n",
    "            'bce_loss': bce_loss,\n",
    "            'dice_loss': dice_loss\n",
    "        }\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.GroupNorm):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def ensure_channel_first(self, x):\n",
    "        if x.dim() == 4 and x.size(1) not in [1, 3, 96, 192, 384, 768]:\n",
    "            return x.permute(0, 3, 1, 2).contiguous()\n",
    "        return x\n",
    "\n",
    "print(SmallObjectUNet())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <center> Create Train, Val, Test Datasets </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store indices in random order list for subsetting\n",
    "indices = torch.randperm(len(sample_ds)).tolist()\n",
    "\n",
    "# Calculate split indices\n",
    "train_split = int(0.8 * len(indices))\n",
    "val_split = int(0.95 * len(indices))\n",
    "\n",
    "train_indices = indices[:train_split]\n",
    "val_indices = indices[train_split:val_split]\n",
    "test_indices = indices[val_split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <center> Tune Model Hyperparameters </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import tune, train\n",
    "from ray.tune.schedulers import HyperBandForBOHB\n",
    "from ray.tune.search.bohb import TuneBOHB\n",
    "from ray.tune.search import ConcurrencyLimiter\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import gc\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "import ray.cloudpickle as pickle\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import random\n",
    "from torch_lr_finder import LRFinder, TrainDataLoaderIter\n",
    "from segmentation_pytorch import engine\n",
    "from segmentation_pytorch.coco_utils import get_coco_api_from_dataset\n",
    "\n",
    "\n",
    "# Set random seed for reproducible training\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def visualize_predictions(model, data_loader, device, epoch, num_samples=2):\n",
    "    \"\"\"\n",
    "    Visualize semantic segmentation predictions.\n",
    "    Shows input image, ground truth mask, and predicted mask.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (images, targets) in enumerate(data_loader):\n",
    "            if i >= num_samples:\n",
    "                break\n",
    "                \n",
    "            images = list(img.to(device) for img in images)\n",
    "            outputs = model(images)\n",
    "            \n",
    "            for b in range(len(images)):\n",
    "                # Get original image and normalize\n",
    "                img = images[b].cpu()\n",
    "                img = img.permute(1, 2, 0).numpy()  # Convert to numpy after permute\n",
    "                img = np.clip(img, 0, 1)  # Normalize to [0,1]\n",
    "                \n",
    "                # Get ground truth mask\n",
    "                gt_mask = targets[b]['masks'].float()\n",
    "                \n",
    "                # Get predicted mask and confidence score\n",
    "                pred_mask = outputs[b]['masks']\n",
    "                pred_score = outputs[b]['scores'][0].item()\n",
    "                \n",
    "                # Create figure\n",
    "                fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "                \n",
    "                # Plot normalized image\n",
    "                ax1.imshow(img)  # Now img is already in correct format\n",
    "                ax1.set_title(f'Input Image\\nEpoch {epoch}, Batch {i}')\n",
    "                ax1.axis('off')\n",
    "                \n",
    "                # Plot ground truth mask\n",
    "                ax2.imshow(gt_mask.squeeze(), cmap='gray', vmin=0, vmax=1)\n",
    "                ax2.set_title(f'Ground Truth\\nPositive pixels: {gt_mask.sum():.0f}')\n",
    "                ax2.axis('off')\n",
    "                \n",
    "                # Plot prediction\n",
    "                ax3.imshow(pred_mask.squeeze().cpu(), cmap='gray', vmin=0, vmax=1)\n",
    "                ax3.set_title(f'Prediction\\nConfidence Score: {pred_score:.3f}')\n",
    "                ax3.axis('off')\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                plt.close()\n",
    "\n",
    "\n",
    "def calculate_f1_score(precision, recall):\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "\n",
    "def is_plateaued(values, tolerance=0.0005):\n",
    "    \"\"\"\n",
    "    Check if values have plateaued by comparing max and min\n",
    "    in the window to see if they're within tolerance\n",
    "    \"\"\"\n",
    "    if len(values) < 5:  # Need at least 5 values to determine plateau\n",
    "        return False\n",
    "    recent_values = values[-5:]  # Look at last 5 epochs\n",
    "    max_val, min_val = max(recent_values), min(recent_values)\n",
    "    return (max_val - min_val) < tolerance\n",
    "\n",
    "\n",
    "def train_lr_finder(batch_size):\n",
    "\n",
    "    class CustomTrainDataLoaderIter(TrainDataLoaderIter):\n",
    "        def inputs_labels_from_batch(self, batch_data):\n",
    "            inputs = [image.to('cuda:0') if isinstance(image, torch.Tensor) else torch.tensor(image).to('cuda:0') for image in batch_data[0]]\n",
    "            labels = [{k: v.to('cuda:0') if isinstance(v, torch.Tensor) else torch.tensor(v).to('cuda:0') for k, v in t.items()} for t in batch_data[1]]\n",
    "            return inputs, labels\n",
    "\n",
    "    dataset_train = ChestnutBurSegmentation(images_dir, \n",
    "                                      masks_dir, \n",
    "                                      annotations_csv,\n",
    "                                      transform = get_transform(train=True))\n",
    "    dataset_train = Subset(dataset_train, train_indices)\n",
    "\n",
    "    accumulation_steps = 1  ## FIXME: hardcoded for now\n",
    "\n",
    "    data_loader_train = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size,\n",
    "                                                    shuffle=True,\n",
    "                                                    collate_fn=ChestnutBurSegmentation.collate_fn,\n",
    "                                                    num_workers=0, pin_memory=False)\n",
    "\n",
    "    model = SmallObjectUNet(in_channels=3, \n",
    "                                out_channels=1, \n",
    "                                base_filters=64, \n",
    "                                dataset=dataset_train,\n",
    "                                grad_clip=1.0,\n",
    "                                l2_lambda=0.001,\n",
    "                                pos_weight=1,\n",
    "                                dice_weight=0.5,\n",
    "                                deep_supervision=True,\n",
    "                                dropout_rate=0.5)\n",
    "\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.Adam(params, lr=1e-7)\n",
    "\n",
    "    train_iter = CustomTrainDataLoaderIter(data_loader_train)\n",
    "    grad_scaler = torch.GradScaler()\n",
    "\n",
    "    class CustomLRFinder(LRFinder):\n",
    "        def __init__(self, model, optimizer, criterion, device=None, amp_backend=\"native\", amp_config=None, grad_scaler=None):\n",
    "            super().__init__(model, optimizer, criterion, device)\n",
    "            self.amp_backend = amp_backend\n",
    "            self.amp_config = amp_config\n",
    "            self.grad_scaler = grad_scaler or torch.GradScaler()\n",
    "\n",
    "        def _train_batch(self, train_iter, accumulation_steps, non_blocking_transfer=True):\n",
    "            self.model.train()\n",
    "            total_loss = 0\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            for _ in range(accumulation_steps):\n",
    "                inputs, labels = next(train_iter)\n",
    "                inputs, labels = self._move_to_device(inputs, labels, non_blocking=non_blocking_transfer)\n",
    "\n",
    "                with torch.autocast(device_type=\"cuda:0\"):\n",
    "                    outputs = self.model(inputs, labels)\n",
    "                    loss = sum(loss for loss in outputs.values())\n",
    "\n",
    "                loss /= accumulation_steps\n",
    "                self.grad_scaler.scale(loss).backward()\n",
    "                total_loss += loss\n",
    "\n",
    "            self.grad_scaler.step(self.optimizer)\n",
    "            self.grad_scaler.update()\n",
    "\n",
    "            return total_loss.item()\n",
    "\n",
    "    lr_finder = CustomLRFinder(model, optimizer, None, device='cuda:0', amp_backend='torch', amp_config=None, grad_scaler=grad_scaler)\n",
    "    lr_finder.range_test(train_iter, end_lr=0.1, num_iter=500, step_mode='exp', accumulation_steps=accumulation_steps)\n",
    "    suggested_lr = lr_finder.plot(suggest_lr=True)\n",
    "\n",
    "    lr_finder.reset()\n",
    "\n",
    "    # Ensure consistent return value\n",
    "    try:\n",
    "        if isinstance(suggested_lr, tuple):\n",
    "            axes, suggested_lr_value = suggested_lr\n",
    "            return suggested_lr_value\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected return type from plot method: {type(suggested_lr)}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Error during learning rate finding: {e}\")\n",
    "        # Return a default learning rate if an error occurs\n",
    "        return 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallObjectUNetTuner:\n",
    "    def __init__(self, num_samples, restore_path=\"\"):\n",
    "        self.num_samples = num_samples\n",
    "        self.restore_path = restore_path\n",
    "\n",
    "    def create_coco_datasets(self, train_dataset, val_dataset, test_dataset):\n",
    "        with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "            train_future = executor.submit(get_coco_api_from_dataset, train_dataset)\n",
    "            val_future = executor.submit(get_coco_api_from_dataset, val_dataset)\n",
    "            test_future = executor.submit(get_coco_api_from_dataset, test_dataset)\n",
    "            train_coco_ds = train_future.result()\n",
    "            val_coco_ds = val_future.result()\n",
    "            test_coco_ds = test_future.result()\n",
    "        return train_coco_ds, val_coco_ds, test_coco_ds\n",
    "\n",
    "    def train_SmallObjectUNet(self, config):\n",
    "\n",
    "        set_seed(666)\n",
    "\n",
    "        dataset = ray.get(config[\"dataset_train_ref\"])\n",
    "        data_loader_val = ray.get(config[\"data_loader_val_ref\"])\n",
    "        train_coco_ds = ray.get(config[\"train_coco_ds_ref\"])\n",
    "        val_coco_ds = ray.get(config[\"val_coco_ds_ref\"])\n",
    "\n",
    "        training_steps = [\n",
    "            {\"step\": 0, \"batch_size\": 32, \"print_freq\": 5, \"accumulation_steps\": 1},\n",
    "            {\"step\": 1, \"batch_size\": 32, \"print_freq\": 5, \"accumulation_steps\": 2},\n",
    "            {\"step\": 2, \"batch_size\": 32, \"print_freq\": 5, \"accumulation_steps\": 4},\n",
    "            {\"step\": 3, \"batch_size\": 32, \"print_freq\": 5, \"accumulation_steps\": 8},\n",
    "            {\"step\": 4, \"batch_size\": 32, \"print_freq\": 5, \"accumulation_steps\": 16} # bs 512\n",
    "        ]\n",
    "\n",
    "        device = \"cpu\"\n",
    "        if torch.cuda.is_available():\n",
    "            device = \"cuda:0\"\n",
    "\n",
    "        checkpoint = train.get_checkpoint()\n",
    "        if checkpoint:\n",
    "            with checkpoint.as_directory() as checkpoint_dir:\n",
    "                data_path = Path(checkpoint_dir) / \"data.pkl\"\n",
    "                with open(data_path, \"rb\") as fp:\n",
    "                    checkpoint_state = pickle.load(fp)\n",
    "                start_epoch = checkpoint_state[\"epoch\"] + 1\n",
    "                step_index = checkpoint_state[\"step_index\"]\n",
    "        else:\n",
    "            start_epoch = 0\n",
    "            step_index = 0\n",
    "\n",
    "        # Loop through training_steps during training to increase batch size and decrease learning rate\n",
    "        while step_index < len(training_steps):\n",
    "            ts = training_steps[step_index]\n",
    "            batch_size, print_freq, accumulation_steps = ts[\"batch_size\"], ts[\"print_freq\"], ts[\"accumulation_steps\"]\n",
    "            scaled_lr = config[\"lr\"] * (batch_size / training_steps[0][\"batch_size\"]) * accumulation_steps\n",
    "\n",
    "            # Reinitialize the model with the current hyperparameters\n",
    "            model = SmallObjectUNet(in_channels=3,\n",
    "                                    out_channels=1,\n",
    "                                    base_filters=config[\"base_filters\"],\n",
    "                                    dataset=dataset,\n",
    "                                    grad_clip=config[\"grad_clip\"],\n",
    "                                    l2_lambda=config[\"l2_lambda\"],\n",
    "                                    pos_weight=config[\"pos_weight\"],\n",
    "                                    dice_weight=config[\"dice_weight\"],\n",
    "                                    deep_supervision=True,\n",
    "                                    dropout_rate=config[\"dropout_rate\"],\n",
    "                                    sup_weights=config[\"sup_weights\"]\n",
    "            )\n",
    "\n",
    "            model.to(device)\n",
    "\n",
    "            # Construct an optimizer with the suggested learning rate\n",
    "            params = [p for p in model.parameters() if p.requires_grad]\n",
    "            optimizer = torch.optim.Adam(params, \n",
    "                                         lr=scaled_lr,\n",
    "                                         betas=(config[\"beta1_loss\"], config[\"beta2_loss\"]),\n",
    "                                         eps=config[\"epsilon_loss\"],)\n",
    "\n",
    "            # Define training and validation data loaders\n",
    "            data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, \n",
    "                                                    shuffle=False, \n",
    "                                                    collate_fn=ChestnutBurSegmentation.collate_fn, \n",
    "                                                    num_workers=0,\n",
    "                                                    pin_memory=True)\n",
    "            \n",
    "            print(f'Training step: {ts[\"step\"]}, effective batch size: {batch_size * accumulation_steps}, scaled lr: {scaled_lr:.6f}\\n')\n",
    "            print()\n",
    "\n",
    "            window_loss, window_f1 = [], []\n",
    "            window_size = 5\n",
    "            plateau_tolerance = 0.0001\n",
    "            minimum_step_epochs = 10\n",
    "            step_epoch_counter = 0\n",
    "\n",
    "            while True:\n",
    "                step_epoch_counter += 1\n",
    "\n",
    "                train_metric_logger, val_metric_logger = engine.train_one_epoch(\n",
    "                    model, optimizer, data_loader, device,\n",
    "                    start_epoch, print_freq, accumulation_steps,\n",
    "                    data_loader_val\n",
    "                )\n",
    "\n",
    "                train_coco_evaluator, val_coco_evaluator = engine.evaluate(\n",
    "                    model, data_loader_val, val_coco_ds, device, data_loader, train_coco_ds\n",
    "                )\n",
    "\n",
    "                current_val_f1 = calculate_f1_score(val_coco_evaluator.coco_eval['segm'].stats[0],\n",
    "                                                      val_coco_evaluator.coco_eval['segm'].stats[6])\n",
    "                current_val_loss = val_metric_logger.total_loss.avg\n",
    "\n",
    "                window_loss.append(current_val_loss)\n",
    "                window_f1.append(current_val_f1)\n",
    "                if len(window_loss) > window_size:\n",
    "                    window_loss.pop(0)\n",
    "                if len(window_f1) > window_size:\n",
    "                    window_f1.pop(0)\n",
    "\n",
    "                if step_epoch_counter >= minimum_step_epochs and len(window_loss) == window_size:\n",
    "                    loss_plateaued = is_plateaued(window_loss, tolerance=plateau_tolerance)\n",
    "                    f1_plateaued = is_plateaued(window_f1, tolerance=plateau_tolerance)\n",
    "                    \n",
    "                    if loss_plateaued or f1_plateaued:\n",
    "                        print(f\"{'Loss' if loss_plateaued else 'F1'} plateaued after {step_epoch_counter} epochs\")\n",
    "                        print(f\"Recent losses: {window_loss}\")\n",
    "                        print(f\"Recent F1 scores: {window_f1}\")\n",
    "                        start_epoch += 1\n",
    "                        print(\"Moving to next training step.\")\n",
    "                        break\n",
    "\n",
    "\n",
    "                checkpoint_data = {\n",
    "                    \"epoch\": start_epoch,\n",
    "                    \"step_index\": step_index,\n",
    "                    \"training_iteration\": step_epoch_counter,\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                }\n",
    "\n",
    "                with tempfile.TemporaryDirectory() as checkpoint_dir:\n",
    "                    data_path = Path(checkpoint_dir) / \"data.pkl\"\n",
    "                    with open(data_path, \"wb\") as fp:\n",
    "                        pickle.dump(checkpoint_data, fp)\n",
    "                    train.report(\n",
    "                        {\n",
    "                            \"epoch\": start_epoch,\n",
    "                            \"step_index\": step_index,\n",
    "                            \"training_iteration\": step_epoch_counter,\n",
    "                            \"train_loss\": train_metric_logger.total_loss.avg,\n",
    "                            \"val_loss\": val_metric_logger.total_loss.avg,\n",
    "                            \"train_mAP\": train_coco_evaluator.coco_eval['segm'].stats[0], # IoU 0.5:0.95\n",
    "                            \"val_mAP\": val_coco_evaluator.coco_eval['segm'].stats[0],\n",
    "                            \"train_mAR\": train_coco_evaluator.coco_eval['segm'].stats[6],\n",
    "                            \"val_mAR\": val_coco_evaluator.coco_eval['segm'].stats[6],\n",
    "                            \"train_f1\": calculate_f1_score(train_coco_evaluator.coco_eval['segm'].stats[0],\n",
    "                                                            train_coco_evaluator.coco_eval['segm'].stats[6]),\n",
    "                            \"val_f1\": current_val_f1\n",
    "                        },\n",
    "                        checkpoint=train.Checkpoint.from_directory(checkpoint_dir),\n",
    "                    )\n",
    "\n",
    "                start_epoch += 1\n",
    "\n",
    "            step_index += 1\n",
    "\n",
    "        print('Tuning Trial Complete!')\n",
    "\n",
    "    def trial_dirname_creator(self, trial):\n",
    "        return f\"{trial.trial_id}\"\n",
    "\n",
    "    def run(self):\n",
    "        ray.shutdown()\n",
    "        ray.init()\n",
    "\n",
    "        dataset = ChestnutBurSegmentation(images_dir, \n",
    "                                      masks_dir, \n",
    "                                      annotations_csv,\n",
    "                                      transform = get_transform(train=True)\n",
    "        )\n",
    "    \n",
    "        dataset_val = ChestnutBurSegmentation(images_dir,\n",
    "                                            masks_dir,\n",
    "                                            annotations_csv,\n",
    "                                            transform = get_transform(train=False)\n",
    "        )\n",
    "\n",
    "        dataset_test = ChestnutBurSegmentation(images_dir,\n",
    "                                            masks_dir,\n",
    "                                            annotations_csv,\n",
    "                                            transform = get_transform(train=False)\n",
    "        )\n",
    "\n",
    "        dataset_train = torch.utils.data.Subset(dataset, train_indices)\n",
    "        dataset_val = torch.utils.data.Subset(dataset_val, val_indices)\n",
    "        dataset_test = torch.utils.data.Subset(dataset_test, test_indices)\n",
    "\n",
    "        data_loader_val = torch.utils.data.DataLoader(\n",
    "            dataset_val, batch_size=1, shuffle=False,\n",
    "            collate_fn=ChestnutBurSegmentation.collate_fn, num_workers=0, pin_memory=True\n",
    "        )\n",
    "\n",
    "        data_loader_test = torch.utils.data.DataLoader(\n",
    "            dataset_test, batch_size=1, shuffle=False,\n",
    "            collate_fn=ChestnutBurSegmentation.collate_fn, num_workers=0, pin_memory=True\n",
    "        )\n",
    "\n",
    "        train_coco_ds, val_coco_ds, test_coco_ds = self.create_coco_datasets(dataset_train, dataset_val, dataset_test)\n",
    "\n",
    "        dataset_train_ref = ray.put(dataset_train)\n",
    "        data_loader_val_ref = ray.put(data_loader_val)\n",
    "        data_loader_test_ref = ray.put(data_loader_test)\n",
    "        train_coco_ds_ref = ray.put(train_coco_ds)\n",
    "        val_coco_ds_ref = ray.put(val_coco_ds)\n",
    "        test_coco_ds_ref = ray.put(test_coco_ds)\n",
    "\n",
    "        config = {\n",
    "            \"lr\": tune.loguniform(1e-5, 1e-2),\n",
    "            \"base_filters\": tune.choice([32, 64, 128]),\n",
    "            \"l2_lambda\": tune.loguniform(1e-5, 1e-2),\n",
    "            \"grad_clip\": tune.uniform(0.5, 1.5),\n",
    "            \"dropout_rate\": tune.uniform(0.2, 0.5),\n",
    "            \"pos_weight_multiplier\": tune.uniform(1.0, 3.0),\n",
    "            \"dice_weight\": tune.uniform(0.3, 0.7),\n",
    "            \"sup_weights\": {\n",
    "                \"sup4_weight\": tune.uniform(0.4, 0.5),\n",
    "                \"sup3_weight\": tune.uniform(0.25, 0.35), \n",
    "                \"sup2_weight\": tune.uniform(0.15, 0.25),\n",
    "                \"sup1_weight\": tune.uniform(0.05, 0.1)\n",
    "            },\n",
    "            \"beta1_loss\": tune.uniform(0.9, 0.95),\n",
    "            \"beta2_loss\": tune.uniform(0.99, 0.999),\n",
    "            \"epsilon_loss\": tune.loguniform(1e-8, 1e-6),\n",
    "            \"dataset_train_ref\": dataset_train_ref,\n",
    "            \"data_loader_val_ref\": data_loader_val_ref,\n",
    "            \"data_loader_test_ref\": data_loader_test_ref,\n",
    "            \"train_coco_ds_ref\": train_coco_ds_ref,\n",
    "            \"val_coco_ds_ref\": val_coco_ds_ref,\n",
    "            \"test_coco_ds_ref\": test_coco_ds_ref\n",
    "        }\n",
    "\n",
    "        if tune.Tuner.can_restore(os.path.abspath(self.restore_path)):\n",
    "            tuner = tune.Tuner.restore(\n",
    "                os.path.abspath(self.restore_path),\n",
    "                trainable=self.train_SmallObjectUNet,\n",
    "                param_space=config,\n",
    "                resume_unfinished=True,\n",
    "                resume_errored=False\n",
    "            )\n",
    "            print(f\"Tuner Restored from {self.restore_path}\")\n",
    "        else:\n",
    "            algo = TuneBOHB(\n",
    "                seed=666\n",
    "            )\n",
    "\n",
    "            algo = ConcurrencyLimiter(algo, max_concurrent=2)\n",
    "\n",
    "            scheduler = HyperBandForBOHB(\n",
    "                time_attr=\"training_iteration\",\n",
    "                reduction_factor=4,\n",
    "                stop_last_trials=False,\n",
    "            )\n",
    "\n",
    "            reporter = tune.JupyterNotebookReporter(overwrite=True,\n",
    "                metric_columns=[\"epoch\", \"step_index\", \"train_loss\", \"val_loss\", \"train_mAP\", \"val_mAP\", \"train_mAR\", \"val_mAR\", \"train_f1\", \"val_f1\"],\n",
    "                parameter_columns=[\"lr\", \"base_filters\", \"l2_lambda\", \"grad_clip\", \"dropout_rate\", \"pos_weight\", \"dice_weight\", \"sup_weights\", \"beta1_loss\", \"beta2_loss\", \"epsilon_loss\"],\n",
    "                print_intermediate_tables=True,\n",
    "                sort_by_metric=True\n",
    "            )\n",
    "\n",
    "            tuner = tune.Tuner(\n",
    "                tune.with_resources(\n",
    "                    self.train_SmallObjectUNet,\n",
    "                    resources={\"cpu\": 12.0, \"gpu\": 1.0}\n",
    "                ),\n",
    "                run_config=train.RunConfig(\n",
    "                    name=f\"BOHB_SmallObjectUNet_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
    "                    failure_config=train.FailureConfig(max_failures=1),\n",
    "                    progress_reporter=reporter,\n",
    "                ),\n",
    "                tune_config=tune.TuneConfig(\n",
    "                    mode=\"min\",\n",
    "                    metric=\"val_loss\",\n",
    "                    search_alg=algo,\n",
    "                    scheduler=scheduler,\n",
    "                    num_samples=int(self.num_samples),\n",
    "                    trial_dirname_creator=self.trial_dirname_creator\n",
    "                ),\n",
    "                param_space=config\n",
    "            )\n",
    "        results = tuner.fit()\n",
    "\n",
    "        best_trial = results.get_best_result(\"val_loss\", \"min\")\n",
    "\n",
    "        print(\"Best trial config: {}\".format(best_trial.config))\n",
    "        print()\n",
    "        print(\"Best trial final training loss: {}\".format(best_trial.metrics[\"train_loss\"]))\n",
    "        print(\"Best trial final validation loss: {}\".format(best_trial.metrics[\"val_loss\"]))\n",
    "        print(\"Best trial final training mAP: {}\".format(best_trial.metrics[\"train_mAP\"]))\n",
    "        print(\"Best trial final validation mAP: {}\".format(best_trial.metrics[\"val_mAP\"]))\n",
    "        print(\"Best trial final training mAR: {}\".format(best_trial.metrics[\"train_mAR\"]))\n",
    "        print(\"Best trial final validation mAR: {}\".format(best_trial.metrics[\"val_mAR\"]))\n",
    "        print(\"Best trial final training f1-score: {}\".format(best_trial.metrics[\"train_f1\"]))\n",
    "        print(\"Best trial final validation f1-score: {}\".format(best_trial.metrics[\"val_f1\"]))\n",
    "        \n",
    "        print()\n",
    "\n",
    "        best_checkpoint = best_trial.get_best_checkpoint(metric=\"val_f1\", mode=\"max\")\n",
    "\n",
    "        self.test_best_model(best_trial, best_checkpoint)\n",
    "\n",
    "        return train_coco_ds, val_coco_ds, test_coco_ds, results, best_trial\n",
    "\n",
    "    def test_best_model(self, best_trial, best_checkpoint):\n",
    "        best_model = SmallObjectUNet(in_channels=3,\n",
    "                                    out_channels=1,\n",
    "                                    base_filters=best_trial.config[\"base_filters\"],\n",
    "                                    dataset=ray.get(best_trial.config[\"dataset_train_ref\"]),\n",
    "                                    grad_clip=best_trial.config[\"grad_clip\"],\n",
    "                                    l2_lambda=best_trial.config[\"l2_lambda\"],\n",
    "                                    pos_weight=best_trial.config[\"pos_weight\"]\n",
    "        )\n",
    "\n",
    "        device = \"cpu\"\n",
    "        if torch.cuda.is_available():\n",
    "            device = \"cuda:0\"\n",
    "        best_model.to(device)\n",
    "\n",
    "        with best_checkpoint.as_directory() as checkpoint_dir:\n",
    "            data_path = Path(checkpoint_dir) / \"data.pkl\"\n",
    "            with open(data_path, \"rb\") as fp:\n",
    "                best_checkpoint_data = pickle.load(fp)\n",
    "            best_model.load_state_dict(best_checkpoint_data[\"model_state_dict\"])\n",
    "\n",
    "        data_loader_test = ray.get(best_trial.config[\"data_loader_test_ref\"])\n",
    "        test_coco_ds = ray.get(best_trial.config[\"test_coco_ds_ref\"])\n",
    "\n",
    "        test_results = engine.evaluate(best_model, data_loader_test, test_coco_ds, device, train_data_loader=None, train_coco_ds=None)\n",
    "\n",
    "        print(f'Best trial test set mAP: {test_results.coco_eval[\"segm\"].stats[0]}')\n",
    "        print(f'Best trial test set mAR: {test_results.coco_eval[\"segm\"].stats[6]}')\n",
    "        print(f'Best trial test set f1-score: {calculate_f1_score(test_results.coco_eval[\"segm\"].stats[0], test_results.coco_eval[\"segm\"].stats[6])}')\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     torch.cuda.empty_cache()\n",
    "#     gc.collect()\n",
    "\n",
    "#     trainer = SmallObjectUNetTuner(num_samples=100, restore_path=\"C:/Users/zack/ray_results/FALSE\")\n",
    "#     train_coco_ds, val_coco_ds, test_coco_ds, results, best_trial = trainer.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <center> Train Model with Best Hyperparameter Combination </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.profiler\n",
    "\n",
    "def main():\n",
    "    \n",
    "    # Set seed\n",
    "    set_seed(666)\n",
    "\n",
    "    training_steps = [\n",
    "        {\"step\": 0, \"batch_size\": 32, \"print_freq\": 5, \"accumulation_steps\": 1},\n",
    "        {\"step\": 1, \"batch_size\": 32, \"print_freq\": 5, \"accumulation_steps\": 2},\n",
    "        {\"step\": 2, \"batch_size\": 32, \"print_freq\": 5, \"accumulation_steps\": 4},\n",
    "        {\"step\": 3, \"batch_size\": 32, \"print_freq\": 5, \"accumulation_steps\": 8},\n",
    "        {\"step\": 4, \"batch_size\": 32, \"print_freq\": 5, \"accumulation_steps\": 16} # bs 512\n",
    "    ]\n",
    "\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "\n",
    "    # Initialize the tensorboard writer\n",
    "    current_datetime = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    writer = SummaryWriter(log_dir=f'C:/Users/zack/Documents/GitHub/Savanna-Institute/Drone-based_Chestnut_Detection/runs/UNet/{current_datetime}')\n",
    "\n",
    "    # Store one checkpoint dictionary for each epoch in a list of dictionaries. \n",
    "    checkpoints = []\n",
    "\n",
    "    # Initialize the profiler\n",
    "    profiler = torch.profiler.profile(\n",
    "        activities=[\n",
    "            torch.profiler.ProfilerActivity.CPU,\n",
    "            torch.profiler.ProfilerActivity.CUDA,\n",
    "        ],\n",
    "        schedule=torch.profiler.schedule(\n",
    "            wait=1,\n",
    "            warmup=1,\n",
    "            active=3,\n",
    "            repeat=2),\n",
    "        on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name = f'C:/Users/zack/Documents/GitHub/Savanna-Institute/Drone-based_Chestnut_Detection/runs/UNet_profiler/{current_datetime}'),\n",
    "        record_shapes=True,\n",
    "        profile_memory=True,\n",
    "        with_stack=True\n",
    "    )\n",
    "\n",
    "    dataset = ChestnutBurSegmentation(images_dir, \n",
    "                                      masks_dir, \n",
    "                                      annotations_csv,\n",
    "                                      transform = get_transform(train=True))\n",
    "    \n",
    "    dataset_val = ChestnutBurSegmentation(images_dir,\n",
    "                                          masks_dir,\n",
    "                                          annotations_csv,\n",
    "                                          transform = get_transform(train=False))\n",
    "\n",
    "    dataset = Subset(dataset, train_indices)\n",
    "    dataset_val = Subset(dataset_val, val_indices)                                  \n",
    "    \n",
    "    data_loader_val = torch.utils.data.DataLoader(\n",
    "        dataset_val, batch_size=1, shuffle=False,\n",
    "        collate_fn=ChestnutBurSegmentation.collate_fn, num_workers=0, pin_memory=True\n",
    "    )\n",
    "    \n",
    "    start_epoch, step_index = 0, 0\n",
    "\n",
    "    # Find the optimal learning rate\n",
    "    suggested_lr = train_lr_finder(batch_size=training_steps[0][\"batch_size\"])\n",
    "\n",
    "\n",
    "    train_coco_ds, val_coco_ds = get_coco_api_from_dataset(dataset), get_coco_api_from_dataset(dataset_val)\n",
    "\n",
    "    # Loop through training_steps during training to increase batch size and decrease learning rate\n",
    "    while step_index < len(training_steps):\n",
    "        ts = training_steps[step_index]\n",
    "        batch_size, print_freq, accumulation_steps = ts[\"batch_size\"], ts[\"print_freq\"], ts[\"accumulation_steps\"]\n",
    "        scaled_lr = suggested_lr * (batch_size / training_steps[0][\"batch_size\"]) * accumulation_steps\n",
    "\n",
    "        # Reinitialize the model with the current hyperparameters\n",
    "        model = SmallObjectUNet(in_channels=3, \n",
    "                                out_channels=1, \n",
    "                                base_filters=64, \n",
    "                                dataset=dataset,\n",
    "                                grad_clip=1.0,\n",
    "                                l2_lambda=0.001,\n",
    "                                pos_weight=1,\n",
    "                                dice_weight=0.5,\n",
    "                                deep_supervision=True,\n",
    "                                dropout_rate=0.5)\n",
    "\n",
    "        model.to(device)\n",
    "\n",
    "        # Construct an optimizer with the suggested learning rate\n",
    "        params = [p for p in model.parameters() if p.requires_grad]\n",
    "        optimizer = torch.optim.Adam(params, \n",
    "                                     lr=scaled_lr,\n",
    "                                     betas=(0.9, 0.999),\n",
    "                                     eps=1e-8)\n",
    "\n",
    "        # Define training and validation data loaders\n",
    "        data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, \n",
    "                                                  shuffle=False, \n",
    "                                                  collate_fn=ChestnutBurSegmentation.collate_fn, \n",
    "                                                  num_workers=0,\n",
    "                                                  pin_memory=True)\n",
    "        \n",
    "        print(f'Training step: {ts[\"step\"]}, effective batch size: {batch_size * accumulation_steps}, scaled lr: {scaled_lr:.6f}\\n')\n",
    "\n",
    "        window_loss, window_f1 = [], []\n",
    "        window_size = 5\n",
    "        plateau_tolerance = 0.0001\n",
    "        minimum_step_epochs = 10\n",
    "        step_epoch_counter = 0\n",
    "\n",
    "        #########################################################\n",
    "        ##               The main training loop                ## \n",
    "        #########################################################\n",
    "        with profiler:\n",
    "            while True:\n",
    "                step_epoch_counter += 1\n",
    "                # Monitor memory usage at the start of the epoch\n",
    "                print(f\"Epoch {start_epoch} - Memory allocated: {torch.cuda.memory_allocated(device)} bytes\")\n",
    "\n",
    "                train_metric_logger, val_metric_logger = engine.train_one_epoch(model, optimizer, data_loader, device, \n",
    "                                                                                start_epoch, print_freq, accumulation_steps,\n",
    "                                                                                data_loader_val)\n",
    "\n",
    "                # Evaluate on the validation dataset\n",
    "                train_coco_evaluator, val_coco_evaluator = engine.evaluate(model, data_loader_val, val_coco_ds, device,\n",
    "                                                                            data_loader, train_coco_ds)\n",
    "\n",
    "                # Add visualization for monitoring validation predictions\n",
    "                if start_epoch % 5 == 0:  # Visualize every 5 epochs\n",
    "                    visualize_predictions(model, data_loader_val, device, start_epoch, num_samples=3)\n",
    "                \n",
    "                current_val_f1 = calculate_f1_score(val_coco_evaluator.coco_eval['segm'].stats[0],\n",
    "                                                      val_coco_evaluator.coco_eval['segm'].stats[6])\n",
    "                current_val_loss = val_metric_logger.total_loss.avg\n",
    "\n",
    "                window_loss.append(current_val_loss)\n",
    "                window_f1.append(current_val_f1)\n",
    "                if len(window_loss) > window_size:\n",
    "                    window_loss.pop(0)\n",
    "                if len(window_f1) > window_size:\n",
    "                    window_f1.pop(0)\n",
    "\n",
    "                if step_epoch_counter >= minimum_step_epochs and len(window_loss) == window_size:\n",
    "                    loss_plateaued = is_plateaued(window_loss, tolerance=plateau_tolerance)\n",
    "                    f1_plateaued = is_plateaued(window_f1, tolerance=plateau_tolerance)\n",
    "                    \n",
    "                    if loss_plateaued or f1_plateaued:\n",
    "                        print(f\"{'Loss' if loss_plateaued else 'F1'} plateaued after {step_epoch_counter} epochs\")\n",
    "                        print(f\"Recent losses: {window_loss}\")\n",
    "                        print(f\"Recent F1 scores: {window_f1}\")\n",
    "                        start_epoch += 1\n",
    "                        print(\"Moving to next training step.\")\n",
    "                        break\n",
    "\n",
    "                # Store training and validation metrics in checkpoint dictionary. \n",
    "                checkpoint = {\n",
    "                    \"epoch\": start_epoch,\n",
    "                    \"step_index\": step_index,\n",
    "                    \"train_loss\": train_metric_logger.total_loss.avg, # average across entire training epoch\n",
    "                    \"val_loss\": val_metric_logger.total_loss.avg,\n",
    "                    \"train_mAP\": train_coco_evaluator.coco_eval['segm'].stats[0],# IoU=0.50:0.95\n",
    "                    \"val_mAP\": val_coco_evaluator.coco_eval['segm'].stats[0],\n",
    "                    \"train_mAR\": train_coco_evaluator.coco_eval['segm'].stats[6],# IoU=0.50:0.95\n",
    "                    \"val_mAR\": val_coco_evaluator.coco_eval['segm'].stats[6],\n",
    "                    \"train_f1\": calculate_f1_score(train_coco_evaluator.coco_eval['segm'].stats[0], \n",
    "                                                    train_coco_evaluator.coco_eval['segm'].stats[6]\n",
    "                                                    ),\n",
    "                    \"val_f1\": calculate_f1_score(val_coco_evaluator.coco_eval['segm'].stats[0],\n",
    "                                                    val_coco_evaluator.coco_eval['segm'].stats[6]\n",
    "                                                    ),\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict()\n",
    "                }\n",
    "\n",
    "                # Append checkpoint to checkpoints list\n",
    "                checkpoints.append(checkpoint)\n",
    "\n",
    "                # Report training and validation scalars to tensorboard\n",
    "                writer.add_scalar('Model/Learning_Rate', optimizer.param_groups[0]['lr'], checkpoint['epoch'])\n",
    "                writer.add_scalar('Model/Optimal_Threshold', model.threshold, checkpoint['epoch'])\n",
    "                writer.add_scalar('Loss/Train', np.array(float(checkpoint[\"train_loss\"])), checkpoint[\"epoch\"]) # use tags to group scalars\n",
    "                writer.add_scalar('Loss/Val', np.array(float(checkpoint[\"val_loss\"])), checkpoint[\"epoch\"])\n",
    "                writer.add_scalar('mAP/Train', np.array(float(checkpoint[\"train_mAP\"])), checkpoint[\"epoch\"])\n",
    "                writer.add_scalar('mAP/Val', np.array(float(checkpoint[\"val_mAP\"])), checkpoint[\"epoch\"])\n",
    "                writer.add_scalar('mAR/Train', np.array(float(checkpoint[\"train_mAR\"])), checkpoint[\"epoch\"])\n",
    "                writer.add_scalar('mAR/Val', np.array(float(checkpoint[\"val_mAR\"])), checkpoint[\"epoch\"])\n",
    "                writer.add_scalar('F1/Train', np.array(float(checkpoint[\"train_f1\"])), checkpoint[\"epoch\"])\n",
    "                writer.add_scalar('F1/Val', np.array(float(checkpoint[\"val_f1\"])), checkpoint[\"epoch\"])\n",
    "\n",
    "                # Clear CUDA cache and collect garbage \n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "                print(f\"Epoch {start_epoch} - Max memory allocated: {torch.cuda.max_memory_allocated(device)} bytes\")\n",
    "                start_epoch += 1\n",
    "\n",
    "        step_index += 1\n",
    "\n",
    "    print('All Training Steps Complete!')\n",
    "    writer.close()\n",
    "    return checkpoints\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    checkpoints = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.profiler\n",
    "\n",
    "def main(train_coco_ds, val_coco_ds, best_trial):\n",
    "\n",
    "    print(best_trial.config)\n",
    "    \n",
    "    # Set seed\n",
    "    set_seed(666)\n",
    "\n",
    "    training_steps = [\n",
    "        {\"step\": 0, \"batch_size\": 4, \"print_freq\": 50, \"accumulation_steps\": 1},\n",
    "        {\"step\": 1, \"batch_size\": 8, \"print_freq\": 25, \"accumulation_steps\": 1},\n",
    "        {\"step\": 2, \"batch_size\": 16, \"print_freq\": 15, \"accumulation_steps\": 1},\n",
    "        {\"step\": 3, \"batch_size\": 32, \"print_freq\": 5, \"accumulation_steps\": 1},\n",
    "        {\"step\": 4, \"batch_size\": 32, \"print_freq\": 5, \"accumulation_steps\": 2},\n",
    "        {\"step\": 5, \"batch_size\": 32, \"print_freq\": 5, \"accumulation_steps\": 4},\n",
    "        {\"step\": 6, \"batch_size\": 32, \"print_freq\": 5, \"accumulation_steps\": 8},\n",
    "        {\"step\": 7, \"batch_size\": 32, \"print_freq\": 5, \"accumulation_steps\": 16} # bs 512\n",
    "    ]\n",
    "\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "\n",
    "    # Initialize the tensorboard writer\n",
    "    current_datetime = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    writer = SummaryWriter(log_dir=f'C:/Users/zack/Documents/GitHub/Savanna-Institute/Drone-based_Chestnut_Detection/runs/UNet/{current_datetime}')\n",
    "\n",
    "    # Store one checkpoint dictionary for each epoch in a list of dictionaries. \n",
    "    checkpoints = []\n",
    "\n",
    "    # Initialize the profiler\n",
    "    profiler = torch.profiler.profile(\n",
    "        activities=[\n",
    "            torch.profiler.ProfilerActivity.CPU,\n",
    "            torch.profiler.ProfilerActivity.CUDA,\n",
    "        ],\n",
    "        schedule=torch.profiler.schedule(\n",
    "            wait=1,\n",
    "            warmup=1,\n",
    "            active=3,\n",
    "            repeat=2),\n",
    "        on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name = f'C:/Users/zack/Documents/GitHub/Savanna-Institute/Drone-based_Chestnut_Detection/runs/UNet_profiler/{current_datetime}'),\n",
    "        record_shapes=True,\n",
    "        profile_memory=True,\n",
    "        with_stack=True\n",
    "    )\n",
    "\n",
    "    dataset = ChestnutBurSegmentation(images_dir, \n",
    "                                      masks_dir, \n",
    "                                      annotations_csv,\n",
    "                                      transform = get_transform(train=True))\n",
    "    \n",
    "    dataset_val = ChestnutBurSegmentation(images_dir,\n",
    "                                          masks_dir,\n",
    "                                          annotations_csv,\n",
    "                                          transform = get_transform(train=False))\n",
    "\n",
    "    dataset = Subset(dataset, train_indices)\n",
    "    dataset_val = Subset(dataset_val, val_indices)                                  \n",
    "    \n",
    "    data_loader_val = torch.utils.data.DataLoader(\n",
    "        dataset_val, batch_size=1, shuffle=False,\n",
    "        collate_fn=ChestnutBurSegmentation.collate_fn, num_workers=0, pin_memory=True\n",
    "    )\n",
    "    \n",
    "    start_epoch, step_index = 0, 0\n",
    "\n",
    "    # Loop through training_steps during training to increase batch size and decrease learning rate\n",
    "    while step_index < len(training_steps):\n",
    "        ts = training_steps[step_index]\n",
    "        batch_size, print_freq, accumulation_steps = ts[\"batch_size\"], ts[\"print_freq\"], ts[\"accumulation_steps\"]\n",
    "        scaled_lr = best_trial.config[\"lr\"] * (batch_size / training_steps[0][\"batch_size\"]) * accumulation_steps\n",
    "\n",
    "        # Reinitialize the model with the current hyperparameters\n",
    "        model = SmallObjectUNet(in_channels=3,\n",
    "                                out_channels=1,\n",
    "                                base_filters=best_trial.config[\"base_filters\"],\n",
    "                                dataset=dataset,\n",
    "                                grad_clip=best_trial.config[\"grad_clip\"],\n",
    "                                l2_lambda=best_trial.config[\"l2_lambda\"],\n",
    "                                pos_weight=best_trial.config[\"pos_weight\"],\n",
    "                                dice_weight=best_trial.config[\"dice_weight\"],\n",
    "                                deep_supervision=True,\n",
    "                                dropout_rate=best_trial.config[\"dropout_rate\"],\n",
    "                                sup_weights=best_trial.config[\"sup_weights\"]\n",
    "            )\n",
    "\n",
    "        model.to(device)\n",
    "\n",
    "        # Construct an optimizer with the suggested learning rate\n",
    "        params = [p for p in model.parameters() if p.requires_grad]\n",
    "        optimizer = torch.optim.Adam(params, \n",
    "                                     lr=scaled_lr,\n",
    "                                     betas=(best_trial.config[\"beta1_loss\"], best_trial.config[\"beta2_loss\"]),\n",
    "                                     eps=best_trial.config[\"epsilon_loss\"])\n",
    "\n",
    "        # Define training and validation data loaders\n",
    "        data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, \n",
    "                                                  shuffle=False, \n",
    "                                                  collate_fn=ChestnutBurSegmentation.collate_fn, \n",
    "                                                  num_workers=0,\n",
    "                                                  pin_memory=True)\n",
    "        \n",
    "        print(f'Training step: {ts[\"step\"]}, effective batch size: {batch_size * accumulation_steps}, scaled lr: {scaled_lr:.6f}\\n')\n",
    "\n",
    "        window_loss, window_f1 = [], []\n",
    "        window_size = 5\n",
    "        plateau_tolerance = 0.0001\n",
    "        minimum_step_epochs = 10\n",
    "        step_epoch_counter = 0\n",
    "\n",
    "        #########################################################\n",
    "        ##               The main training loop                ## \n",
    "        #########################################################\n",
    "        with profiler:\n",
    "            while True:\n",
    "                step_epoch_counter += 1\n",
    "                # Monitor memory usage at the start of the epoch\n",
    "                print(f\"Epoch {start_epoch} - Memory allocated: {torch.cuda.memory_allocated(device)} bytes\")\n",
    "\n",
    "                train_metric_logger, val_metric_logger = engine.train_one_epoch(model, optimizer, data_loader, device, \n",
    "                                                                                start_epoch, print_freq, accumulation_steps,\n",
    "                                                                                data_loader_val)\n",
    "\n",
    "                # Evaluate on the validation dataset\n",
    "                train_coco_evaluator, val_coco_evaluator = engine.evaluate(model, data_loader_val, val_coco_ds, device,\n",
    "                                                                            data_loader, train_coco_ds)\n",
    "                \n",
    "                # Add visualization for monitoring validation predictions\n",
    "                if start_epoch % 10 == 0:  # Visualize every 5 epochs\n",
    "                    visualize_predictions(model, data_loader_val, device, start_epoch)\n",
    "                \n",
    "                current_val_f1 = calculate_f1_score(val_coco_evaluator.coco_eval['segm'].stats[0],\n",
    "                                                      val_coco_evaluator.coco_eval['segm'].stats[6])\n",
    "                current_val_loss = val_metric_logger.total_loss.avg\n",
    "\n",
    "                window_loss.append(current_val_loss)\n",
    "                window_f1.append(current_val_f1)\n",
    "                if len(window_loss) > window_size:\n",
    "                    window_loss.pop(0)\n",
    "                if len(window_f1) > window_size:\n",
    "                    window_f1.pop(0)\n",
    "\n",
    "                if step_epoch_counter >= minimum_step_epochs and len(window_loss) == window_size:\n",
    "                    loss_plateaued = is_plateaued(window_loss, tolerance=plateau_tolerance)\n",
    "                    f1_plateaued = is_plateaued(window_f1, tolerance=plateau_tolerance)\n",
    "                    \n",
    "                    if loss_plateaued or f1_plateaued:\n",
    "                        print(f\"{'Loss' if loss_plateaued else 'F1'} plateaued after {step_epoch_counter} epochs\")\n",
    "                        print(f\"Recent losses: {window_loss}\")\n",
    "                        print(f\"Recent F1 scores: {window_f1}\")\n",
    "                        start_epoch += 1\n",
    "                        print(\"Moving to next training step.\")\n",
    "                        break\n",
    "\n",
    "                # Store training and validation metrics in checkpoint dictionary. \n",
    "                checkpoint = {\n",
    "                    \"epoch\": start_epoch,\n",
    "                    \"step_index\": step_index,\n",
    "                    \"train_loss\": train_metric_logger.total_loss.avg, # average across entire training epoch\n",
    "                    \"val_loss\": val_metric_logger.total_loss.avg,\n",
    "                    \"train_mAP\": train_coco_evaluator.coco_eval['segm'].stats[0],# IoU=0.50:0.95\n",
    "                    \"val_mAP\": val_coco_evaluator.coco_eval['segm'].stats[0],\n",
    "                    \"train_mAR\": train_coco_evaluator.coco_eval['segm'].stats[6],# IoU=0.50:0.95\n",
    "                    \"val_mAR\": val_coco_evaluator.coco_eval['segm'].stats[6],\n",
    "                    \"train_f1\": calculate_f1_score(train_coco_evaluator.coco_eval['segm'].stats[0], \n",
    "                                                    train_coco_evaluator.coco_eval['segm'].stats[6]\n",
    "                                                    ),\n",
    "                    \"val_f1\": calculate_f1_score(val_coco_evaluator.coco_eval['segm'].stats[0],\n",
    "                                                    val_coco_evaluator.coco_eval['segm'].stats[6]\n",
    "                                                    ),\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict()\n",
    "                }\n",
    "\n",
    "                # Append checkpoint to checkpoints list\n",
    "                checkpoints.append(checkpoint)\n",
    "\n",
    "                # Report training and validation scalars to tensorboard\n",
    "                writer.add_scalar('Learning_Rate', optimizer.param_groups[0]['lr'], checkpoint['epoch'])\n",
    "                writer.add_scalar('Segmentation Loss/Train', np.array(float(checkpoint[\"train_loss\"])), checkpoint[\"epoch\"]) # use tags to group scalars\n",
    "                writer.add_scalar('Segmentation Loss/Val', np.array(float(checkpoint[\"val_loss\"])), checkpoint[\"epoch\"])\n",
    "                writer.add_scalar('Segmentation mAP/Train', np.array(float(checkpoint[\"train_mAP\"])), checkpoint[\"epoch\"])\n",
    "                writer.add_scalar('Segmentation mAP/Val', np.array(float(checkpoint[\"val_mAP\"])), checkpoint[\"epoch\"])\n",
    "                writer.add_scalar('Segmentation mAR/Train', np.array(float(checkpoint[\"train_mAR\"])), checkpoint[\"epoch\"])\n",
    "                writer.add_scalar('Segmentation mAR/Val', np.array(float(checkpoint[\"val_mAR\"])), checkpoint[\"epoch\"])\n",
    "                writer.add_scalar('Segmentation F1/Train', np.array(float(checkpoint[\"train_f1\"])), checkpoint[\"epoch\"])\n",
    "                writer.add_scalar('Segmentation F1/Val', np.array(float(checkpoint[\"val_f1\"])), checkpoint[\"epoch\"])\n",
    "\n",
    "                # Clear CUDA cache and collect garbage \n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect() \n",
    "\n",
    "                # Monitor memory usage at the end of the epoch\n",
    "                print(f\"Epoch {start_epoch} - Max memory allocated: {torch.cuda.max_memory_allocated(device)} bytes\")\n",
    "\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "                print(f\"Epoch {start_epoch} - Max memory allocated: {torch.cuda.max_memory_allocated(device)} bytes\")\n",
    "                start_epoch += 1\n",
    "\n",
    "        step_index += 1\n",
    "\n",
    "    print('All Training Steps Complete!')\n",
    "    writer.close()\n",
    "    return checkpoints\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     torch.cuda.empty_cache()\n",
    "#     gc.collect()\n",
    "    \n",
    "#     checkpoints = main(train_coco_ds, val_coco_ds, best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best train epoch is dictionary in checkpoints with highest val_mAP_50 value\n",
    "best_train_epoch = max(checkpoints, key = lambda x: x['val_f1'])\n",
    "\n",
    "model = SmallObjectUNet(in_channels=3,\n",
    "                        out_channels=1,\n",
    "                        base_filters=best_trial.config[\"base_filters\"],\n",
    "                        dataset=None,\n",
    "                        grad_clip=best_trial.config[\"grad_clip\"],\n",
    "                        l2_lambda=best_trial.config[\"l2_lambda\"],\n",
    "                        pos_weight=best_trial.config[\"pos_weight\"],\n",
    "                        dice_weight=best_trial.config[\"dice_weight\"],\n",
    "                        deep_supervision=True,\n",
    "                        dropout_rate=best_trial.config[\"dropout_rate\"],\n",
    "                        sup_weights=best_trial.config[\"sup_weights\"]\n",
    "    )\n",
    "\n",
    "# load model weights from best_train_epoch\n",
    "model.load_state_dict(best_train_epoch[\"model_state_dict\"])\n",
    "\n",
    "# save model weights to .pth file\n",
    "torch.save(model.state_dict(), f\"S:/Zack/Deep Learning/Chestnut_Bur_Instance_Segmentation/U-Net/U-Net_{time.strftime('%Y%m%d')}_{time.strftime('%H%M%S')}.pth\")\n",
    "\n",
    "# copy checkpoints and remove model and optimizer state dicts\n",
    "checkpoints_copy = checkpoints.copy()\n",
    "for c in checkpoints_copy:\n",
    "    del c[\"model_state_dict\"]\n",
    "    del c[\"optimizer_state_dict\"]\n",
    "\n",
    "# save checkpoints list to text file\n",
    "with open(f\"S:/Zack/Deep Learning/Chestnut_Bur_Instance_Segmentation/U-Net/checkpoints_{time.strftime('%Y%m%d')}_{time.strftime('%H%M%S')}.txt\", 'w') as f:\n",
    "    for item in checkpoints_copy:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <center> Test Set Performance </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = ChestnutBurSegmentation(images_dir,\n",
    "                                       masks_dir,\n",
    "                                       annotations_csv,\n",
    "                                       transform = get_transform(train=False)) \n",
    "\n",
    "dataset_test = Subset(dataset_test, test_indices)\n",
    "\n",
    "test_coco_ds = get_coco_api_from_dataset(dataset_test)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(dataset_test, batch_size=1, shuffle=False,\n",
    "                                                  collate_fn=ChestnutBurSegmentation.collate_fn, \n",
    "                                                  num_workers=0, pin_memory=True)\n",
    "\n",
    "# create dictionary of test indices (as key) and tree_ids (as value)\n",
    "test_dict = {}\n",
    "for i in range(len(dataset_test)):\n",
    "    test_dict[i] = dataset_test.file_names[i]\n",
    "\n",
    "# save test_dict to text file just to be safe\n",
    "with open(f\"S:/Zack/Deep Learning/Chestnut_Bur_Instance_Segmentation/U-Net/test_dict_{time.strftime('%Y%m%d')}_{time.strftime('%H%M%S')}.txt\", 'w') as f:\n",
    "    for key, value in test_dict.items():\n",
    "        f.write('%s:%s\\n' % (key, value))\n",
    "\n",
    "test_performance = engine.evaluate(model, data_loader_test, test_coco_ds, device=torch.device('cpu'), train_data_loader=None, train_coco_ds=None)\n",
    "\n",
    "print(f'Best trial test set mAP_50: {test_performance.coco_eval[\"segm\"].stats[0]}') \n",
    "print(f'Best trial test set mAR_100: {test_performance.coco_eval[\"segm\"].stats[6]}')\n",
    "print(f'Best trial test set f1 score: {calculate_f1_score(test_performance.coco_eval[\"segm\"].stats[0], test_performance.coco_eval[\"segm\"].stats[6])}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bohb_pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
